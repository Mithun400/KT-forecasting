{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Looking in indexes: https://repository.walmart.com/repository/pypi-proxy/simple/\nRequirement already satisfied: optuna in /opt/conda/anaconda/lib/python3.6/site-packages (2.10.1)\nRequirement already satisfied: numpy in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (1.19.5)\nRequirement already satisfied: alembic in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (1.5.7)\nRequirement already satisfied: cliff in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (3.10.1)\nRequirement already satisfied: scipy!=1.4.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (1.5.4)\nRequirement already satisfied: sqlalchemy>=1.1.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (1.4.22)\nRequirement already satisfied: PyYAML in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (5.4.1)\nRequirement already satisfied: cmaes>=0.8.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (0.8.2)\nRequirement already satisfied: tqdm in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (4.64.1)\nRequirement already satisfied: colorlog in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (6.7.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from packaging>=20.0->optuna) (3.0.7)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/anaconda/lib/python3.6/site-packages (from sqlalchemy>=1.1.0->optuna) (1.1.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/anaconda/lib/python3.6/site-packages (from sqlalchemy>=1.1.0->optuna) (3.10.1)\nRequirement already satisfied: python-dateutil in /opt/conda/anaconda/lib/python3.6/site-packages (from alembic->optuna) (2.8.2)\nRequirement already satisfied: python-editor>=0.3 in /opt/conda/anaconda/lib/python3.6/site-packages (from alembic->optuna) (1.0.4)\nRequirement already satisfied: Mako in /opt/conda/anaconda/lib/python3.6/site-packages (from alembic->optuna) (1.1.4)\nRequirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from cliff->optuna) (5.11.0)\nRequirement already satisfied: cmd2>=1.0.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from cliff->optuna) (2.4.2)\nRequirement already satisfied: PrettyTable>=0.7.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from cliff->optuna) (2.5.0)\nRequirement already satisfied: stevedore>=2.0.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from cliff->optuna) (3.5.2)\nRequirement already satisfied: autopage>=0.4.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from cliff->optuna) (0.5.1)\nRequirement already satisfied: attrs>=16.3.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from cmd2>=1.0.0->cliff->optuna) (18.1.0)\nRequirement already satisfied: typing-extensions in /opt/conda/anaconda/lib/python3.6/site-packages (from cmd2>=1.0.0->cliff->optuna) (3.10.0.2)\nRequirement already satisfied: pyperclip>=1.6 in /opt/conda/anaconda/lib/python3.6/site-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\nRequirement already satisfied: wcwidth>=0.1.7 in /opt/conda/anaconda/lib/python3.6/site-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.6.0)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from Mako->alembic->optuna) (2.0.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from python-dateutil->alembic->optuna) (1.16.0)\nRequirement already satisfied: importlib-resources in /opt/conda/anaconda/lib/python3.6/site-packages (from tqdm->optuna) (5.4.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nLooking in indexes: https://repository.walmart.com/repository/pypi-proxy/simple/\nRequirement already satisfied: pmdarima in /opt/conda/anaconda/lib/python3.6/site-packages (1.2.1)\nRequirement already satisfied: pandas>=0.19 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima) (1.1.5)\nRequirement already satisfied: numpy>=1.15 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima) (1.19.5)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima) (1.1.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima) (1.16.0)\nRequirement already satisfied: statsmodels>=0.9.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima) (0.12.2)\nRequirement already satisfied: Cython>=0.29 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima) (0.29.32)\nRequirement already satisfied: scikit-learn>=0.19 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima) (0.24.2)\nCollecting scipy<1.3,>=1.2\n  Downloading https://repository.walmart.com/repository/pypi-proxy/packages/scipy/scipy-1.2.3-cp36-cp36m-manylinux1_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2M3L2Q2LzY2MWNmNGZiMzJlY2RhNDBmYzAzNjg1ZWYwOTk1ZGIzZDMzMDBjNjE5ZjQ2MTc1MTYyMWY2NDZhYjNmNi9zY2lweS0xLjIuMy1jcDM2LWNwMzZtLW1hbnlsaW51eDFfeDg2XzY0LndobCNzaGEyNTY9YzA4MDEzZTBmZTE1NTQzNzJkYTkzMTJkNWJhZDU4ODQwMmY3MWMwNjM2ZjBmODZhOWI5YjYxZDUwN2M1OWJhYw== (24.8 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24.8 MB 47.7 MB/s \n\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from pandas>=0.19->pmdarima) (2022.5)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/anaconda/lib/python3.6/site-packages (from pandas>=0.19->pmdarima) (2.8.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from scikit-learn>=0.19->pmdarima) (3.1.0)\nRequirement already satisfied: patsy>=0.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from statsmodels>=0.9.0->pmdarima) (0.5.3)\nInstalling collected packages: scipy\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.5.4\n    Uninstalling scipy-1.5.4:\n      Successfully uninstalled scipy-1.5.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsktime 0.9.0 requires numpy<=1.19.3, but you have numpy 1.19.5 which is incompatible.\nsktime 0.9.0 requires statsmodels<=0.12.1, but you have statsmodels 0.12.2 which is incompatible.\u001b[0m\nSuccessfully installed scipy-1.2.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsktime 0.9.0 requires numpy<=1.19.3, but you have numpy 1.19.5 which is incompatible.\nsktime 0.9.0 requires statsmodels<=0.12.1, but you have statsmodels 0.12.2 which is incompatible.\u001b[0m\nLooking in indexes: https://repository.walmart.com/repository/pypi-proxy/simple/\nRequirement already satisfied: yellowbrick==0.9.1 in /opt/conda/anaconda/lib/python3.6/site-packages (0.9.1)\nCollecting scikit-learn==0.22.2\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/scikit-learn/scikit_learn-0.22.2-cp36-cp36m-manylinux1_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2UxLzdmLzM2NmRjYmExYmEwNzZhODhhNTBiZWE3MzJkYmMwMzNjMGM1YmJmNzg3NjAxMGU2ZWRjNjc5NDg1NzlkNS9zY2lraXRfbGVhcm4tMC4yMi4yLWNwMzYtY3AzNm0tbWFueWxpbnV4MV94ODZfNjQud2hsI3NoYTI1Nj1iYzhiMzdjYTBkZTc5NDgzODM3NmNmZmRkYWI2OWE0NmE5YTU0YzQzMzA4MDBlYTFmOTNjYWI2MWNhNjdkZmEz (7.1 MB)\nRequirement already satisfied: numpy>=1.13.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from yellowbrick==0.9.1) (1.19.5)\nRequirement already satisfied: matplotlib!=3.0.0,>=1.5.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from yellowbrick==0.9.1) (2.2.4)\nRequirement already satisfied: cycler>=0.10.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from yellowbrick==0.9.1) (0.10.0)\nRequirement already satisfied: scipy>=1.0.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from yellowbrick==0.9.1) (1.2.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/anaconda/lib/python3.6/site-packages (from scikit-learn==0.22.2) (1.1.1)\nRequirement already satisfied: six in /opt/conda/anaconda/lib/python3.6/site-packages (from cycler>=0.10.0->yellowbrick==0.9.1) (1.16.0)\nRequirement already satisfied: pytz in /opt/conda/anaconda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=1.5.1->yellowbrick==0.9.1) (2022.5)\nRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=1.5.1->yellowbrick==0.9.1) (2.8.2)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=1.5.1->yellowbrick==0.9.1) (1.0.1)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=1.5.1->yellowbrick==0.9.1) (3.0.7)\nRequirement already satisfied: setuptools in /opt/conda/anaconda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=1.5.1->yellowbrick==0.9.1) (59.6.0)\nInstalling collected packages: scikit-learn\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 0.24.2\n    Uninstalling scikit-learn-0.24.2:\n      Successfully uninstalled scikit-learn-0.24.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsktime 0.9.0 requires numpy<=1.19.3, but you have numpy 1.19.5 which is incompatible.\nsktime 0.9.0 requires scikit-learn>=0.24.0, but you have scikit-learn 0.22.2 which is incompatible.\nsktime 0.9.0 requires statsmodels<=0.12.1, but you have statsmodels 0.12.2 which is incompatible.\u001b[0m\nSuccessfully installed scikit-learn-0.22.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nLooking in indexes: https://repository.walmart.com/repository/pypi-proxy/simple/\nCollecting sktime\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/sktime/sktime-0.9.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzczLzcyLzg0MGMzMmI0ZGJhMjM2Y2ZkYmQzZDA0NmJjNTUzZDNlY2ZlNDQwNzliY2VkZTNlYThmMDAwY2E0Mzk2ZC9za3RpbWUtMC45LjAtY3AzNi1jcDM2bS1tYW55bGludXhfMl8xN194ODZfNjQubWFueWxpbnV4MjAxNF94ODZfNjQud2hsI3NoYTI1Nj0zZDgwY2ZhZjgwY2NhNjA5M2UwYjg2MjQxMTA5ZDgzNmQ2ZWJlMGE1NzAyNDBkMTliNDE1MmY2NDc5MzVjMTdh (6.2 MB)\nCollecting scikit-learn>=0.24.0\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/scikit-learn/scikit_learn-0.24.2-cp36-cp36m-manylinux2010_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2QzL2ViL2QwZTY1ODQ2NWMwMjlmZWI3MDgzMTM5ZDllYWQ1MTAwMDc0MmU4OGIxZmI3ZjE1MDRlMTllMWI0Y2U2ZS9zY2lraXRfbGVhcm4tMC4yNC4yLWNwMzYtY3AzNm0tbWFueWxpbnV4MjAxMF94ODZfNjQud2hsI3NoYTI1Nj01ZmYzZTRlNGNmNzU5MmQzNjU0MWVkZWM0MzRlMDlmYjhhYjliYTZiNDc2MDhjNGZmZTMwYzkwMzhkMzAxODk3 (22.2 MB)\nCollecting numba>=0.53\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/numba/numba-0.53.1-cp36-cp36m-manylinux2014_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzRhL2MxL2U3ZmRiZmM4ODZhOWQ5YzExNzY3NTMzOTAzZGIwZDgxNmMwZjY1NmZkNjAyOWY0YTA2MTc0Mjg5MzY5NC9udW1iYS0wLjUzLjEtY3AzNi1jcDM2bS1tYW55bGludXgyMDE0X3g4Nl82NC53aGwjc2hhMjU2PThmYTVjOTYzYTQzODU1MDUwYTg2ODEwNmE4N2NkNjE0ZjNjM2Y0NTk5NTFjOGZjNDY4YWVjMjYzZWY4MGQwNjM= (3.4 MB)\nCollecting numpy<=1.19.3\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/numpy/numpy-1.19.3-cp36-cp36m-manylinux2010_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzBlL2Y3L2E3ZDdlMGRlOTlhN2I0M2JkOTVhYWRkY2YyOWU2NWI1YTE4NWNhMzg5ZGQxMzI5YTUzY2M5ODZlZGMzOC9udW1weS0xLjE5LjMtY3AzNi1jcDM2bS1tYW55bGludXgyMDEwX3g4Nl82NC53aGwjc2hhMjU2PTcxOTdlZTBhMjU2MjllZDc4MmM3YmQwMTg3MWVlNDA3MDJmZmVlZjM1YmM0ODAwNGJjMmZkY2M3MWUyOWJhOWQ= (14.9 MB)\nCollecting deprecated>=1.2.13\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/deprecated/Deprecated-1.2.13-py2.py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzUxLzZhL2MzYTA0MDg2NDY0MDhmNzI4M2I3YmM1NTBjMzBhMzJjYzc5MTE4MWVjNDYxODU5MmVlYzEzZTA2NmNlMy9EZXByZWNhdGVkLTEuMi4xMy1weTIucHkzLW5vbmUtYW55LndobCNzaGEyNTY9NjQ3NTZlM2UxNGM4YzVlZWE5Nzk1ZDkzYzUyNDU1MTQzMmEwYmU3NTYyOWY4ZjI5ZTY3YWI4Y2FmMDc2Yzc2ZA== (9.6 kB)\nCollecting statsmodels<=0.12.1\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/statsmodels/statsmodels-0.12.1-cp36-cp36m-manylinux1_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2JlLzRjLzllMjQzNWNhNjY0NWQ2YmFmYTJiNTFiYjExZjBhMzY1YjI4OTM0YTJmZmU5ZDZlMzM5ZDY3MTMwOTI2ZC9zdGF0c21vZGVscy0wLjEyLjEtY3AzNi1jcDM2bS1tYW55bGludXgxX3g4Nl82NC53aGwjc2hhMjU2PTE0MmVhY2Q1YTFiZDg3MjgzNThmZjQ4MTAxZWUwZTUxY2EzZDQyYTkzZjZlNWNiNjFmY2ZhY2Y2MTM5NzdiY2Y= (9.5 MB)\nCollecting wheel\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/wheel/wheel-0.37.1-py2.py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzI3L2Q2LzAwM2U1OTMyOTZhODVmZDZlZDYxNmVkOTYyNzk1YjJmODc3MDljM2VlZTJiY2E0ZjZkMGZlNTVjNmQwMC93aGVlbC0wLjM3LjEtcHkyLnB5My1ub25lLWFueS53aGwjc2hhMjU2PTRiZGNkN2Q4NDAxMzgwODYxMjZjZDA5MjU0ZGM2MTk1ZmI0ZmM2ZjAxYzA1MGExZDcyMzZmMjYzMGRiMWQyMmE= (35 kB)\nCollecting pandas>=1.1.0\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/pandas/pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2MzL2UyLzAwY2FjZWNhZmJhYjA3MWM3ODcwMTlmMDBhZDg0Y2EzMTg1OTUyZjZiYjliY2E5NTUwZWQ4Mzg3MGQ0ZC9wYW5kYXMtMS4xLjUtY3AzNi1jcDM2bS1tYW55bGludXgxX3g4Nl82NC53aGwjc2hhMjU2PWI2MTA4MDc1MGQxOWEwMTIyNDY5YWI1OWIwODczODA3MjFkNmI3MmE0ZTdkOTYyZTRkN2U2M2UwYzQ1MDQ4MTQ= (9.5 MB)\nCollecting wrapt<2,>=1.10\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/wrapt/wrapt-1.14.1-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2UwLzgwL2FmOWRhNzM3OWVlNmRmNTgzODc1ZDBhZWI4MGY5ZDVmMGJkNWYwODFkZDFlZTVjZTA2NTg3ZDhiZmVjNy93cmFwdC0xLjE0LjEtY3AzNi1jcDM2bS1tYW55bGludXhfMl81X3g4Nl82NC5tYW55bGludXgxX3g4Nl82NC5tYW55bGludXhfMl8xN194ODZfNjQubWFueWxpbnV4MjAxNF94ODZfNjQud2hsI3NoYTI1Nj0yMWFjMDE1NmM0YjA4OWIzMzBiNzY2NmRiNDBmZWVlMzBhNWQ1MjYzNGNjNDU2MGUxOTA1ZDY1MjlhMzg5N2Zm (74 kB)\nCollecting llvmlite<0.37,>=0.36.0rc1\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/llvmlite/llvmlite-0.36.0-cp36-cp36m-manylinux2010_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzRkLzVhLzcwN2NjN2UwNzJkNzFiYzE5ODY5ZDA5M2U1Y2Y5YjdiZTk4Y2I0MmQyMzk4NDg5NDY1NDc0ZDAwN2NlOC9sbHZtbGl0ZS0wLjM2LjAtY3AzNi1jcDM2bS1tYW55bGludXgyMDEwX3g4Nl82NC53aGwjc2hhMjU2PTc3Njg2NTg2NDZjNDE4YjliM2JlY2NiNzA0NDI3N2E2MDhiYzhjNjJiODJhODVlNzNjN2U1YzA2NWU0MTU3YzI= (25.3 MB)\nCollecting setuptools\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/setuptools/setuptools-59.6.0-py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2IwLzNhLzg4YjIxMGRiNjhlNTY4NTRkMGJjZjRiMzhlMTY1ZTAzYmUzNzdlMTM5MDc3NDZmODI1NzkwZjNkZjViZi9zZXR1cHRvb2xzLTU5LjYuMC1weTMtbm9uZS1hbnkud2hsI3NoYTI1Nj00Y2U5MmYxZTFmOGYwMTIzM2VlOTk1MmMwNGY2YjgxZDFlMDI5MzlkNmUxYjQ4ODQyODE1NDk3NGE0ZDA3ODNl (952 kB)\nCollecting python-dateutil>=2.7.3\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/python-dateutil/python_dateutil-2.8.2-py2.py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzM2LzdhLzg3ODM3ZjM5ZDAyOTZlNzIzYmI5YjYyYmJiMjU3ZDAzNTVjN2Y2MTI4ODUzYzc4OTU1ZjU3MzQyYTU2ZC9weXRob25fZGF0ZXV0aWwtMi44LjItcHkyLnB5My1ub25lLWFueS53aGwjc2hhMjU2PTk2MWQwM2RjMzQ1M2ViYmM1OWRiZGVhOWU0ZTExYzU2NTE1MjBhODc2ZDBmNGRiMTYxZTg2NzRhYWU5MzVkYTk= (247 kB)\nCollecting pytz>=2017.2\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/pytz/pytz-2022.5-py2.py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2I1L2Q3LzkxZmQ4OTExZDIyZTdmYWM3OTQ4MDMwOTVkZDE5MmJmMWViZDcwYzc2MDMyNzIwODUyMzBkOTE1ZTczOC9weXR6LTIwMjIuNS1weTIucHkzLW5vbmUtYW55LndobCNzaGEyNTY9MzM1YWI0NjkwMGIxNDY1ZTcxNGI0ZmRhNDk2M2Q4NzM2MzI2NGViNjYyYWFiNWU2NWRhMDM5YzI1ZjFmNWIyMg== (500 kB)\nCollecting six>=1.5\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/six/six-1.16.0-py2.py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2Q5LzVhL2U3YzMxYWRiZTg3NWYyYWJiYjkxYmQ4NGNmMmRjNTJkNzkyYjVhMDE1MDY3ODFkYmNmMjVjOTFkYWYxMS9zaXgtMS4xNi4wLXB5Mi5weTMtbm9uZS1hbnkud2hsI3NoYTI1Nj04YWJiMmYxZDg2ODkwYTJkZmI5ODlmOWE3N2NmY2ZkM2U0N2MyYTM1NGIwMTExMTc3MTMyNmY4YWEyNmUwMjU0 (11 kB)\nCollecting scipy>=0.19.1\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/scipy/scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2M4Lzg5LzYzMTcxMjI4ZDVjZWQxNDhmNWNlZDUwMzA1Yzg5ZTg1NzZmZmM2OTVhOTBiNThmZTViYjYwMmI5MTBjMi9zY2lweS0xLjUuNC1jcDM2LWNwMzZtLW1hbnlsaW51eDFfeDg2XzY0LndobCNzaGEyNTY9MzY4YzBmNjlmOTMxODYzMDllMWI0YmViOGUyNmQ1MWRkNmY1MDEwYjc5MjY0YzBmMWU5Y2EwMGNkOTJlYThjOQ== (25.9 MB)\nCollecting joblib>=0.11\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/joblib/joblib-1.1.1-py2.py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzdjLzkxL2QzYmEwNDAxZTYyZDdlNDI4MTZiYzdkOTdiODJkMTljOTVjMTY0YjNlMTQ5YTg3YzBhMWMwMjZhNzM1ZS9qb2JsaWItMS4xLjEtcHkyLnB5My1ub25lLWFueS53aGwjc2hhMjU2PWY5ZDZjM2NkZjJhNzc3OGU5MDU4ZTEwZTlkYmEwMjhlNDc3NzFhMWEzNTVlNTc2OGY0NjcwNGJmMDUzNDJlYmE= (309 kB)\nCollecting threadpoolctl>=2.0.0\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/threadpoolctl/threadpoolctl-3.1.0-py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzYxL2NmLzZlMzU0MzA0YmNiOWM2NDEzYzRlMDJhNzQ3YjYwMDA2MWMyMWQzOGJhNTFlN2U1NDRhYzdiYzY2YWVjYy90aHJlYWRwb29sY3RsLTMuMS4wLXB5My1ub25lLWFueS53aGwjc2hhMjU2PThiOTlhZGRhMjY1ZmViNjc3MzI4MGRmNDFlZWNlN2IyZTY1NjFiNzcyZDIxZmZkNTJlMzcyZjk5OTAyNDkwN2I= (14 kB)\nCollecting patsy>=0.5\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/patsy/patsy-0.5.3-py2.py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzJhL2U0L2IzMjYzYjBlMzUzZjJiZTdiMTRmMDQ0ZDU3ODc0NDkwYzljZWYxNzk4YTQzNWYwMzg2ODNhY2VhNWM5OC9wYXRzeS0wLjUuMy1weTIucHkzLW5vbmUtYW55LndobCNzaGEyNTY9N2ViNTM0OTc1NGVkNmFhOTgyYWY4MWY2MzY0NzliMWI4ZGI5ZDViMWE2ZTk1N2E2MDE2ZWMwNTM0YjVjODZiNw== (233 kB)\nInstalling collected packages: six, pytz, python-dateutil, numpy, wrapt, threadpoolctl, setuptools, scipy, patsy, pandas, llvmlite, joblib, wheel, statsmodels, scikit-learn, numba, deprecated, sktime\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndistributed 1.21.8 requires msgpack, which is not installed.\npmdarima 1.2.1 requires scipy<1.3,>=1.2, but you have scipy 1.5.4 which is incompatible.\u001b[0m\nSuccessfully installed deprecated-1.2.13 joblib-1.1.1 llvmlite-0.36.0 numba-0.53.1 numpy-1.19.5 pandas-1.1.5 patsy-0.5.3 python-dateutil-2.8.2 pytz-2022.5 scikit-learn-0.24.2 scipy-1.5.4 setuptools-59.6.0 six-1.16.0 sktime-0.9.0 statsmodels-0.12.2 threadpoolctl-3.1.0 wheel-0.37.1 wrapt-1.14.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nLooking in indexes: https://repository.walmart.com/repository/pypi-proxy/simple/\nRequirement already satisfied: delayed in /opt/conda/anaconda/lib/python3.6/site-packages (0.11.0b1)\nRequirement already satisfied: hiredis in /opt/conda/anaconda/lib/python3.6/site-packages (from delayed) (2.0.0)\nRequirement already satisfied: redis in /opt/conda/anaconda/lib/python3.6/site-packages (from delayed) (4.3.4)\nRequirement already satisfied: typing-extensions in /opt/conda/anaconda/lib/python3.6/site-packages (from redis->delayed) (3.10.0.2)\nRequirement already satisfied: packaging>=20.4 in /opt/conda/anaconda/lib/python3.6/site-packages (from redis->delayed) (21.3)\nRequirement already satisfied: importlib-metadata>=1.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from redis->delayed) (3.10.1)\nRequirement already satisfied: deprecated>=1.2.3 in /opt/conda/anaconda/lib/python3.6/site-packages (from redis->delayed) (1.2.13)\nRequirement already satisfied: async-timeout>=4.0.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from redis->delayed) (4.0.2)\nRequirement already satisfied: wrapt<2,>=1.10 in /opt/conda/anaconda/lib/python3.6/site-packages (from deprecated>=1.2.3->redis->delayed) (1.14.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from importlib-metadata>=1.0->redis->delayed) (3.6.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from packaging>=20.4->redis->delayed) (3.0.7)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nLooking in indexes: https://repository.walmart.com/repository/pypi-proxy/simple/\nCollecting fbprophet\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/fbprophet/fbprophet-0.7.1.tar.gz?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzFhL2I1LzljM2ZlZmE4YTdiODM5NzI5ZGY1N2RlZWRmMGE2OTgxNTg0MWRmYjg4ZjBkZjkxMWYzNGQ5OTgyMzBiNy9mYnByb3BoZXQtMC43LjEudGFyLmd6I3NoYTI1Nj0zYWM2M2EwNWNhZjUxMTdkOWM2ZWQ4M2ZhNmY5ZDRmZmMyMDdiNjBhNTY4NTRjOTA1ZjI3ODlkOGE4OWMyYjZm (64 kB)\nRequirement already satisfied: Cython>=0.22 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (0.29.32)\nRequirement already satisfied: cmdstanpy==0.9.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (0.9.5)\nRequirement already satisfied: pystan>=2.14 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (2.19.1.1)\nRequirement already satisfied: numpy>=1.15.4 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (1.19.5)\nRequirement already satisfied: pandas>=1.0.4 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (1.1.5)\nRequirement already satisfied: matplotlib>=2.0.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (2.2.4)\nRequirement already satisfied: LunarCalendar>=0.0.9 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (0.0.9)\nRequirement already satisfied: convertdate>=2.1.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (2.3.2)\nRequirement already satisfied: holidays>=0.10.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (0.13)\nRequirement already satisfied: setuptools-git>=1.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (1.2)\nRequirement already satisfied: python-dateutil>=2.8.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (2.8.2)\nRequirement already satisfied: tqdm>=4.36.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (4.64.1)\nRequirement already satisfied: pymeeus<=1,>=0.3.13 in /opt/conda/anaconda/lib/python3.6/site-packages (from convertdate>=2.1.2->fbprophet) (0.5.11)\nRequirement already satisfied: pytz>=2014.10 in /opt/conda/anaconda/lib/python3.6/site-packages (from convertdate>=2.1.2->fbprophet) (2022.5)\nRequirement already satisfied: korean-lunar-calendar in /opt/conda/anaconda/lib/python3.6/site-packages (from holidays>=0.10.2->fbprophet) (0.3.1)\nRequirement already satisfied: hijri-converter in /opt/conda/anaconda/lib/python3.6/site-packages (from holidays>=0.10.2->fbprophet) (2.2.4)\nRequirement already satisfied: ephem>=3.7.5.3 in /opt/conda/anaconda/lib/python3.6/site-packages (from LunarCalendar>=0.0.9->fbprophet) (4.1.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from matplotlib>=2.0.0->fbprophet) (1.0.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/anaconda/lib/python3.6/site-packages (from matplotlib>=2.0.0->fbprophet) (0.10.0)\nRequirement already satisfied: six>=1.10 in /opt/conda/anaconda/lib/python3.6/site-packages (from matplotlib>=2.0.0->fbprophet) (1.16.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from matplotlib>=2.0.0->fbprophet) (3.0.7)\nRequirement already satisfied: setuptools in /opt/conda/anaconda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->fbprophet) (59.6.0)\nRequirement already satisfied: importlib-resources in /opt/conda/anaconda/lib/python3.6/site-packages (from tqdm>=4.36.1->fbprophet) (5.4.0)\nRequirement already satisfied: zipp>=3.1.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from importlib-resources->tqdm>=4.36.1->fbprophet) (3.6.0)\nBuilding wheels for collected packages: fbprophet\n  Building wheel for fbprophet (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fbprophet: filename=fbprophet-0.7.1-py3-none-any.whl size=8597722 sha256=f2ff7cf9f0699968eab3de0125fa6361efbc36e9782b1190149573fe5cf4d7f5\n  Stored in directory: /root/.cache/pip/wheels/1b/e4/14/2f55ebbab2a51c29ccf1af51acc28c07a22f461c271631cf2f\nSuccessfully built fbprophet\n\u001b[33mWARNING: Error parsing requirements for scipy: [Errno 2] No such file or directory: '/opt/conda/anaconda/lib/python3.6/site-packages/scipy-1.5.4.dist-info/METADATA'\u001b[0m\nInstalling collected packages: fbprophet\nSuccessfully installed fbprophet-0.7.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nLooking in indexes: https://repository.walmart.com/repository/pypi-proxy/simple/\nRequirement already satisfied: tbats in /opt/conda/anaconda/lib/python3.6/site-packages (1.1.1)\nRequirement already satisfied: pmdarima in /opt/conda/anaconda/lib/python3.6/site-packages (from tbats) (1.2.1)\nRequirement already satisfied: numpy in /opt/conda/anaconda/lib/python3.6/site-packages (from tbats) (1.19.5)\nRequirement already satisfied: scikit-learn in /opt/conda/anaconda/lib/python3.6/site-packages (from tbats) (0.24.2)\nRequirement already satisfied: scipy in /opt/conda/anaconda/lib/python3.6/site-packages (from tbats) (1.2.3)\nRequirement already satisfied: statsmodels>=0.9.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima->tbats) (0.12.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima->tbats) (1.16.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima->tbats) (1.1.1)\nRequirement already satisfied: Cython>=0.29 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima->tbats) (0.29.32)\nRequirement already satisfied: pandas>=0.19 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima->tbats) (1.1.5)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/anaconda/lib/python3.6/site-packages (from pandas>=0.19->pmdarima->tbats) (2.8.2)\nRequirement already satisfied: pytz>=2017.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from pandas>=0.19->pmdarima->tbats) (2022.5)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from scikit-learn->tbats) (3.1.0)\nRequirement already satisfied: patsy>=0.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from statsmodels>=0.9.0->pmdarima->tbats) (0.5.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nLooking in indexes: https://repository.walmart.com/repository/pypi-proxy/simple/\nRequirement already satisfied: workalendar in /opt/conda/anaconda/lib/python3.6/site-packages (16.4.0)\nRequirement already satisfied: convertdate in /opt/conda/anaconda/lib/python3.6/site-packages (from workalendar) (2.3.2)\nRequirement already satisfied: backports.zoneinfo in /opt/conda/anaconda/lib/python3.6/site-packages (from workalendar) (0.2.1)\nRequirement already satisfied: lunardate in /opt/conda/anaconda/lib/python3.6/site-packages (from workalendar) (0.2.0)\nRequirement already satisfied: python-dateutil in /opt/conda/anaconda/lib/python3.6/site-packages (from workalendar) (2.8.2)\nRequirement already satisfied: pyluach in /opt/conda/anaconda/lib/python3.6/site-packages (from workalendar) (1.4.2)\nRequirement already satisfied: importlib-metadata in /opt/conda/anaconda/lib/python3.6/site-packages (from workalendar) (3.10.1)\nRequirement already satisfied: importlib-resources in /opt/conda/anaconda/lib/python3.6/site-packages (from backports.zoneinfo->workalendar) (5.4.0)\nRequirement already satisfied: pymeeus<=1,>=0.3.13 in /opt/conda/anaconda/lib/python3.6/site-packages (from convertdate->workalendar) (0.5.11)\nRequirement already satisfied: pytz>=2014.10 in /opt/conda/anaconda/lib/python3.6/site-packages (from convertdate->workalendar) (2022.5)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/anaconda/lib/python3.6/site-packages (from importlib-metadata->workalendar) (3.10.0.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from importlib-metadata->workalendar) (3.6.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from python-dateutil->workalendar) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"}], "source": "!pip install optuna\n!pip install pmdarima\n!pip install pmdarima scipy==1.2 -Uqq\n!pip install yellowbrick==0.9.1 scikit-learn==0.22.2\n!pip install sktime --ignore-installed\n!pip install delayed\n!pip install fbprophet\n!pip install tbats\n!pip install workalendar\n\nimport optuna\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n# os.chdir(\"../../..\")\nimport pandas as pd\nimport numpy as np\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import *\nfrom pyspark import SparkConf\nfrom pyspark.sql import SparkSession\n\nimport sys, os, time\nimport matplotlib.cm as cm\n\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nfrom workalendar.core import Calendar\nfrom workalendar.registry import registry\nfrom pandas.tseries.offsets import BDay\nimport numpy as np\nfrom pandas.tseries.holiday import (\n    AbstractHolidayCalendar, Holiday, DateOffset, \\\n    SU, MO, TU, WE, TH, FR, SA, \\\n    next_monday, nearest_workday, sunday_to_monday,\n    EasterMonday, GoodFriday, Easter\n)\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error#, mean_absolute_percentage_error\n\n\nfrom datetime import datetime\nimport statistics\n\nimport random\n\n\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom dateutil.relativedelta import relativedelta\n\nimport re\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn import preprocessing\nfrom torch.utils.data import Dataset, DataLoader\n\nspark = SparkSession \\\n    .builder \\\n    .appName(\"WMX - impr forecast - KT\") \\\n    .enableHiveSupport()\\\n    .config(\"hive.exec.dynamic.partition\", \"true\")\\\n    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\\\n    .config(\"spark.sql.catalogImplementation\",\"hive\")\\\n    .config(\"spark.sql.hive.convertMetastoreOrc\", \"false\")\\\n    .config(\"spark.executor.memory\", \"3G\")\\\n    .config(\"spark.driver.memory\", \"3G\")\\\n    .config(\"spark.network.timeout\", \"43200\")\\\n    .config(\"spark.rdd.compress\", \"true\")\\\n    .config(\"spark.cores.max\", \"512\")\\\n    .config(\"spark.default.parallelism\", \"256\")\\\n    .config(\"spark.dynamicAllocation.minExecutors\", \"64\")\\\n    .config(\"spark.dynamicAllocation.maxExecutors\", \"2048\")\\\n    .config(\"spark.executor.memoryOverhead\", \"1G\")\\\n    .config(\"spark.sql.hive.convertMetastoreOrc\", \"false\")\\\n    .config(\"spark.sql.execution.arrow.enabled\",\"true\")\\\n    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n    .config(\"spark.driver.maxResultSize\",\"0\")\\\n    .config(\"spark.memory.offHeap.enabled\",\"true\") \\\n    .config(\"spark.memory.offHeap.size\",\"1g\")\\\n    .config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest_2.11.jar')\\\n    .getOrCreate()"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "cuda:0\n1\nTesla V100-SXM2-16GB\n"}], "source": "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nprint(torch.cuda.device_count())\nprint(torch.cuda.get_device_name(torch.cuda.current_device()))\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n# os.chdir(\"../../..\")\nimport pandas as pd\nimport numpy as np\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import *\nfrom pyspark import SparkConf\nfrom pyspark.sql import SparkSession\n\nimport sys, os, time\nimport matplotlib.cm as cm\n\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error#, mean_absolute_percentage_error\n\n\nfrom datetime import datetime\nimport statistics\n\nimport random\n\n\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom dateutil.relativedelta import relativedelta\n\nimport re\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn import preprocessing\nfrom torch.utils.data import Dataset, DataLoader\n\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n# os.chdir(\"../../..\")\nimport pandas as pd\nimport numpy as np\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import *\nfrom pyspark import SparkConf\nfrom pyspark.sql import SparkSession\nimport os,sys\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\nfrom dateutil.relativedelta import relativedelta\nimport numpy as np\nimport pandas as pd\nimport re\nimport os\nimport sys, os, time\nimport matplotlib.cm as cm\n\nimport matplotlib.pyplot as plt\n\nimport matplotlib.ticker as mticker\n\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error#, mean_absolute_percentage_error\nfrom datetime import datetime\nimport statistics\n\nimport random\nimport time\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\nimport time\nfrom torch.optim.lr_scheduler import ExponentialLR\nimport sys, os, time\nimport matplotlib.cm as cm\n\nimport torch\nfrom datetime import datetime\nimport statistics\nimport random\nimport time\nimport optuna\nfrom optuna.trial import TrialState\nimport torch.nn.functional as F\nfrom datetime import datetime\nimport torch\nfrom sklearn import preprocessing\nfrom datetime import datetime"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": "import pmdarima as pmd\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport numpy as np\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n#import statsmodels.api as sm\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.arima_model import ARIMA\nimport sktime\nimport delayed\nfrom sktime.forecasting.naive import NaiveForecaster\nfrom sktime.forecasting.ets import AutoETS\nfrom warnings import simplefilter\n\nfrom sktime.forecasting.base import ForecastingHorizon\nfrom sktime.forecasting.compose import (\n    EnsembleForecaster,\n    MultiplexForecaster,\n    TransformedTargetForecaster,\n    make_reduction,\n)\nfrom sktime.forecasting.exp_smoothing import ExponentialSmoothing\nfrom sktime.forecasting.model_evaluation import evaluate\nfrom sktime.forecasting.model_selection import (\n    ExpandingWindowSplitter,\n    ForecastingGridSearchCV,\n    SlidingWindowSplitter,\n    temporal_train_test_split,\n)\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom sktime.forecasting.theta import ThetaForecaster\nfrom sktime.forecasting.trend import PolynomialTrendForecaster\nfrom sktime.transformations.series.detrend import Deseasonalizer, Detrender\nfrom sktime.forecasting.theta import ThetaForecaster\nfrom statsmodels.tsa.api import STLForecast\nfrom sktime.forecasting.bats import BATS\nfrom sktime.forecasting.tbats import TBATS\nfrom sktime.forecasting.croston import Croston\nfrom statsmodels.tsa.exponential_smoothing.ets import ETSModel\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n\n"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "Importing plotly failed. Interactive plots will not work.\n"}], "source": "\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport statistics\nimport random\nimport time\n\nfrom tqdm.auto import tqdm\n\nimport warnings\nfrom pyspark.sql import Window\nimport pyspark.sql.functions as f\n\nimport os,sys\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\nfrom dateutil.relativedelta import relativedelta\nimport numpy as np\nimport pandas as pd\nimport re\nimport os\nfrom sklearn.preprocessing import MinMaxScaler\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, TimeDistributed, Conv1D, MaxPooling1D, Flatten, Bidirectional, Input, Flatten, Activation, Reshape, RepeatVector, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nimport torch\nfrom sktime.forecasting.fbprophet import Prophet"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": "class dataprep:\n    def __init__(self, df, input_window=90, output_window=30, stride=1, start='2020-11-01'):\n        # le = preprocessing.LabelEncoder()\n        df['ds'] = pd.to_datetime(df['ds'], format='%Y-%m-%d')\n        start = pd.to_datetime(start, format='%Y-%m-%d') #+ relativedelta(months=months)\n        end = start + relativedelta(days=365 + 30) #+ relativedelta(months=months)\n        # start1=df.ds.sort_values()[0]\n        # end1=df.ds.sort_values()[len(df_panda)-1]\n        # if end1<end:end=end1\n        # if start1>start:start=start1\n        # print(start,end)\n        df = df[(df.ds >= start) & (df.ds <= end)]\n\n\n        df['query_idx'] = df['query_idx'].map(str)\n        df.sort_values('ds', inplace=True)\n        if df['ds'].iloc[-1]!=end:\n            last_date=df.iloc[-1,:]\n            last_date['ds']=end\n            last_date['daily_supply'] = 0\n            df = df.append(last_date)\n        if df['ds'].iloc[0]!=start:\n            first_date=df.iloc[0,:]\n            first_date['ds']=end\n            first_date['daily_supply'] = 0\n            df = df.append(first_date)\n        df.sort_values('ds', inplace=True)\n        df['grp'] = df.apply(\n            lambda x: '%s_%s_%s_%s' % (x['group_id'], x['query_idx'], x['SearchKeyValue'], x['LocationKeyValue']),\n            axis=1)\n        dts = (end - start).days + 1\n\n        #qr = df.groupby(['grp'])['daily_supply'].count()\n        # self.df_test=df[['ds','daily_supply','grp']]\n###############################\n        #qrs = list(qr[(dts - qr) <= 50].index)\n\n        #df = df.loc[df['grp'].isin(qrs)]\n###############################\n\n        df.sort_values('ds', inplace=True)\n        try:\n            df = df.set_index(['ds', 'grp']).unstack(fill_value=0). \\\n            asfreq('D', fill_value=0).stack(). \\\n            sort_index(level=1).reset_index()\n        except:\n            df = df.groupby(['ds', 'grp']).agg({'daily_supply':['sum'],'group_id':['first'],'query_idx':['first'], 'SearchKeyValue':['first'], 'LocationKeyValue':['first']})\n            df = df.unstack(fill_value=0). \\\n                asfreq('D', fill_value=0).stack(). \\\n                sort_index(level=1).reset_index()\n            df.columns = df.columns.get_level_values(0)\n        #cls = ['year', 'month', 'week', 'quarter', 'dayofweek', 'isWeekend', 'Trend', 'weeklyChange', 'priceMov',\n        #       'Season', 'isHoliday', 'isPromotion', 'dayofyear_sin', 'dayofyear_cos', 'month_sin', 'month_cos']\n        df.sort_values('ds', inplace=True)\n        self.df = df\n        self.test_split_date = pd.to_datetime((end + relativedelta(days=-30)).strftime('%Y-%m-%d'))\n        self.val_split_date = pd.to_datetime((self.test_split_date + relativedelta(days=-30)).strftime('%Y-%m-%d'))\n\n        self.df = self.df.drop(['group_id', 'query_idx', 'SearchKeyValue', 'LocationKeyValue'], axis=1)\n\n        self.df_trains = self.df[self.df.ds <= self.val_split_date]\n        self.df_tests = self.df[self.df.ds > self.test_split_date]\n        df_train_temp = self.df[self.df.ds <= self.test_split_date]\n\n        self.df_test_org = pd.pivot_table(self.df_tests, values='daily_supply', index=['ds'], columns=['grp'])\n        self.df_train_org = pd.pivot_table(df_train_temp, values='daily_supply', index=['ds'],columns=['grp'])  # ,aggfunc=np.sum\n\n        self.df_train_org.sort_index(inplace=True)\n        self.q_train = self.df_train_org.iloc[-14:, :].sum(axis=0) / np.sum(np.sum(self.df_train_org.iloc[-14:, :]))\n        self.q_test = self.df_test_org.sum(axis=0) / np.sum(np.sum(self.df_test_org))\n        del self.df_train_org\n        del self.df_trains\n        del self.df_tests\n        del df_train_temp\n        self.df = self.df.groupby('ds').agg(\n            {'daily_supply': 'sum'\n\n             })\n\n        self.inp = input_window\n        self.out = output_window\n        self.stride = stride\n\n    def _get_data(self):\n        Cols_minmax = ['daily_supply']\n        for cols in Cols_minmax:\n            if cols != 'shifted_annual_daily_supply':\n                self.scaler_fz = MinMaxScaler(feature_range=(-1, 1))\n                self.scaler_fz = self.scaler_fz.fit(self.df[cols].values.reshape(-1, 1))\n            if cols == 'daily_supply':\n                self.scaler_f = self.scaler_fz\n            self.df[cols] = self.scaler_fz.transform(self.df[cols].values.reshape(-1, 1))\n\n        self.df_train = self.df[self.df.index <= self.val_split_date]\n        self.df_val = self.df[(self.df.index > self.val_split_date) & (self.df.index <= self.test_split_date)]\n        self.df_test = self.df[self.df.index > self.test_split_date]\n\n        train_date, test_date = list(self.df_train.index), list(self.df_test.index)\n\n        average = 0\n\n        try: val_shape = self.df_val['daily_supply'].shape[0]\n        except:val_shape = self.df_val.shape[0]\n        self.df_train.reset_index(inplace=True, drop=True)\n        self.df_val.reset_index(inplace=True, drop=True)\n        self.df_test.reset_index(inplace=True, drop=True)\n\n\n        if self.inp > val_shape:\n            df_test = np.concatenate([self.df_train.iloc[-(self.inp - val_shape):], self.df_val, self.df_test], axis=0)\n            df_val = np.concatenate([self.df_train.iloc[-self.inp:], self.df_val], axis=0)\n        else:\n            df_test = np.concatenate([self.df_val.iloc[:, :][-self.inp:], self.df_test], axis=0)#$$$$$$need to change for more features\n            df_val = np.concatenate([self.df_train.iloc[-self.inp:], self.df_val], axis=0)#$$$$$$need to change for more features\n        self.df_test, self.df_val = df_test, df_val\n        df_test = pd.DataFrame(df_test)\n        try:df_test.columns = self.df_train.columns\n        except:df_test.name = self.df_train.name\n\n        df_val = pd.DataFrame(df_val)\n        try:df_val.columns = self.df_train.columns\n        except:df_val.name = self.df_train.name\n\n        self.df_val, self.df_test = df_val, df_test\n\n        X_train, Y_train = self.sliding_windows(self.df_train, seq_length_x=self.inp, seq_length_y=self.out,\n                                                time_step=self.stride)\n        X_val, Y_val = self.sliding_windows(self.df_val, seq_length_x=self.inp, seq_length_y=self.out,\n                                            time_step=self.stride)\n        X_test, Y_test = self.sliding_windows(self.df_test, seq_length_x=self.inp, seq_length_y=self.out,\n                                              time_step=self.stride)\n        return X_train, Y_train, X_val, Y_val, X_test, Y_test, self.df_test_org.values, self.q_train, self.q_test\n\n    def sliding_windows(self, data, seq_length_x, seq_length_y, time_step):\n        x = []\n        y = []\n        for i in range(0, len(data) - seq_length_x - seq_length_y + 1, time_step):\n            try:\n                _x = data.iloc[i:(i + seq_length_x), :].values\n                _y = data.loc[i + seq_length_x:i + seq_length_x + seq_length_y - 1,'daily_supply'].values\n            except:\n                _x = data.iloc[i:(i + seq_length_x)].values\n                _y = data.loc[i + seq_length_x:i + seq_length_x + seq_length_y - 1].values\n              # data.loc[i + seq_length_x:i + seq_length_x + seq_length_y,'daily_supply'].values\n            x.append(_x)\n            y.append(_y)\n        return np.array(x), np.array(y)\n"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": "class Data(Dataset):\n    def __init__(self, x, y):\n        np.random.seed(int(43))\n        super(Data, self).__init__()\n        # store the raw tensors\n        self._x = x\n        self._y = y\n\n    def __len__(self):\n    # a DataSet must know it size\n        return self._x.shape[0]\n\n    def __getitem__(self, index):\n        x = self._x[index, :]\n        y = self._y[index, :]\n        return x, y\n    \nclass LSTM(nn.Module):\n\n    def __init__(self, output_size, input_size=1, hidden_size=4, num_layers=1, dropout=.3,relu='False'):\n        super(LSTM, self).__init__()\n\n        self.output_size = output_size\n        self.num_layers = num_layers\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        self.relu=relu\n        # self.dp = nn.Dropout(dropout)\n        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n                            num_layers=num_layers, batch_first=True, dropout=dropout)  # .25\n        # self.batchnorm=nn.BatchNorm1d(hidden_size)\n        self.rel=nn.LeakyReLU(hidden_size)\n        # self.fc = nn.Linear(hidden_size, hidden_size)\n        # self.dp = nn.Dropout(.5)\n        self.fc=nn.Linear(hidden_size, output_size)\n    def forward(self, x):\n        for it in self.lstm.parameters():\n            w = it\n            break\n\n        h_0 = w.new_zeros(\n            self.num_layers, x.size(0), self.hidden_size, requires_grad=w.requires_grad).to(self.device)\n\n        c_0 = w.new_zeros(\n            self.num_layers, x.size(0), self.hidden_size, requires_grad=w.requires_grad).to(self.device)\n\n        # Propagate input through LSTM\n\n        h_out, _ = self.lstm(x, (h_0, c_0))\n        #h_out.requires_grad_()\n        h_out = h_out[:, -1, :]\n        h_out = h_out.reshape(-1, self.hidden_size)\n        #out = self.dp(h_out)\n\n        #out = self.batchnorm(h_out)\n        if self.relu=='True':\n            h_out=  self.rel(h_out)\n        # out=  self.fc(h_out)\n        # out = self.dp(out)\n        out = self.fc(h_out)\n        #out.requires_grad_()\n        return out\n\nclass LSTM1(nn.Module):\n\n    def __init__(self, output_size, input_size=1, hidden_size=4, num_layers=1,dropout=.1):\n        super(LSTM, self).__init__()\n\n        self.output_size = output_size\n        self.num_layers = num_layers\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n                            num_layers=num_layers, batch_first=True,dropout = .2)\n        self.batchnorm=nn.BatchNorm1d(hidden_size)\n        self.dp=nn.Dropout(dropout)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n\n    def forward(self, x):\n        for it in self.lstm.parameters():\n            w = it\n            break\n\n        h_0 = w.new_zeros(\n            self.num_layers, x.size(0), self.hidden_size, requires_grad=w.requires_grad).to(self.device)\n\n        c_0 = w.new_zeros(\n            self.num_layers, x.size(0), self.hidden_size, requires_grad=w.requires_grad).to(self.device)\n\n        # Propagate input through LSTM\n\n        h_out, _ = self.lstm(x, (h_0, c_0))\n        h_out.requires_grad_()\n        h_out = h_out[:, -1, :]\n        h_out = h_out.reshape(-1, self.hidden_size)\n        out=self.batchnorm(h_out)\n        out=self.dp(out)\n        out = self.fc(out)\n\n        out.requires_grad_()\n        return out\n    \n    \n\nclass LSTM3(nn.Module):\n\n    def __init__(self, output_size, input_size=1, hidden_size=4, num_layers=1,dropout=.1):\n        super(LSTM, self).__init__()\n\n        self.output_size = output_size\n        self.num_layers = num_layers\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n                            num_layers=num_layers, batch_first=True,dropout = dropout)\n        self.lstm2 = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size,\n                            num_layers=num_layers, batch_first=True,dropout = dropout)\n        self.batchnorm=nn.BatchNorm1d(hidden_size)\n        self.dp=nn.Dropout(dropout)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n\n    def forward(self, x):\n        for it in self.lstm1.parameters():\n            w = it\n            break\n\n        h_0 = w.new_zeros(\n            self.num_layers, x.size(0), self.hidden_size, requires_grad=w.requires_grad).to(self.device)\n\n        c_0 = w.new_zeros(\n            self.num_layers, x.size(0), self.hidden_size, requires_grad=w.requires_grad).to(self.device)\n        for it in self.lstm2.parameters():\n            w1 = it\n            break\n        h_1 = w1.new_zeros(\n            self.num_layers, x.size(0), self.hidden_size, requires_grad=w1.requires_grad).to(self.device)\n\n        c_1 = w1.new_zeros(\n            self.num_layers, x.size(0), self.hidden_size, requires_grad=w1.requires_grad).to(self.device)\n\n        torch.nn.init.xavier_normal_(h_0)\n        torch.nn.init.xavier_normal_(c_0)\n        torch.nn.init.xavier_normal_(h_1)\n        torch.nn.init.xavier_normal_(c_1)\n        # Propagate input through LSTM\n\n        h_out, _ = self.lstm1(x, (h_0, c_0))\n        h_out.requires_grad_()\n        #h_out = h_out[:, -1, :]\n        #h_out = h_out.reshape(-1, self.hidden_size)\n        \n        h_out1, _ = self.lstm2(h_out, (h_0, c_0))\n        h_out1.requires_grad_()\n        h_out1 = h_out1[:, -1, :]\n        h_out1 = h_out1.reshape(-1, self.hidden_size)\n        \n        out=self.batchnorm(h_out1)\n        out=self.dp(out)\n        out = self.fc(out)\n\n        out.requires_grad_()\n        return out\n"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": "def SMAPE(A, F):\n    smape_mean = 0\n    for i in range(A.shape[0]):\n        if A[i] == F[i]:\n            continue\n        smape_mean += 2 * np.abs(F[i] - A[i]) / (np.abs(A[i]) + np.abs(F[i]))\n    smape_mean = 1 / A.shape[0] * smape_mean\n    return smape_mean\ndef SMAPE2(A, F):\n    smape_mean = 0\n    for i in range(A.shape[0]):\n        if A[i] == F[i]:\n            continue\n        smape_mean += 2 * np.abs(F[i] - A[i]) / (np.abs(A[i]) + np.abs(F[i]))\n    smape_mean = 1 / A.shape[0] * smape_mean\n    return smape_mean\ndef WAPE(A, F):\n    return np.sum((np.abs(F - A))/np.sum(A))\ndef weightedSMAPE(A,F,wq):\n    res=0\n    print('number of smape: ',len(wq))\n    for i in range(len(wq)):\n        res+=wq[i]*SMAPE(A[:,i],F[:,i])\n    return res\ndef WAPE_fun(actual,predicted,w):\n    #w=query percentage in the cluster\n    wape_tot=0\n    #print('number of smape: ', len(w))\n    for i in range(len(w)):\n        if np.sum(actual[:,i].reshape(-1,1))<=0:\n            continue\n        wape_tot+=w[i]*WAPE(actual[:,i].reshape(-1,1),predicted[:,i].reshape(-1,1))\n    return wape_tot\ndef PE(A, F):\n    return np.abs(np.sum(F)-np.sum(A))/np.sum(A)\ndef PE_fun(actual,predicted,w):\n    wape_tot = 0\n    for i in range(len(w)):\n        if np.sum(actual[:, i].reshape(-1, 1)) <= 0:\n            continue\n        wape_tot += w[i] * PE(actual[:, i].reshape(-1, 1), predicted[:, i].reshape(-1, 1))\n    return wape_tot\ndef estimate(actual,pred,q):\n    if type(q)==pd.core.series.Series:q=q.values\n    #pred=pred * q.reshape(-1, )\n    smape_error = weightedSMAPE(actual, pred, q)\n    wape=WAPE_fun(actual,pred,q)\n    pe=PE_fun(actual,pred,q)\n    return smape_error,wape,pe\ndef metric_dist(A,F,q,metric='wape'):\n    res=[]\n    if metric=='wape':\n        for i in range(len(q)):\n            if np.sum(A[:, i].reshape(-1, 1)) <= 0:\n                continue\n            res.append(WAPE(A[:, i].reshape(-1, 1), F[:, i].reshape(-1, 1)))\n    elif metric=='smape':\n        for i in range(len(q)):\n            if np.sum(A[:, i].reshape(-1, 1)) <= 0:\n                continue\n            res.append(SMAPE(A[:, i].reshape(-1, 1), F[:, i].reshape(-1, 1)))\n    else:\n        for i in range(len(q)):\n            if np.sum(A[:, i].reshape(-1, 1)) <= 0:\n                continue\n            res.append(PE(A[:, i].reshape(-1, 1), F[:, i].reshape(-1, 1)))\n    return [np.percentile(res,50),np.percentile(res,80),np.percentile(res,90)]\ndef Estimate(actual,pred,metric='wape'):\n    if metric=='pe':return PE(actual,pred)\n    elif metric=='wape':return WAPE(actual.reshape(-1,1),pred.reshape(-1,1))\n    elif metric=='smape':return SMAPE(actual.reshape(-1,1),pred.reshape(-1,1))\n    else:\n        raise ValueError('incorrect metric is defined')\ndef myloss(A,F):\n    #loss=torch.mean(np.abs((actual.detach().numpy() - predict.detach().numpy()) / actual))\n    loss=torch.mean(1 / A.size(dim=0)* torch.sum(2 * torch.abs(F - A) / (torch.abs(A) + torch.abs(F))))\n    return loss#torch.from_numpy(np.asarray(loss))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": "def train_lstm(config,save_model=False):\n    num_epochs = config['num_epochs']\n    learning_rate = config['learning_rate']\n    window_size = 30\n    output_size = 30\n    relu=config['relu']\n    input_size = train_set_in.shape[2]\n    hidden_size = config['hidden_size']\n    num_layers = 1\n    drop_out = config['dropout']\n    batch_size=config['batch_size']\n    gamma=config['gamma']\n    optimizer_name=config['optimizer_name']\n    num_layers=config['num_layer']\n    #dropout1=config['dropout1']\n    lstm = trainLSTM(num_epochs, learning_rate, output_size, input_size, hidden_size, num_layers, drop_out,gamma,optimizer_name,relu)\n    lstm.train(train_set_in, train_set_out,val_set_in,val_set_out,test_in_set,test_out_set,batch_size=batch_size)\n    _,_, valid_smape, valid_wape,valid_pe= lstm.valid(val_set_in, val_set_out)\n    testing_predict, testing_truth, test_smape, test_wape,test_pe= lstm.valid(test_in_set, test_out_set)\n    #testing_predict, testing_truth, Wsmape, Wape, Wpe, smape_dis, wape_dis, pe_dis, smape_agg, wape_agg,pe_agg = lstm.valid(test_in_set, test_out_set,not_test=False,pred_is_req=True,df_test_org=df_test_org)\n    print('for sanity check', valid_smape, valid_wape,valid_pe)\n    return [test_smape, test_wape,test_pe,lstm,testing_predict.reshape(-1,),valid_wape]#[training_smape*100,valid_smape*100, testing_smape*100,training_wape*100,valid_wape*100, testing_wape*100,lstm]\ndef running_lstm(config,save_model=False,Device='cpu'):\n    seed = 43\n    torch.manual_seed(seed)\n    num_epochs = config['num_epochs']\n    learning_rate = config['learning_rate']\n    window_size = 30\n    output_size = 30\n    batch_size=config['batch_size']\n    input_size = train_set_in.shape[2]\n    hidden_size = config['hidden_size']\n    num_layers = 1\n    drop_out=config['dropout']\n    gamma=config['gamma']\n    optimizer_name=config['optimizer_name']\n    num_layers=config['num_layer']\n    relu=config['relu']\n    #dropout1=config['dropout1']\n    lstm = trainLSTM(num_epochs, learning_rate, output_size, input_size, hidden_size, num_layers,drop_out,gamma,optimizer_name,relu)\n    #lstm.to(Device)\n    loss=lstm.train(train_set_in, train_set_out,val_set_in,val_set_out,test_in_set,test_out_set,batch_size=batch_size)\n    _,_, Wsmape, Wwape,Wpe = lstm.valid(val_set_in, val_set_out)\n    #testing_predict, testing_truth, testing_smape,testing_wape,smape_agg,wape_agg = lstm.valid(val_set_in, val_set_out)\n    return Wsmape,Wwape,Wpe"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": "# # # Tail clusters\ntest_group_id_list = [\n '/1005862/1007220/1044020/3392907',\n '/3944/77622/8375901/1230415/1230417',\n '/4171/645779/2165088/3723286',\n '/1334134/5899871/7445675/5456260',\n '/5428/1102183/8617474/1564368',\n '/2637/5249422/5222147',\n '/1334134/6355365/1285843/3783885',\n '/976759/976787/1001390/2523039/2119183',\n '/91083/2962826/8564995/2827548',\n '/4125/4134/1026285/9642310/1251682',\n '/1005862/1007220/1007399/6461098',\n '/5427/133283/9095197',\n '/1115193/1071967/1149380/5135111',\n '/976759/976794/5403011/9731488',\n '/4125/4134/2350251/1078395',\n '/5428/8018905/5374562/7304919/6954191',\n '/6197502/5702707/9458810/9028847',\n '/91083/1074769/6350088',\n '/5440/1001299/6148116',\n '/1334134/6172404/7659444/3836217',\n '/3920/1987289/582374/8324310/1974555',\n '/4125/4161/4165/1075707',\n '/976760/1414629/6578740/1310637',\n '/91083/1074765/9183778/3753296',\n '/3920/582053/584177/585913',\n '/976760/1005863/8121112',\n '/1105910/133161/1072306/5297453/5115891',\n '/976760/1005860/4157476/8398146/6630246',#\n '/2637/6749799/2053705',\n '/1072864/1067612/6801317/3181011/4259773',\n '/1085666/5859906/3311357',\n '/976760/9575239/2981737',\n '/2637/9489599/2275107',\n '/4125/4134/1078384/1078388',\n '/5440/202073/1749780/3053202/2610736',\n '/1105910/133161/1103334/3123349/9119436',\n '/1334134/6172404/4854691/2291474',\n '/4171/7357538/6711451/4147327',\n '/1334134/8495017/8541719',\n '/4125/546956/4128/4547850/2751930',\n '/1072864/1067618/1230774/8250798/6489704',\n '/1072864/1808984/4037404/9537015',#\n '/1085666/3316357/6852893/3607949',\n '/1229749/6562796/6709176',\n '/91083/1077064/8595272/2238009',#\n '/5440/9672181',\n '/1072864/1067618/1230774/8250798/6489704',\n '/1085666/3147628/8028415/6121950',#\n '/4044/103150/4038/7389568/2455597',#\n '/976760/1876667/7219872']"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": "start_month=['2020-09-01','2020-10-01','2020-11-01','2020-12-01','2021-01-01','2021-02-01','2021-03-01',\n            '2021-04-01','2021-05-01','2021-06-01','2021-07-01','2021-08-01']\nname=['September','October','November','December','January','February','March','April','May','June','July','August']"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "September  :  2020-09-01\nOctober  :  2020-10-01\nNovember  :  2020-11-01\nDecember  :  2020-12-01\nJanuary  :  2021-01-01\nFebruary  :  2021-02-01\nMarch  :  2021-03-01\nApril  :  2021-04-01\nMay  :  2021-05-01\nJune  :  2021-06-01\nJuly  :  2021-07-01\nAugust  :  2021-08-01\n"}], "source": "for i in range(len(name)):\n    print(name[i],' : ',start_month[i])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": "def objective(trial):\n    params={\n        #\"num_layers\":trial.suggest_int(\"num_layer\",1,2),\n        'optimizer_name':trial.suggest_categorical(\"optimizer_name\", [\"Adam\",'RMSprop','AdamW','SGD']),#\"Adam\",'RMSprop',\n        \"hidden_size\":trial.suggest_int('hidden_size',8,20),\n        'dropout':trial.suggest_uniform(\"dropout\",0,0.10),\n        #'dropout1':trial.suggest_uniform(\"dropout1\",0,0.3),\n        'learning_rate':trial.suggest_loguniform('learning_rate',1e-4,1e-0),\n        'num_epochs':trial.suggest_int('num_epochs',100,100),\n        'batch_size':trial.suggest_int('batch_size',32,64),\n        'gamma':trial.suggest_uniform(\"gamma\",0.8,1),\n        'num_layer':trial.suggest_int('num_layer',1,3),\n        'relu':trial.suggest_categorical('relu',['True','False'])}\n    temp_loss=running_lstm(params,save_model=False)\n    return temp_loss\n"}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [], "source": "class trainLSTM:\n    def __init__(self, num_epochs, learning_rate, output_size, input_size, hidden_size, num_layers, drop_out, gamma,\n                 optimizer_name, relu=False):\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        self.num_epochs = num_epochs\n        self.learning_rate = learning_rate\n        self.output_size = output_size\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.drop_out = drop_out\n        self.lstm = LSTM(output_size, input_size, hidden_size, num_layers, drop_out, relu)\n        self.lstm.to(self.device)\n        self.criterion = torch.nn.MSELoss()  # mean-squared error for regression: MSELoss  #L1Loss\n        self.gamma = gamma\n        self.optimizer_name = optimizer_name\n        self.optimizer = getattr(torch.optim, optimizer_name)(self.lstm.parameters(), lr=learning_rate)\n        self.val_loss_ep = []\n        self.train_loss_ep = []\n        self.test_loss_ep = []\n        # torch.optim.Adam(self.lstm.parameters(), lr=learning_rate)\n\n    def train(self, trainX, trainY, valX, valY, testX, testY, batch_size=32, n_epochs_stop=10):\n        # Train the model\n        # self.lstm.train()\n        trainX, trainY = trainX.to(self.device), trainY.to(self.device)\n        valX, valY = valX.to(self.device), valY.to(self.device)\n        DF = Data(trainX, trainY)\n        trainloader = DataLoader(dataset=DF, batch_size=batch_size, shuffle=True, num_workers=0)  ####\n        # Early stopping\n        last_loss = 1e10\n        patience = n_epochs_stop\n        trigger_times = 0\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', patience=75, factor=self.gamma)  #ExponentialLR(self.optimizer, gamma=self.gamma)#\n        clip = 5\n        PATH = \"best_mod_agg.pt\"\n        for epoch in range(self.num_epochs):\n            train_losses = 0\n            self.lstm.train()\n            start1 = time.time()\n            for x, y in trainloader:\n                # trainX.requires_grad_()\n                # print(x[0,:5,:])\n                outputs = self.lstm(x)\n\n                # obtain the loss function\n                loss = self.criterion(outputs.reshape(-1, 1),\n                                      y.reshape(-1, 1))  # myloss(outputs.reshape(-1, 1), y.reshape(-1, 1))#.\n                # loss.requires_grad_()\n                self.optimizer.zero_grad()\n                loss.backward()  # retain_graph=True\n                # nn.utils.clip_grad_norm_(self.lstm.parameters(), clip)\n                self.optimizer.step()\n                train_losses += loss.item()  # *x.size(0)\n            train_losses = train_losses / len(DF)\n            # if epoch % 5 == 0:\n            #    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item() / batch_size))\n            self.train_loss_ep.append(train_losses)\n\n            current_loss = self.eval(valX, valY)  # Estimate(valY.data.cpu().numpy(), output.data.cpu().numpy())\n            scheduler.step(current_loss)\n            self.val_loss_ep.append(current_loss)\n            test_loss = self.eval(testX, testY)\n            self.test_loss_ep.append(test_loss)\n            if current_loss > last_loss and epoch >= 50:\n                trigger_times += 1\n                # print('Trigger Times:', trigger_times)\n                if trigger_times >= patience:\n                    print('Early stopping!\\nStart to test process.', epoch)\n                    break\n            else:\n                trigger_times = 0\n                # torch.save({\n                #     'epoch': epoch,\n                #     'model_state_dict': self.lstm.state_dict(),\n                #     'optimizer_state_dict': self.optimizer.state_dict(),\n                #     'loss': train_losses,\n                #     'val_loss':current_loss\n                # }, PATH)\n                # print(self.lstm.state_dict()['lstm.bias_hh_l4'][:5])\n                torch.save({'model': self.lstm, 'val_loss': current_loss}, PATH)\n                # print(self.lstm.state_dict())\n            last_loss = current_loss\n            # print('Epoach : ', epoch, ' train_loss: ', train_losses, ' valid_loss: ', current_loss, 'test loss',\n            #      test_loss, 'time taken', time.time() - start1)\n        checkpoint = torch.load(PATH)\n        self.lstm = checkpoint['model']\n        # self.lstm.load_state_dict(checkpoint['model_state_dict'])\n        # self.lstm=LSTM(self.output_size, self.input_size, self.hidden_size, self.num_layers, self.drop_out)\n        # self.lstm.load_state_dict(checkpoint['model_state_dict'])\n        # self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        # print('lets check the model poaram')\n        # print(self.lstm.state_dict()['lstm.bias_hh_l4'][:5])\n        current_loss = checkpoint['val_loss']\n        return current_loss\n\n    def eval(self, valX, valY, batch_size=32):\n        valX, valY = valX.to(self.device), valY.to(self.device)\n        DFv = Data(valX, valY)\n        val_loader = torch.utils.data.DataLoader(dataset=DFv, batch_size=batch_size, shuffle=False)\n        val_loss = 0\n        self.lstm.eval()  # prep model for evaluation\n        with torch.no_grad():\n            for x, y in val_loader:\n                outputs = self.lstm(x)\n                loss = self.criterion(y, outputs)  # Estimate(valY.data.cpu().numpy(), outputs.data.cpu().numpy())\n                val_loss += loss.item()  # *batch_size#*x.size(0)\n            current_loss = val_loss / len(DFv)\n        return current_loss\n\n    def valid(self, testX, testY, not_test=True, pred_is_req=False, df_test_org=None):\n        # evaluate the model\n        self.lstm.eval()\n        testX, testY = testX.to(self.device), testY.to(self.device)\n        test_predict = self.lstm(testX)\n        data_predict = test_predict.data.cpu().numpy()\n        dataY_plot = testY.data.cpu().numpy()  # to(self.device)\n        data_predict1 = prep.scaler_f.inverse_transform(data_predict.T)\n        try:\n            dataY_plot = prep.scaler_f.inverse_transform(dataY_plot.T)\n        except:\n            dataY_plot = dataY_plot.reshape(dataY_plot.shape[0], dataY_plot.shape[1])\n            dataY_plot = prep.scaler_f.inverse_transform(dataY_plot.T)\n        data_predict1[data_predict1 < 0] = 0\n        if not_test:\n            smape, wape, pe = Estimate(dataY_plot, data_predict1, metric='smape'), Estimate(dataY_plot, data_predict1, metric='wape'), Estimate(dataY_plot, data_predict1, metric='pe')\n            print('wape is :', smape, wape, pe)\n            return data_predict1, dataY_plot, smape, wape, pe\n        else:\n            smape_agg, wape_agg, pe_agg = Estimate(dataY_plot, data_predict1, metric='smape'), Estimate(dataY_plot,data_predict1, metric='wape'), Estimate(\n                dataY_plot, data_predict1, metric='pe')\n            print('smape is: ', smape_agg, 'wape is: ', wape_agg, 'pe is:', pe_agg)\n            data_predict11 = np.tile(data_predict1.T, (q.shape[0], 1)).T\n            data_predict2 = data_predict11 * q.values.reshape(-1, 1).T\n            Wsmape, Wwape, Wpe = estimate(df_test_org, data_predict2, wq)\n            smape_dis, wape_dis, pe_dis = metric_dist(df_test_org, data_predict2, wq, metric='smape'), metric_dist( df_test_org, data_predict2, wq, metric='wape'), metric_dist(df_test_org, data_predict2, wq, metric='pe')\n        if pred_is_req:\n            data_predict_2 = data_predict.T\n            data_predict_2 = prep.scaler_f.inverse_transform(data_predict_2)\n            data_predict_2[data_predict_2 < 0] = 0\n        return data_predict_2, dataY_plot, Wsmape, Wwape, Wpe, smape_dis, wape_dis, pe_dis, smape_agg[\n            0], wape_agg, pe_agg\n"}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [], "source": "from torch.optim.lr_scheduler import ExponentialLR\n\nAggSmapeL=[]\nAggWapeL=[]\nAggPeL=[]\ntimes_org=[]\nTimesL=[]\nwape_agg_lstm=np.zeros(50, dtype=np.float64)\ndf_lstm=np.zeros((30,50), dtype=np.float64)"}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [], "source": "AggSmapeL=[0]*50\nAggWapeL=[0]*50\nAggPeL=[0]*50\ntimes_org=[0]*50\nTimesL=[0]*50"}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": "obs_grp=[39,38,30,20,33]"}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [], "source": "seed=43\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\nnp.random.seed(seed)  # Numpy module.\nrandom.seed(seed)  # Python random module.\ntorch.manual_seed(seed)\ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True"}, {"cell_type": "code", "execution_count": 26, "metadata": {"scrolled": true}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Running for the month of : September\ncluster id:  1\nBeginning the training...\nval shape is:  torch.Size([1, 90, 1]) torch.Size([1, 30])\ntrial run:  1\nwape is : [0.16151819] 0.15951401 0.01121351\nthe  smape is 0.1615181863307953 wape is 0.15951400995254517 and pe is 0.011213510297238827 and total is 0.11074856917063396\ndone!\ntrial run:  2\nwape is : [0.18684617] 0.18115751 0.080749124\nthe  smape is 0.18684616684913635 wape is 0.18115751445293427 and pe is 0.08074912428855896 and total is 0.14958426356315613\ntrial run:  3\nwape is : [0.16151819] 0.15951401 0.01121351\nthe  smape is 0.1615181863307953 wape is 0.15951400995254517 and pe is 0.011213510297238827 and total is 0.11074856917063396\ntrial run:  4\nwape is : [0.15236889] 0.14958872 0.05556028\nthe  smape is 0.15236888825893402 wape is 0.1495887190103531 and pe is 0.05556027963757515 and total is 0.11917263269424438\ntrial run:  5\nwape is : [0.17330033] 0.16665854 0.13839468\nthe  smape is 0.17330032587051392 wape is 0.16665853559970856 and pe is 0.13839468359947205 and total is 0.15945117672284445\ntrial run:  6\nwape is : [0.14151046] 0.13977791 0.025744645\nthe  smape is 0.14151045680046082 wape is 0.13977791368961334 and pe is 0.025744644924998283 and total is 0.10234434405962627\ndone!\ntrial run:  7\nwape is : [0.17330033] 0.16665854 0.13839468\nthe  smape is 0.17330032587051392 wape is 0.16665853559970856 and pe is 0.13839468359947205 and total is 0.15945117672284445\ntrial run:  8\nwape is : [0.15823123] 0.15391475 0.052644737\nthe  smape is 0.1582312285900116 wape is 0.15391474962234497 and pe is 0.05264473706483841 and total is 0.12159690260887146\ntrial run:  9\nwape is : [0.14057128] 0.13808656 0.0089327805\nthe  smape is 0.14057128131389618 wape is 0.13808655738830566 and pe is 0.008932780474424362 and total is 0.0958635409673055\ndone!\ntrial run:  10\nwape is : [0.14785403] 0.14567842 0.016676899\nthe  smape is 0.1478540301322937 wape is 0.14567841589450836 and pe is 0.016676899045705795 and total is 0.10340311129887898\ntrial run:  11\nwape is : [0.14057128] 0.13808656 0.0089327805\nthe  smape is 0.14057128131389618 wape is 0.13808655738830566 and pe is 0.008932780474424362 and total is 0.0958635409673055\ntrial run:  12\nwape is : [0.1375834] 0.13501509 0.01793488\nthe  smape is 0.1375834047794342 wape is 0.1350150853395462 and pe is 0.017934879288077354 and total is 0.09684446454048157\ntrial run:  13\nwape is : [0.13878556] 0.13659832 0.016853284\nthe  smape is 0.13878555595874786 wape is 0.13659831881523132 and pe is 0.01685328409075737 and total is 0.09741238753000896\ntrial run:  14\nwape is : [0.14988376] 0.1474364 0.024944155\nthe  smape is 0.14988376200199127 wape is 0.14743639528751373 and pe is 0.024944154545664787 and total is 0.10742143789927165\ntrial run:  15\nwape is : [0.13878556] 0.13659832 0.016853284\nthe  smape is 0.13878555595874786 wape is 0.13659831881523132 and pe is 0.01685328409075737 and total is 0.09741238753000896\ntrial run:  16\nwape is : [0.15440723] 0.15315187 0.037395794\nthe  smape is 0.15440723299980164 wape is 0.15315186977386475 and pe is 0.037395793944597244 and total is 0.11498496929804485\ntrial run:  17\nwape is : [0.14133768] 0.13887861 0.0013890383\nthe  smape is 0.14133767783641815 wape is 0.13887861371040344 and pe is 0.0013890382833778858 and total is 0.0938684344291687\ndone!\ntrial run:  18\nwape is : [0.13950552] 0.13709524 0.010817289\nthe  smape is 0.1395055204629898 wape is 0.13709524273872375 and pe is 0.010817289352416992 and total is 0.09580602248509724\ntrial run:  19\nwape is : [0.14133768] 0.13887861 0.0013890383\nthe  smape is 0.14133767783641815 wape is 0.13887861371040344 and pe is 0.0013890382833778858 and total is 0.0938684344291687\ntrial run:  20\nwape is : [0.14023311] 0.13784736 0.008979358\nthe  smape is 0.140233114361763 wape is 0.13784736394882202 and pe is 0.008979357779026031 and total is 0.09568660457928975\ntrial run:  21\nwape is : [0.21839958] 0.20645322 0.18551289\nthe  smape is 0.21839958429336548 wape is 0.20645321905612946 and pe is 0.18551288545131683 and total is 0.2034552296002706\ntrial run:  22\nwape is : [0.14170346] 0.13924983 0.0065588173\nthe  smape is 0.14170345664024353 wape is 0.13924983143806458 and pe is 0.006558817345649004 and total is 0.09583736459414165\ntrial run:  23\nwape is : [0.21839958] 0.20645322 0.18551289\nthe  smape is 0.21839958429336548 wape is 0.20645321905612946 and pe is 0.18551288545131683 and total is 0.2034552296002706\ntrial run:  24\nwape is : [0.13952374] 0.13709535 0.0016401343\nthe  smape is 0.13952374458312988 wape is 0.1370953470468521 and pe is 0.0016401342581957579 and total is 0.09275307257970174\ndone!\ntrial run:  25\nEarly stopping!\nStart to test process. 63\nwape is : [0.36495432] 0.31493002 0.26755655\nthe  smape is 0.3649543225765228 wape is 0.3149300217628479 and pe is 0.2675565481185913 and total is 0.3158136208852132\ntrial run:  26\nwape is : [0.1362276] 0.13249674 0.014105373\nthe  smape is 0.1362275928258896 wape is 0.13249674439430237 and pe is 0.014105373062193394 and total is 0.09427656730016072\ntrial run:  27\nEarly stopping!\nStart to test process. 63\nwape is : [0.36495432] 0.31493002 0.26755655\nthe  smape is 0.3649543225765228 wape is 0.3149300217628479 and pe is 0.2675565481185913 and total is 0.3158136208852132\ntrial run:  28\nwape is : [0.1374168] 0.13353433 0.013696023\nthe  smape is 0.1374167948961258 wape is 0.13353432714939117 and pe is 0.013696023263037205 and total is 0.09488237897555034\ntrial run:  29\nwape is : [0.13984352] 0.13689667 0.0069442536\nthe  smape is 0.13984352350234985 wape is 0.13689666986465454 and pe is 0.006944253575056791 and total is 0.09456147750218709\ntrial run:  30\nwape is : [0.13838036] 0.13576213 0.0038188782\nthe  smape is 0.13838036358356476 wape is 0.13576212525367737 and pe is 0.0038188782054930925 and total is 0.09265379110972087\ndone!\ntrial run:  31\nwape is : [0.13984352] 0.13689667 0.0069442536\nthe  smape is 0.13984352350234985 wape is 0.13689666986465454 and pe is 0.006944253575056791 and total is 0.09456147750218709\ntrial run:  32\nwape is : [0.14057724] 0.13828124 0.020883793\nthe  smape is 0.14057724177837372 wape is 0.13828124105930328 and pe is 0.020883793011307716 and total is 0.09991409381230672\nwape is : [0.13901703] 0.13655518 0.013520889\nwape is : [0.66543436] 0.5493375 0.54933757\nfor sanity check [0.13901703] 0.13655518 0.013520889\nval wape is:  0.13655517995357513\ntotal time  322.391802072525\nAgg SMAPE:  [0.66543436] Agg WAPE:  0.5493375 Agg PE:  0.54933757\ntotal time  322.391802072525\n===============================================\n"}], "source": "month=0\nprint('Running for the month of :',name[month])\n################LSTM##########################\n################LSTM##########################\nfor i in range(50):\n    if i not in [44,47,48]: \n        print('cluster id: ',i+1)\n        current_date = '20220831'\n        test_group_id = test_group_id_list[i]#'/3944/1060825/447913'\n        #for top cluster:\n        #path = f\"gs://adtech-artifacts/ds/forecast/month_eval/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_tft_data_{current_date}.csv\"\n        path = f\"gs://adtech-artifacts/ds/forecast/month_eval/tail_clusters/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_data_20220831.csv\"\n        # read in data\n        df_panda = spark.read.option(\"header\", True).csv(path).toPandas() # The usage should be the same as previous\n        df_panda=df_panda[['ds', 'group_id', 'query_idx', 'SearchKeyValue',  'LocationKeyValue', 'daily_supply']]\n        #df_panda=spark.read.csv(\"gs://adtech-artifacts/ds/forecast/query_taxonomy_sample_Addedfeatures_cluster_\"+str(i)+\".csv\",header=True).toPandas()\n        Cols=['daily_supply']#, 'shifted_annual_daily_supply','Trend_Var','dayofyear_sin', 'dayofyear_cos', 'month_sin', 'month_cos']\n        for cols in Cols:  \n            df_panda[cols]=df_panda[cols].astype(float,errors='raise')\n        start=time.time()\n        seed=43\n        start=time.time()\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        minError=1e10\n        param = {\n            \"learning_rate\": [.3,.03,.003,.0003],\n            \"hidden_size\": [10,20],\n            'batch_size': [32],\n            'dropout': [0.05,.1],\n            'gamma': [0.9],\n            'optimizer_name': ['AdamW'],\n            'num_layer': [1,2],\n            'relu': [False]\n        }\n        prep = dataprep(df_panda, input_window=90, output_window=30, stride=1,start=start_month[month])\n        print('Beginning the training...')\n        train_set_in, train_set_out, val_set_in, val_set_out, test_in_set, test_out_set,df_test_org,  q,wq = prep._get_data()\n        #print('print len of q, value and wq value',len(q),q[0],wq[0])\n        train_set_in = torch.from_numpy(train_set_in.astype(np.float64)).float()\n        train_set_out = torch.from_numpy(train_set_out.astype(np.float64)).float()\n        val_set_in = torch.from_numpy(val_set_in.astype(np.float64)).float()\n        val_set_out = torch.from_numpy(val_set_out.astype(np.float64)).float()\n        test_in_set = torch.from_numpy(test_in_set.astype(np.float64)).float()\n        test_out_set = torch.from_numpy(test_out_set.astype(np.float64)).float()\n        print('val shape is: ',val_set_in.shape, val_set_out.shape)\n        end1=time.time()\n        if len(train_set_in.shape) < 3:\n            train_set_in = train_set_in.reshape(train_set_in.shape[0], train_set_in.shape[1], 1)\n            val_set_in = val_set_in.reshape(val_set_in.shape[0], val_set_in.shape[1], 1)\n            test_in_set = test_in_set.reshape(test_in_set.shape[0], test_in_set.shape[1], 1)\n            test_out_set=test_out_set.reshape(test_out_set.shape[0], test_out_set.shape[1])\n        counter=1\n        for lr in param['learning_rate']:\n            for h_s in param['hidden_size']:\n                for b_s in param['batch_size']:\n                    for dp in param['dropout']:\n                        for opt in param['optimizer_name']:\n                            for n_l in param['num_layer']:\n                                for act in param['relu']:\n                                    for gm in param['gamma']:\n                                        config = {\n                                            \"num_epochs\": 200,\n                                            \"learning_rate\": lr,\n                                            \"hidden_size\": h_s,\n                                            \"window_size\": 30,\n                                            \"output_size\": 30,\n                                            'batch_size': b_s,\n                                            'dropout': dp,\n                                            'dropout1': 0,\n                                            'gamma': gm,\n                                            'optimizer_name': opt,\n                                            'num_layer': n_l,\n                                            'relu': act\n                                        }\n                                        torch.manual_seed(seed)\n                                        print('trial run: ',counter)\n                                        counter+=1\n                                        e1,e2,e3 = running_lstm(config, save_model=True)\n                                        e1=e1[0]\n                                        print('the  smape is {} wape is {} and pe is {} and total is {}'.format(e1,e2,e3,(e1+e2+e3)/3))\n                                        #print('the config is {}',config)\n                                        if (e1+e2+e3)/3<minError:\n                                            minError=(e1+e2+e3)/3\n                                            best_config=config\n                                            print('done!')\n        end = time.time()\n        #print('thus best config is', best_config)\n        torch.manual_seed(seed)\n        smape_agg, wape_agg, pe_agg, model, df_lstm[:,i],wape_agg_lstm[i] = train_lstm(best_config, save_model=True)\n        print('val wape is: ',wape_agg_lstm[i])\n        print('total time ', end - end1)\n        print('Agg SMAPE: ',smape_agg,'Agg WAPE: ',wape_agg,'Agg PE: ',pe_agg)\n        print('total time ', end - end1)\n        #print('time needed to run the model: ',end-start)\n        AggSmapeL.append(smape_agg)\n        AggWapeL.append(wape_agg)\n        AggPeL.append(pe_agg)\n        TimesL.append(end-end1)\n        times_org.append(end1-start)\n    print('===============================================')\n    "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#df_panda.to_csv('df_36_noF.csv', index=False)"}, {"cell_type": "code", "execution_count": 27, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "TimesL= [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 322.391802072525]\nAggSmapeL= [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, array([0.66543436], dtype=float32)]\nAggWapeL= [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.5493375]\nAggPeL= [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.54933757]\ntimes_org= [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.15117454528808594]\n"}], "source": "print('TimesL=',TimesL)\nprint('AggSmapeL=',AggSmapeL)\nprint('AggWapeL=',AggWapeL)\nprint('AggPeL=',AggPeL)\nprint('times_org=',times_org)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "print('Running for the month of :',name[month])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#df_panda.to_csv('df_36.csv', index=False)"}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [], "source": "class statModels:\n    def __init__(self):\n        # self.df_train=df\n        self.models = {}\n        self.sc = MinMaxScaler(feature_range=(0, 1))\n        self.clus_wape={}\n        self.clus_smape={}\n        self.clus_pe={}\n        self.wape_Weighted={}\n        self.stores_times={}\n        self.dfStore=None\n    def arimamodel(self, timeseriesarray):\n        autoarima_model = pmd.auto_arima(timeseriesarray, start_p=0, d=0, trace=1,\n                                         suppress_warnings=True, seasonal=True, m=12, D=1,\n                                         start_q=0, error_action='ignore', stationary=True, njob=-1,\n                                         test=\"adf\", stepwise=True)\n        return autoarima_model\n\n    def preprocessing(self, df_train):\n        temp = df_train  # np.log(df_train)\n        temp = temp.reshape((-1, 1))\n        return self.sc.fit_transform(temp)\n\n    def postprocess(self, prediction, df_test_org,dfx_test, q, wq, ensemble=False):\n        # prediction=np.exp(prediction)\n        if type(prediction)==pd.core.frame.DataFrame:\n            prediction = prediction.values.reshape(-1, 1)\n        else:prediction = prediction.reshape(-1, 1)\n        if not ensemble:\n            prediction = self.sc.inverse_transform(prediction)\n        re_elements = wq.shape[0]  # testY.shape[-1]\n        testing_predict = prediction.reshape(-1, )\n        #else:testing_predict = prediction\n        clus_wape,clus_smape,clus_pe=WAPE(dfx_test,testing_predict),SMAPE(dfx_test,testing_predict),PE(dfx_test,testing_predict)\n        \n        testing_predict = np.tile(testing_predict, (re_elements, 1))\n        testing_predict = testing_predict.T     \n        testing_predict *= q\n\n#         Wsmape, Wwape,Wpe = estimate(df_test_org.values, testing_predict, wq)\n#         smape_dis,wape_dis,pe_dis=metric_dist(df_test_org.values, testing_predict, wq,metric='smape'),metric_dist(df_test_org.values, testing_predict, wq,metric='wape') ,metric_dist(df_test_org.values, testing_predict, wq,metric='pe')\n        return clus_smape,clus_wape,clus_pe, prediction  # ,testing_predict\n\n    def training_infer(self, df_train, df_test, df_test_org,q, wq, forecast_period=30,lstm_pred=None):\n        '''\n        df_test: pandas (shape:(:,1))\n        df_test_org: pandas (shape(:,no_ts))\n        '''\n\n        # self.df_train_log,self.df_test_log=test_train_spl(self.df,ratio=.8)\n        ###\n        fh = np.arange(forecast_period) + 1\n        ###\n        df_train['daily_supply'] = self.preprocessing(df_train.values)\n        self.df_train = df_train\n        # print(df_train['daily_supply'])\n        df_store = df_test.to_frame()\n        df_store = df_store.rename(columns={0: 'daily_supply'})\n        ###################################################\n        #df_store['lstm']=lstm_pred\n        #print('naive')\n        # naive\n        start = time.time()\n        forecaster=NaiveForecaster(strategy=\"last\")\n        forecaster.fit(self.df_train)\n        y_pred = forecaster.predict(fh)\n        df_store['naive'] = y_pred.values\n        # print(df_store['snaive'])\n        # print(df_store)\n        clus_smape,clus_wape,clus_pe,df_store['naive'] = self.postprocess(df_store['naive'].values, df_test_org,df_test,q, wq)\n        \n        end = time.time()\n        self.clus_smape['naive']=clus_smape\n        self.clus_wape['naive']=clus_wape\n        self.clus_pe['naive']=clus_pe\n        self.stores_times['naive'] = end - start\n        self.models['naive'] = forecaster\n        \n\n        #print('snaive')\n        # snaive\n        start = time.time()\n#         y = self.pysnaive(self.df_train, 52, forecast_period)[1].iloc[:forecast_period]\n#         df_store['snaive'] = y.values\n        forecaster1=NaiveForecaster(strategy=\"last\",sp=12)\n        forecaster1.fit(self.df_train)\n        y_pred = forecaster1.predict(fh)\n        clus_smape,clus_wape,clus_pe,df_store['snaive'] = self.postprocess(y_pred.values, df_test_org,df_test,q, wq)\n        \n        end = time.time()\n        self.clus_smape['snaive']=clus_smape\n        self.clus_wape['snaive']=clus_wape\n        self.clus_pe['snaive']=clus_pe\n        \n        self.stores_times['snaive'] = end - start\n        self.models['snaive'] = forecaster1\n        \n        #print('arima')\n        start = time.time()\n        try: \n            model_fit = ARIMA(self.df_train, order=(1, 1, 1)).fit()\n            df_store['arima'], _, _ = model_fit.forecast(forecast_period)  # 95% conf\n            print('done')\n        except (np.linalg.linalg.LinAlgError,ValueError):\n            model_fit = self.arimamodel(self.df_train)\n            df_store['arima']=model_fit.predict(forecast_period)\n            print('done3')\n#         else:\n#             model_fit = ARIMA(self.df_train, order=(0, 0, 0)).fit()\n#             df_store['arima']=model_fit.predict(forecast_period)\n#             print('done3')\n            \n        \n        #df_store['arima']= model_fit.forecast(forecast_period).values  # 95% conf\n        clus_smape,clus_wape,clus_pe,df_store['arima'] = self.postprocess(df_store['arima'].values, df_test_org,df_test,q, wq)\n        \n        end = time.time()\n\n        self.models['arima'] = model_fit\n        # self.stores_mape['arima'] = mape\n        self.clus_smape['arima']=clus_smape\n        self.clus_wape['arima']=clus_wape\n        self.clus_pe['arima']=clus_pe\n        \n        self.stores_times['arima'] = end - start\n        self.models['arima'] = model_fit\n\n        #print('ses')\n        # ses\n        start = time.time()\n        fit3 = SimpleExpSmoothing(self.df_train, initialization_method=\"estimated\").fit()\n        df_store['ses'] = fit3.forecast(forecast_period).values\n        clus_smape,clus_wape,clus_pe,df_store['ses'] = self.postprocess(df_store['ses'].values, df_test_org,df_test,q, wq)\n        \n        end = time.time()\n        self.models['ses'] = fit3\n\n        #self.stores_mape['ses'] = mape\n        self.clus_smape['ses']=clus_smape\n        self.clus_wape['ses']=clus_wape\n        self.clus_pe['ses']=clus_pe\n        self.stores_times['ses'] = end - start\n        #print('holts')\n        # holts\n        start = time.time()\n        try:\n            fit3 = Holt(self.df_train, damped_trend=True, exponential=True, initialization_method=\"estimated\").fit(\n                smoothing_level=0.8, smoothing_trend=0.1)\n        except (np.linalg.linalg.LinAlgError,ValueError):\n            fit3 = Holt(self.df_train, damped_trend=True, exponential=False, initialization_method=\"estimated\").fit(\n                smoothing_level=0.8, smoothing_trend=0.1)\n        df_store['holts'] = fit3.forecast(forecast_period).values\n        clus_smape,clus_wape,clus_pe,df_store['holts'] = self.postprocess(df_store['holts'].values, df_test_org,df_test,q, wq)\n        \n        end = time.time()\n\n        self.models['holts'] = fit3\n        #self.stores_mape['holts'] = mape\n        self.clus_smape['holts']=clus_smape\n        self.clus_wape['holts']=clus_wape\n        self.clus_pe['holts']=clus_pe\n        self.stores_times['holts'] = end - start\n        #print('Exp_smoothing')\n        # Exp_smoothing\n        start = time.time()\n        try:\n            fit4 = ExponentialSmoothing(self.df_train, seasonal_periods=52, trend=\"add\", seasonal=\"add\",\n                                        damped_trend=True, use_boxcox=True, initialization_method=\"estimated\", ).fit()\n        except:\n            fit4 = ExponentialSmoothing(self.df_train, seasonal_periods=52, trend=\"add\", seasonal=\"add\",\n                                        damped_trend=True, use_boxcox=False, initialization_method=\"estimated\", ).fit()\n        df_store['Es'] = fit4.forecast(forecast_period).values\n        clus_smape,clus_wape,clus_pe,df_store['Es'] = self.postprocess(df_store['Es'].values, df_test_org,df_test,q, wq)\n        \n        end = time.time()\n        self.models['Es'] = fit4\n        #self.stores_mape['Es'] = mape\n        self.clus_smape['Es']=clus_smape\n        self.clus_wape['Es']=clus_wape\n        self.clus_pe['Es']=clus_pe\n        self.stores_times['Es'] = end - start\n\n        \n        #print('Auto_ETS')\n        # Auto_ETS\n        start = time.time()\n        try: \n            forecaster = AutoETS(auto=True, sp=12, n_jobs=-1, error='add', trend='True', damped_trend='True', seasonal='add')\n            forecaster.fit(self.df_train)\n        except (ValueError):\n            forecaster = AutoETS(auto=False, sp=7, n_jobs=-1, error='add')\n            forecaster.fit(self.df_train)\n        df_store['auto_ets'] = forecaster.predict(fh).values\n        clus_smape,clus_wape,clus_pe,df_store['auto_ets'] = self.postprocess(df_store['auto_ets'].values, df_test_org,df_test,q, wq)\n        \n        end = time.time()\n        self.models['auto_ets'] = forecaster\n        #self.stores_mape['auto_ets'] = mape\n        self.clus_smape['auto_ets']=clus_smape\n        self.clus_wape['auto_ets']=clus_wape\n        self.clus_pe['auto_ets']=clus_pe\n        self.stores_times['auto_ets'] = end - start\n        \n        #theta\n        start = time.time()\n        forecaster4 = ThetaForecaster(sp=52, deseasonalize=False)\n        forecaster4.fit(self.df_train)\n        df_store['theta'] = forecaster4.predict(fh).values\n        clus_smape,clus_wape,clus_pe,df_store['theta'] = self.postprocess(df_store['theta'].values, df_test_org,df_test,q, wq)\n        \n        end = time.time()\n\n        self.models['theta'] = forecaster4\n        #self.stores_mape['theta'] = mape\n        self.clus_smape['theta']=clus_smape\n        self.clus_wape['theta']=clus_wape\n        self.clus_pe['theta']=clus_pe\n\n        self.stores_times['theta'] = end - start\n\n        #print('Croston')\n        # Croston\n        start = time.time()\n        forecaster5 = Croston(smoothing=.2)\n        forecaster5.fit(self.df_train)\n        df_store['croston'] = forecaster5.predict(fh).values\n        clus_smape,clus_wape,clus_pe,df_store['croston'] = self.postprocess(df_store['croston'].values, df_test_org,df_test,q, wq)\n        \n        end = time.time()\n\n        self.models['croston'] = forecaster5\n        #self.stores_mape['croston'] = mape\n        self.clus_smape['croston']=clus_smape\n        self.clus_wape['croston']=clus_wape\n        self.clus_pe['croston']=clus_pe\n        self.stores_times['croston'] = end - start\n\n        #prophet\n        start = time. time()\n        df_train=self.df_train.squeeze()\n        df_train.index.name='Period'\n        df_train.index=df_train.index.to_timestamp()\n        forecaster6 = Prophet(seasonality_mode='additive',n_changepoints=int(len(df_train) / 12),\n            add_country_holidays={'country_name': 'US'},yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=True)\n        forecaster6.fit(df_train)\n        df_store['prophet'] = forecaster6.predict(fh).values\n        clus_smape,clus_wape,clus_pe,df_store['prophet'] = self.postprocess(df_store['prophet'].values, df_test_org,df_test,q, wq)\n        \n        end = time.time()\n\n        self.models['prophet'] = forecaster6\n        # self.stores_mape['croston'] = mape\n        self.clus_smape['prophet']=clus_smape\n        self.clus_wape['prophet']=clus_wape\n        self.clus_pe['prophet']=clus_pe\n        self.stores_times['prophet'] = end - start\n\n        #print('Average')\n        # Average of stats models\n        tog_time = np.sum(list(self.stores_times.values()))\n        start = time.time()\n        df_temp = df_store.drop(['daily_supply'], axis=1)  # df_store.loc[:, df_store.columns != 'daily_supply']\n        df_store['Avg_ensemble'] = df_temp.mean(axis=1).values\n\n        clus_smape,clus_wape,clus_pe,df_store['Avg_ensemble'] = self.postprocess(df_store['Avg_ensemble'].values, df_test_org,df_test,q, wq,ensemble=True)\n        \n        end = time.time()\n        #self.stores_mape['Avg_ensemble'] = mape\n        self.clus_smape['Avg_ensemble']=clus_smape\n        self.clus_wape['Avg_ensemble']=clus_wape\n        self.clus_pe['Avg_ensemble']=clus_pe\n        self.stores_times['Avg_ensemble'] = (end - start) + tog_time-self.stores_times['Lstm']#%%%%%%%%%%%%\n        self.dfStore=df_store\n\n    def weighted_avg(self,df_train, df_test,df_val_org, df_test_org,q, wq, forecast_period=60,lstm_pred=None):\n        \n        fh = np.arange(forecast_period) + 1\n        ###\n        df_train['daily_supply'] = self.preprocessing(df_train.values)\n        self.df_trainW = df_train\n        # print(df_train['daily_supply'])\n        df_store = df_test.to_frame()\n        df_store = df_store.rename(columns={0: 'daily_supply'})\n        #############################################\n        start = time.time()\n        forecaster=NaiveForecaster(strategy=\"last\")\n        forecaster.fit(self.df_trainW)\n        y_pred = forecaster.predict(fh)\n\n        smape,wape,pe,_= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq)\n        self.wape_Weighted['naive'] = (smape+wape+pe)/3\n\n        \n\n        #print('snaive')\n        # snaive\n       # start = time.time()\n#         y = self.pysnaive(self.df_train, 52, forecast_period)[1].iloc[:forecast_period]\n#         df_store['snaive'] = y.values\n        forecaster1=NaiveForecaster(strategy=\"last\",sp=12)\n        forecaster1.fit(self.df_trainW)\n        y_pred = forecaster1.predict(fh)\n        \n        smape,wape,pe,_= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq)\n        self.wape_Weighted['snaive'] = (smape+wape+pe)/3\n        \n        #print('arima')\n        #start = time.time()\n        try: \n            model_fit = ARIMA(self.df_trainW, order=(1, 1, 1)).fit()\n            y_pred, _, _ = model_fit.forecast(forecast_period)  # 95% conf\n        except (np.linalg.linalg.LinAlgError):\n            model_fit = self.arimamodel(self.df_trainW)\n            y_pred=model_fit.predict(forecast_period)\n        \n        smape,wape,pe,_= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq)\n        self.wape_Weighted['arima'] = (smape+wape+pe)/3\n    \n\n        fit3 = SimpleExpSmoothing(self.df_trainW, initialization_method=\"estimated\").fit()\n        y_pred = fit3.forecast(forecast_period).values\n        \n        smape,wape,pe,_= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq)\n        self.wape_Weighted['ses'] = (smape+wape+pe)/3\n        \n        #print('holts')\n        # holts\n        start = time.time()\n        try:\n            fit3 = Holt(self.df_trainW, damped_trend=True, exponential=True, initialization_method=\"estimated\").fit(\n                smoothing_level=0.8, smoothing_trend=0.1)\n        except (np.linalg.linalg.LinAlgError,ValueError):\n            fit3 = Holt(self.df_trainW, damped_trend=True, exponential=False, initialization_method=\"estimated\").fit(\n                smoothing_level=0.8, smoothing_trend=0.1)\n        y_pred = fit3.forecast(forecast_period).values\n        \n        smape,wape,pe,_= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq)\n        self.wape_Weighted['holts'] = (smape+wape+pe)/3\n        # Exp_smoothing\n        #start = time.time()\n        try:\n            fit4 = ExponentialSmoothing(self.df_trainW, seasonal_periods=52, trend=\"add\", seasonal=\"add\",\n                                        damped_trend=True, use_boxcox=True, initialization_method=\"estimated\", ).fit()\n        except:\n            fit4 = ExponentialSmoothing(self.df_trainW, seasonal_periods=52, trend=\"add\", seasonal=\"add\",\n                                        damped_trend=True, use_boxcox=False, initialization_method=\"estimated\", ).fit()\n        y_pred = fit4.forecast(forecast_period).values\n        \n        smape,wape,pe,_= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq)\n        self.wape_Weighted['Es'] = (smape+wape+pe)/3\n\n        \n        #print('Auto_ETS')\n        # Auto_ETS\n        #start = time.time()\n        try:\n            forecaster = AutoETS(auto=True, sp=12, n_jobs=-1, error='add', trend='True', damped_trend='True', seasonal='add')\n            forecaster.fit(self.df_trainW)\n        except (ValueError):\n            forecaster = AutoETS(auto=False, sp=7, n_jobs=-1, error='add')\n            forecaster.fit(self.df_trainW)\n        y_pred = forecaster.predict(fh).values\n        \n        smape,wape,pe,_= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq)\n        self.wape_Weighted['auto_ets'] = (smape+wape+pe)/3\n        \n        #theta\n        #start = time.time()\n        forecaster4 = ThetaForecaster(sp=52, deseasonalize=False)\n        forecaster4.fit(self.df_trainW)\n        y_pred = forecaster4.predict(fh).values\n        \n        smape,wape,pe,_= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq)\n        self.wape_Weighted['theta'] = (smape+wape+pe)/3\n\n        #print('Croston')\n        # Croston\n        #start = time.time()\n        forecaster5 = Croston(smoothing=.2)\n        forecaster5.fit(self.df_trainW)\n        y_pred = forecaster5.predict(fh).values\n        \n        smape,wape,pe,_= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq)\n        self.wape_Weighted['croston'] = (smape+wape+pe)/3\n\n        #prophet\n        #start = time. time()\n        df_train=self.df_trainW.squeeze()\n        df_train.index.name='Period'\n        df_train.index=df_train.index.to_timestamp()\n        forecaster6 = Prophet(seasonality_mode='additive',n_changepoints=int(len(df_train) / 12),\n            add_country_holidays={'country_name': 'US'},yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=True)\n        forecaster6.fit(df_train)\n        y_pred = forecaster6.predict(fh).values\n        \n        smape,wape,pe,_= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq)\n        self.wape_Weighted['prophet'] = (smape+wape+pe)/3\n        \n        store_error = self.wape_Weighted.copy()\n\n        for k, v in self.wape_Weighted.items():\n            store_error[k] = 1 / v\n        norms = list(store_error.values())\n        norm = 0\n        for i in norms: norm += i\n        for k, v in store_error.items():\n            store_error[k] = v / norm\n        self.store_error = store_error\n        #print('weighted')\n        # weighted average\n        #del self.dfStore['daily_supply']\n        print(self.store_error)\n        subset = self.dfStore.drop(['daily_supply', 'Avg_ensemble'], axis=1)#.columns\n        self.dfStore['Weighted_ensemble'] = (subset * store_error).sum(1).values\n        clus_smape,clus_wape,clus_pe,self.dfStore['Weighted_ensemble'] = self.postprocess(self.dfStore['Weighted_ensemble'].values, df_test_org,df_test,q, wq,ensemble=True)\n        \n        end = time.time()\n        self.clus_smape['Weighted_ensemble']=clus_smape\n        self.clus_wape['Weighted_ensemble']=clus_wape\n        self.clus_pe['Weighted_ensemble']=clus_pe\n        self.stores_times['Weighted_ensemble'] = (end - start)#+self.stores_times['Lstm']\n        #self.dfStore['Weighted_ensemble']=df_store['Weighted_ensemble'].values\n    \n"}, {"cell_type": "code", "execution_count": 29, "metadata": {}, "outputs": [], "source": "def _generate_train_test_data(df, window_size=30,val_req=False,start='2020-11-01'):\n    start = pd.to_datetime(start, format='%Y-%m-%d') #+ relativedelta(months=months)\n    end = start + relativedelta(days=365 + 30) #+ relativedelta(months=months)    \n    df3 = df.astype({'query_idx': 'string'})\n    df3.sort_values('ds', inplace=True)\n    df1 = pd.pivot_table(df3, values='daily_supply', index=['ds'],\n                         columns=['group_id', 'query_idx', 'SearchKeyValue', 'LocationKeyValue'])\n\n    df1.columns = ['_'.join(col) for col in df1.columns.values]\n    print('Total TS before filter: ', df1.shape[1])\n\n    df1.index = pd.to_datetime(df1.index)\n\n#     start = df1.index[0]#****\n#     end = df1.index[-1]#*****\n            \n    \n    #start,end=pd.to_datetime('2020-05-02',format='%Y-%m-%d'),pd.to_datetime('2022-05-02',format='%Y-%m-%d')\n#     start1=df1.index.sort_values()[0]\n#     end1=df1.index.sort_values()[len(df1)-1]\n#     #print(df1.index.sort_values())\n#     if end1<end:end=end1\n#     if start1>start:start=start1\n    df1=df1[(df1.index>=start) & (df1.index<=end)]\n    df1.sort_index(inplace=True)\n\n    train_test_split = end - pd.to_timedelta(30, unit='d')  ############30 needs to be a variable\n    # start_temp2 = pd.to_datetime((train_test_split + relativedelta(days=-4 * window_size)).strftime('%Y-%m-%d'))\n\n    end_train = pd.to_datetime((train_test_split).strftime('%Y-%m-%d'))\n    start_test = pd.to_datetime((train_test_split + relativedelta(days=1)).strftime('%Y-%m-%d'))\n    #print(df1.index.unique())\n    dates = pd.date_range(start=start, end=end, freq='D').format()\n    s = pd.Series(np.nan, index=pd.date_range(start, end, freq='D'))\n    #df1['ds']=pd.to_datetime(df_['ds'])\n    #df_.set_index('ds',inplace=True)\n    df1=pd.concat([df1,s[~s.index.isin(df1.index)]]).sort_index()\n    \n    #print(df1)\n    #del df1['0']\n    #dates = pd.date_range(start=start, end=end, freq='D').format()\n    #df1=df1.reindex(dates, fill_value=0)\n    df1.index = pd.to_datetime(df1.index)\n    df1 = df1.fillna(0)\n#########################    \n    #df1 = df1.loc[:, df1.isin([0]).sum(axis=0) <= 50]\n########################\n    #     df1=df1.loc[:,df1.notna().sum()>90]\n    #     df1 = df1.fillna(0)\n    del df1[0]\n    # impute_missing_time_modify(temp, train_test_split, end, 4, train=True)\n    df_train, df_test = df1[(df1.index <= end_train)], df1[df1.index >= start_test]\n    # temp_train, temp_test = temp[temp['ds'] < train_test_split], temp[temp['ds'] >= train_test_split]\n    print('Total TS after filter: ', df1.shape[1])\n    df_train.sort_index(inplace=True)\n    #print(df_train.iloc[-14:,:])\n    \n    #df_train_temp=df_train_temp[(df_train_temp.index <= end_train)], df_train_temp[df_train_temp.index >= start_test]\n    \n    qtrain = df_train.iloc[-14:,:].sum(axis=0) / np.sum(df_train.iloc[-14:,:].sum(axis=0))\n    qtest = df_test.sum(axis=0) / np.sum(df_test.sum(axis=0))  # df_train.iloc[-30:,].sum(axis=0)/np.sum(df_train.iloc[-30:,].sum(axis=0))*100\n    # average=df_train.mean(axis=0).values\n    # df_train+=average\n    # df_test+= average\n    train_date, test_date = df_train.index, df_test.index\n    df_train.reset_index(inplace=True, drop=True)\n    df_test.reset_index(inplace=True, drop=True)\n    # df.values.sum()\n    # q=df.sum(axis=1)/np.sum(np.sum(df))\n    # print(q)\n    if val_req:\n        df_train,df_val=df_train.iloc[:-30,:],df_train.iloc[-30:,:]\n        return df_train, df_val,df_test, train_date.values[:-30], test_date.values, qtrain, qtest\n    return df_train, df_test, train_date.values, test_date.values, qtrain, qtest\n"}, {"cell_type": "code", "execution_count": 30, "metadata": {}, "outputs": [], "source": "Times=[]\nAggSmape=[]\nAggWape=[]\nAggPe=[]\n"}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [], "source": "# month=0\n# start_month[month]"}, {"cell_type": "code", "execution_count": 33, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "cluster id:  1\ngroup id /1005862/1007220/1044020/3392907\nTotal TS before filter:  24\nTotal TS after filter:  24\nTotal TS before filter:  24\nTotal TS after filter:  24\ndone\n{'naive': array([0.0863043]), 'snaive': array([0.06426028]), 'arima': array([0.1109767]), 'ses': array([0.11065485]), 'holts': array([0.09654775]), 'Es': array([0.09959604]), 'auto_ets': array([0.11065486]), 'theta': array([0.11152086]), 'croston': array([0.10998713]), 'prophet': array([0.09949722])}\nWape is:  {'Lstm': 0, 'naive': 0.4944994499449945, 'snaive': 0.551017601760176, 'arima': 0.5250015586565667, 'ses': 0.5260723900561737, 'holts': 0.47564184809286797, 'Es': 0.5162312922448008, 'auto_ets': 0.5260692927512924, 'theta': 0.5197298562839489, 'croston': 0.5354074554563869, 'prophet': 0.5614001714593064, 'Avg_ensemble': 0.5214229803185959, 'Weighted_ensemble': 0.5215478248677629}\nSmape is:  {'Lstm': 0, 'naive': 0.5676103474816037, 'snaive': 0.6718547157523794, 'arima': 0.6197751102084198, 'ses': 0.6210545583388087, 'holts': 0.5374420380185141, 'Es': 0.6044108402350407, 'auto_ets': 0.6210491225702467, 'theta': 0.6102269910908773, 'croston': 0.637563310973726, 'prophet': 0.6865774808009908, 'Avg_ensemble': 0.6129333724371206, 'Weighted_ensemble': 0.6132116974467587}\nPe is:  {'Lstm': 0, 'naive': 0.5676103474816037, 'snaive': 0.6718547157523794, 'arima': 0.6197751102084198, 'ses': 0.6210545583388087, 'holts': 0.5374420380185141, 'Es': 0.6044108402350407, 'auto_ets': 0.6210491225702467, 'theta': 0.6102269910908773, 'croston': 0.637563310973726, 'prophet': 0.6865774808009908, 'Avg_ensemble': 0.6129333724371206, 'Weighted_ensemble': 0.6132116974467587}\nTime is: {'Lstm': 0, 'naive': 0.4801980198019802, 'snaive': 0.5477172717271728, 'arima': 0.521892537647029, 'ses': 0.5220706855900191, 'holts': 0.4562895686541582, 'Es': 0.5089700835821585, 'auto_ets': 0.5220668139589173, 'theta': 0.5152790116579935, 'croston': 0.5337395173402855, 'prophet': 0.5584571739162951, 'Avg_ensemble': 0.5166680683876009, 'Weighted_ensemble': 0.5165753282473032}\n===============================================\n"}], "source": "\ncounter=0\nfor i in range(50):\n    if i not in [44,47,48]:\n        st=time.time()\n        print('cluster id: ',i+1)\n        current_date = '20220831'\n        test_group_id = test_group_id_list[i]#'/3944/1060825/447913'\n        print('group id',test_group_id)\n        #for top cluster:\n        #path = f\"gs://adtech-artifacts/ds/forecast/month_eval/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_tft_data_{current_date}.csv\"\n        path = f\"gs://adtech-artifacts/ds/forecast/month_eval/tail_clusters/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_data_20220831.csv\"\n        # read in data\n        df = spark.read.option(\"header\", True).csv(path).toPandas() # The usage should be the same as previous\n        df=df[['ds', 'group_id', 'query_idx', 'SearchKeyValue',  'LocationKeyValue', 'daily_supply']]\n        #print('query id: ',df['query_idx'].unique())\n        #df_panda=spark.read.csv(\"gs://adtech-artifacts/ds/forecast/query_taxonomy_sample_Addedfeatures_cluster_\"+str(i)+\".csv\",header=True).toPandas()\n        Cols=['daily_supply']#, 'shifted_annual_daily_supply','Trend_Var','dayofyear_sin', 'dayofyear_cos', 'month_sin', 'month_cos']\n        for cols in Cols:df[cols]=df[cols].astype(float,errors='raise')\n        #print('time needed to preprocess the data is: ',time.time()-st)\n        df_train,df_test,train_date,test_date,q,wq=_generate_train_test_data(df,start=start_month[month])\n        df_trainW,df_valW,_,train_dateW,_,_,_=_generate_train_test_data(df,val_req=True,start=start_month[month])\n        df1_train,df1_test=df_train.sum(axis=1),df_test.sum(axis=1)\n        df1_trainW,df_valW=df_trainW.sum(axis=1),df_valW.sum(axis=1)\n        df1_train,df1_trainW,df_valW=df1_train.to_frame(),df1_trainW.to_frame(),df_valW.to_frame()\n        df1_train.index=train_date\n        df1_trainW.index=train_dateW\n        df1_train.index = df1_train.index.to_period(freq = 'D')\n        df1_trainW.index = df1_trainW.index.to_period(freq = 'D')\n        #df_test_log_temp.index = df_test_log_temp.index.to_period(freq = 'D')\n        df1_train=df1_train.rename(columns={0:'daily_supply'})\n        df1_trainW=df1_trainW.rename(columns={0:'daily_supply'})\n    #     if i==0:\n    #         print(q,wq)\n        start=time.time()\n        Models=statModels()\n\n        Models.clus_smape['Lstm']=AggSmapeL[counter]\n        Models.clus_wape['Lstm']=AggWapeL[counter]\n        Models.clus_pe['Lstm']=AggPeL[counter]\n        #Models.models['Lstm'] = fit4\n        #self.stores_mape['Es'] = mape\n        Models.stores_times['Lstm'] = TimesL[counter]\n        ##################\n        #Models.wape_Weighted['Lstm'] = wape_agg_lstm[i]\n\n        Models.training_infer(df1_train,df1_test,df_test,q.values,wq.values,df1_test.shape[0],df_lstm[:,i])#no_ts\n                             #(df_train, df_test, df_test_org,q, wq, forecast_period=30,lstm_pred=None)\n        Models.weighted_avg(df1_trainW, df1_test,df_valW.values, df_test,q.values,wq.values, forecast_period=60,lstm_pred=df_lstm[:,i])\n\n        end=time.time()\n        print('Wape is: ',Models.clus_wape)\n        print('Smape is: ',Models.clus_smape)\n        print('Pe is: ',Models.clus_smape)\n        print('Time is:' ,Models.clus_pe)\n        #print('time needed to run the model: ',end-start)\n        Times.append(Models.stores_times)\n        AggSmape.append(Models.clus_smape)\n        AggWape.append(Models.clus_wape)\n        AggPe.append(Models.clus_pe)\n        #training_j.append(i)\n        counter+=1\n    print('===============================================')\n    "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "\nTime={i:[] for i in Times[0]}\n\nAggSmapes={i:[] for i in AggSmape[0]}\nAggWapes={i:[] for i in AggWape[0]}\nAggPes={i:[] for i in AggPe[0]}\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "\nfor i in range(counter):\n\n    for keys,values in Times[i].items():\n        Time[keys].append(values)\n    for keys,values in AggSmape[i].items():\n        AggSmapes[keys].append(values)\n    for keys,values in AggWape[i].items():\n        AggWapes[keys].append(values)\n    for keys,values in AggPe[i].items():\n        AggPes[keys].append(values)\n    \n    "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "\nprint('Time is: ',Time)\nprint('====================')\nprint('AggSmapes is: ',AggSmapes)\nprint('====================')\nprint('AggWapes is: ',AggWapes)\nprint('====================')\nprint('AggPes is: ',AggPes)\nprint('====================')\n\nprint('====================')\nprint('Original LSTM time is', times_org)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "len(AggPes[\"Lstm\"])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "%matplotlib inline\nimport numpy as np\navg_,med_,e80_,e90_,name=[],[],[],[],[]\n#a=[MAPE_clus,MAPE_agg,WAPE_clus,WAPE_agg]\nfor k,v in Time.items():\n    avg_.append(np.mean(v))\n    med_.append(np.percentile(v,50))\n    e80_.append(np.percentile(v,80))\n    e90_.append(np.percentile(v,90))\n    name.append(k)\n\n\n\nx_axis = np.arange(len(Time))\n\n# Multi bar Chart\n\nbar_w=.1\nplt.bar(x_axis -0.15, avg_, width=bar_w, label = 'Avg',color='red')\nplt.bar(x_axis -0.05, med_, width=bar_w, label = 'Median',color='green')\nplt.bar(x_axis +0.05, e80_, width=bar_w, label = '80-%ile',color='cyan')\nplt.bar(x_axis +0.15, e90_, width=bar_w, label = '90-%tile',color='black')\n\nplt.xticks(x_axis, Time.keys(),rotation = 70)\nplt.legend(bbox_to_anchor=(0.41,0.65))\nplt.ylabel('Time (seconds)')\nplt.xlabel('Model')\nplt.show()  "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import numpy as np\navg_,med_,e80_,e90_,name=[],[],[],[],[]\n#a=[MAPE_clus,MAPE_agg,WAPE_clus,WAPE_agg]\nfor k,v in AggPes.items():\n    avg_.append(np.mean(v))\n    med_.append(np.percentile(v,50))\n    e80_.append(np.percentile(v,80))\n    e90_.append(np.percentile(v,90))\n    name.append(k)\n\n\n\nx_axis = np.arange(len(AggPes))\n\n# Multi bar Chart\nplt.bar(x_axis -0.15, avg_, width=bar_w, label = 'Avg',color='red')\nplt.bar(x_axis -0.05, med_, width=bar_w, label = 'Median',color='green')\nplt.bar(x_axis +0.05, e80_, width=bar_w, label = '80-%ile',color='cyan')\nplt.bar(x_axis +0.15, e90_, width=bar_w, label = '90-%tile',color='black')\n\nplt.xticks(x_axis, AggPes.keys(),rotation = 70)\nplt.legend(bbox_to_anchor=(0.41,0.65))\nplt.ylabel('Agg Weighted PE')\nplt.xlabel('Model')\nplt.show()"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": "import numpy as np\navg_,med_,e80_,e90_,name=[],[],[],[],[]\n#a=[MAPE_clus,MAPE_agg,WAPE_clus,WAPE_agg]\nfor k,v in AggSmapes.items():\n    avg_.append(np.mean(v))\n    med_.append(np.percentile(v,50))\n    e80_.append(np.percentile(v,80))\n    e90_.append(np.percentile(v,90))\n    name.append(k)\n\n\n\nx_axis = np.arange(len(AggPes))\n\n# Multi bar Chart\nplt.bar(x_axis -0.15, avg_, width=bar_w, label = 'Avg',color='red')\nplt.bar(x_axis -0.05, med_, width=bar_w, label = 'Median',color='green')\nplt.bar(x_axis +0.05, e80_, width=bar_w, label = '80-%ile',color='cyan')\nplt.bar(x_axis +0.15, e90_, width=bar_w, label = '90-%tile',color='black')\n\nplt.xticks(x_axis, AggPes.keys(),rotation = 70)\nplt.legend(bbox_to_anchor=(0.41,0.65))\nplt.ylabel('Agg Weighted SMAPE')\nplt.xlabel('Model')\nplt.show()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import numpy as np\navg_,med_,e80_,e90_,name=[],[],[],[],[]\n#a=[MAPE_clus,MAPE_agg,WAPE_clus,WAPE_agg]\nfor k,v in AggWapes.items():\n    avg_.append(np.mean(v))\n    med_.append(np.percentile(v,50))\n    e80_.append(np.percentile(v,80))\n    e90_.append(np.percentile(v,90))\n    name.append(k)\n\n\n\nx_axis = np.arange(len(AggPes))\n\n# Multi bar Chart\nplt.bar(x_axis -0.15, avg_, width=bar_w, label = 'Avg',color='red')\nplt.bar(x_axis -0.05, med_, width=bar_w, label = 'Median',color='green')\nplt.bar(x_axis +0.05, e80_, width=bar_w, label = '80-%ile',color='cyan')\nplt.bar(x_axis +0.15, e90_, width=bar_w, label = '90-%tile',color='black')\n\nplt.xticks(x_axis, AggPes.keys(),rotation = 70)\nplt.legend(bbox_to_anchor=(0.41,0.65))\nplt.ylabel('Agg WAPE')\nplt.xlabel('Model')\nplt.show()"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.13"}}, "nbformat": 4, "nbformat_minor": 2}