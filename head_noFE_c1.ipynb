{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Looking in indexes: https://repository.walmart.com/repository/pypi-proxy/simple/\nRequirement already satisfied: optuna in /opt/conda/anaconda/lib/python3.6/site-packages (2.10.1)\nRequirement already satisfied: tqdm in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (4.64.1)\nRequirement already satisfied: PyYAML in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (5.4.1)\nRequirement already satisfied: alembic in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (1.5.7)\nRequirement already satisfied: numpy in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (1.19.5)\nRequirement already satisfied: scipy!=1.4.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (1.2.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (21.3)\nRequirement already satisfied: cliff in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (3.10.1)\nRequirement already satisfied: cmaes>=0.8.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (0.8.2)\nRequirement already satisfied: sqlalchemy>=1.1.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (1.4.22)\nRequirement already satisfied: colorlog in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (6.7.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from packaging>=20.0->optuna) (3.0.7)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/anaconda/lib/python3.6/site-packages (from sqlalchemy>=1.1.0->optuna) (1.1.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/anaconda/lib/python3.6/site-packages (from sqlalchemy>=1.1.0->optuna) (3.10.1)\nRequirement already satisfied: Mako in /opt/conda/anaconda/lib/python3.6/site-packages (from alembic->optuna) (1.1.4)\nRequirement already satisfied: python-dateutil in /opt/conda/anaconda/lib/python3.6/site-packages (from alembic->optuna) (2.8.2)\nRequirement already satisfied: python-editor>=0.3 in /opt/conda/anaconda/lib/python3.6/site-packages (from alembic->optuna) (1.0.4)\nRequirement already satisfied: autopage>=0.4.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from cliff->optuna) (0.5.1)\nRequirement already satisfied: PrettyTable>=0.7.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from cliff->optuna) (2.5.0)\nRequirement already satisfied: cmd2>=1.0.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from cliff->optuna) (2.4.2)\nRequirement already satisfied: stevedore>=2.0.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from cliff->optuna) (3.5.1)\nRequirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from cliff->optuna) (5.10.0)\nRequirement already satisfied: attrs>=16.3.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from cmd2>=1.0.0->cliff->optuna) (18.1.0)\nRequirement already satisfied: wcwidth>=0.1.7 in /opt/conda/anaconda/lib/python3.6/site-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\nRequirement already satisfied: typing-extensions in /opt/conda/anaconda/lib/python3.6/site-packages (from cmd2>=1.0.0->cliff->optuna) (3.10.0.2)\nRequirement already satisfied: pyperclip>=1.6 in /opt/conda/anaconda/lib/python3.6/site-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.6.0)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from Mako->alembic->optuna) (2.0.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from python-dateutil->alembic->optuna) (1.16.0)\nRequirement already satisfied: importlib-resources in /opt/conda/anaconda/lib/python3.6/site-packages (from tqdm->optuna) (5.4.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nLooking in indexes: https://repository.walmart.com/repository/pypi-proxy/simple/\nRequirement already satisfied: pmdarima in /opt/conda/anaconda/lib/python3.6/site-packages (1.2.1)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima) (1.1.1)\nRequirement already satisfied: numpy>=1.15 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima) (1.19.5)\nRequirement already satisfied: six>=1.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima) (1.16.0)\nRequirement already satisfied: statsmodels>=0.9.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima) (0.12.2)\nRequirement already satisfied: scipy<1.3,>=1.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima) (1.2.3)\nRequirement already satisfied: Cython>=0.29 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima) (0.29.32)\nRequirement already satisfied: scikit-learn>=0.19 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima) (0.24.2)\nRequirement already satisfied: pandas>=0.19 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima) (1.1.5)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/anaconda/lib/python3.6/site-packages (from pandas>=0.19->pmdarima) (2.8.2)\nRequirement already satisfied: pytz>=2017.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from pandas>=0.19->pmdarima) (2022.4)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from scikit-learn>=0.19->pmdarima) (3.1.0)\nRequirement already satisfied: patsy>=0.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from statsmodels>=0.9.0->pmdarima) (0.5.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsktime 0.9.0 requires numpy<=1.19.3, but you have numpy 1.19.5 which is incompatible.\nsktime 0.9.0 requires statsmodels<=0.12.1, but you have statsmodels 0.12.2 which is incompatible.\u001b[0m\nLooking in indexes: https://repository.walmart.com/repository/pypi-proxy/simple/\nRequirement already satisfied: yellowbrick==0.9.1 in /opt/conda/anaconda/lib/python3.6/site-packages (0.9.1)\nCollecting scikit-learn==0.22.2\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/scikit-learn/scikit_learn-0.22.2-cp36-cp36m-manylinux1_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2UxLzdmLzM2NmRjYmExYmEwNzZhODhhNTBiZWE3MzJkYmMwMzNjMGM1YmJmNzg3NjAxMGU2ZWRjNjc5NDg1NzlkNS9zY2lraXRfbGVhcm4tMC4yMi4yLWNwMzYtY3AzNm0tbWFueWxpbnV4MV94ODZfNjQud2hsI3NoYTI1Nj1iYzhiMzdjYTBkZTc5NDgzODM3NmNmZmRkYWI2OWE0NmE5YTU0YzQzMzA4MDBlYTFmOTNjYWI2MWNhNjdkZmEz (7.1 MB)\nRequirement already satisfied: numpy>=1.13.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from yellowbrick==0.9.1) (1.19.5)\nRequirement already satisfied: cycler>=0.10.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from yellowbrick==0.9.1) (0.10.0)\nRequirement already satisfied: matplotlib!=3.0.0,>=1.5.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from yellowbrick==0.9.1) (2.2.4)\nRequirement already satisfied: scipy>=1.0.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from yellowbrick==0.9.1) (1.2.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/anaconda/lib/python3.6/site-packages (from scikit-learn==0.22.2) (1.1.1)\nRequirement already satisfied: six in /opt/conda/anaconda/lib/python3.6/site-packages (from cycler>=0.10.0->yellowbrick==0.9.1) (1.16.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=1.5.1->yellowbrick==0.9.1) (3.0.7)\nRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=1.5.1->yellowbrick==0.9.1) (2.8.2)\nRequirement already satisfied: pytz in /opt/conda/anaconda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=1.5.1->yellowbrick==0.9.1) (2022.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=1.5.1->yellowbrick==0.9.1) (1.0.1)\nRequirement already satisfied: setuptools in /opt/conda/anaconda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=1.5.1->yellowbrick==0.9.1) (59.6.0)\nInstalling collected packages: scikit-learn\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 0.24.2\n    Uninstalling scikit-learn-0.24.2:\n      Successfully uninstalled scikit-learn-0.24.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsktime 0.9.0 requires numpy<=1.19.3, but you have numpy 1.19.5 which is incompatible.\nsktime 0.9.0 requires scikit-learn>=0.24.0, but you have scikit-learn 0.22.2 which is incompatible.\nsktime 0.9.0 requires statsmodels<=0.12.1, but you have statsmodels 0.12.2 which is incompatible.\u001b[0m\nSuccessfully installed scikit-learn-0.22.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nLooking in indexes: https://repository.walmart.com/repository/pypi-proxy/simple/\nCollecting sktime\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/sktime/sktime-0.9.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzczLzcyLzg0MGMzMmI0ZGJhMjM2Y2ZkYmQzZDA0NmJjNTUzZDNlY2ZlNDQwNzliY2VkZTNlYThmMDAwY2E0Mzk2ZC9za3RpbWUtMC45LjAtY3AzNi1jcDM2bS1tYW55bGludXhfMl8xN194ODZfNjQubWFueWxpbnV4MjAxNF94ODZfNjQud2hsI3NoYTI1Nj0zZDgwY2ZhZjgwY2NhNjA5M2UwYjg2MjQxMTA5ZDgzNmQ2ZWJlMGE1NzAyNDBkMTliNDE1MmY2NDc5MzVjMTdh (6.2 MB)\nCollecting numpy<=1.19.3\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/numpy/numpy-1.19.3-cp36-cp36m-manylinux2010_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzBlL2Y3L2E3ZDdlMGRlOTlhN2I0M2JkOTVhYWRkY2YyOWU2NWI1YTE4NWNhMzg5ZGQxMzI5YTUzY2M5ODZlZGMzOC9udW1weS0xLjE5LjMtY3AzNi1jcDM2bS1tYW55bGludXgyMDEwX3g4Nl82NC53aGwjc2hhMjU2PTcxOTdlZTBhMjU2MjllZDc4MmM3YmQwMTg3MWVlNDA3MDJmZmVlZjM1YmM0ODAwNGJjMmZkY2M3MWUyOWJhOWQ= (14.9 MB)\nCollecting statsmodels<=0.12.1\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/statsmodels/statsmodels-0.12.1-cp36-cp36m-manylinux1_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2JlLzRjLzllMjQzNWNhNjY0NWQ2YmFmYTJiNTFiYjExZjBhMzY1YjI4OTM0YTJmZmU5ZDZlMzM5ZDY3MTMwOTI2ZC9zdGF0c21vZGVscy0wLjEyLjEtY3AzNi1jcDM2bS1tYW55bGludXgxX3g4Nl82NC53aGwjc2hhMjU2PTE0MmVhY2Q1YTFiZDg3MjgzNThmZjQ4MTAxZWUwZTUxY2EzZDQyYTkzZjZlNWNiNjFmY2ZhY2Y2MTM5NzdiY2Y= (9.5 MB)\nCollecting deprecated>=1.2.13\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/deprecated/Deprecated-1.2.13-py2.py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzUxLzZhL2MzYTA0MDg2NDY0MDhmNzI4M2I3YmM1NTBjMzBhMzJjYzc5MTE4MWVjNDYxODU5MmVlYzEzZTA2NmNlMy9EZXByZWNhdGVkLTEuMi4xMy1weTIucHkzLW5vbmUtYW55LndobCNzaGEyNTY9NjQ3NTZlM2UxNGM4YzVlZWE5Nzk1ZDkzYzUyNDU1MTQzMmEwYmU3NTYyOWY4ZjI5ZTY3YWI4Y2FmMDc2Yzc2ZA== (9.6 kB)\nCollecting wheel\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/wheel/wheel-0.37.1-py2.py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzI3L2Q2LzAwM2U1OTMyOTZhODVmZDZlZDYxNmVkOTYyNzk1YjJmODc3MDljM2VlZTJiY2E0ZjZkMGZlNTVjNmQwMC93aGVlbC0wLjM3LjEtcHkyLnB5My1ub25lLWFueS53aGwjc2hhMjU2PTRiZGNkN2Q4NDAxMzgwODYxMjZjZDA5MjU0ZGM2MTk1ZmI0ZmM2ZjAxYzA1MGExZDcyMzZmMjYzMGRiMWQyMmE= (35 kB)\nCollecting pandas>=1.1.0\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/pandas/pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2MzL2UyLzAwY2FjZWNhZmJhYjA3MWM3ODcwMTlmMDBhZDg0Y2EzMTg1OTUyZjZiYjliY2E5NTUwZWQ4Mzg3MGQ0ZC9wYW5kYXMtMS4xLjUtY3AzNi1jcDM2bS1tYW55bGludXgxX3g4Nl82NC53aGwjc2hhMjU2PWI2MTA4MDc1MGQxOWEwMTIyNDY5YWI1OWIwODczODA3MjFkNmI3MmE0ZTdkOTYyZTRkN2U2M2UwYzQ1MDQ4MTQ= (9.5 MB)\nCollecting numba>=0.53\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/numba/numba-0.53.1-cp36-cp36m-manylinux2014_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzRhL2MxL2U3ZmRiZmM4ODZhOWQ5YzExNzY3NTMzOTAzZGIwZDgxNmMwZjY1NmZkNjAyOWY0YTA2MTc0Mjg5MzY5NC9udW1iYS0wLjUzLjEtY3AzNi1jcDM2bS1tYW55bGludXgyMDE0X3g4Nl82NC53aGwjc2hhMjU2PThmYTVjOTYzYTQzODU1MDUwYTg2ODEwNmE4N2NkNjE0ZjNjM2Y0NTk5NTFjOGZjNDY4YWVjMjYzZWY4MGQwNjM= (3.4 MB)\nCollecting scikit-learn>=0.24.0\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/scikit-learn/scikit_learn-0.24.2-cp36-cp36m-manylinux2010_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2QzL2ViL2QwZTY1ODQ2NWMwMjlmZWI3MDgzMTM5ZDllYWQ1MTAwMDc0MmU4OGIxZmI3ZjE1MDRlMTllMWI0Y2U2ZS9zY2lraXRfbGVhcm4tMC4yNC4yLWNwMzYtY3AzNm0tbWFueWxpbnV4MjAxMF94ODZfNjQud2hsI3NoYTI1Nj01ZmYzZTRlNGNmNzU5MmQzNjU0MWVkZWM0MzRlMDlmYjhhYjliYTZiNDc2MDhjNGZmZTMwYzkwMzhkMzAxODk3 (22.2 MB)\nCollecting wrapt<2,>=1.10\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/wrapt/wrapt-1.14.1-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2UwLzgwL2FmOWRhNzM3OWVlNmRmNTgzODc1ZDBhZWI4MGY5ZDVmMGJkNWYwODFkZDFlZTVjZTA2NTg3ZDhiZmVjNy93cmFwdC0xLjE0LjEtY3AzNi1jcDM2bS1tYW55bGludXhfMl81X3g4Nl82NC5tYW55bGludXgxX3g4Nl82NC5tYW55bGludXhfMl8xN194ODZfNjQubWFueWxpbnV4MjAxNF94ODZfNjQud2hsI3NoYTI1Nj0yMWFjMDE1NmM0YjA4OWIzMzBiNzY2NmRiNDBmZWVlMzBhNWQ1MjYzNGNjNDU2MGUxOTA1ZDY1MjlhMzg5N2Zm (74 kB)\nCollecting setuptools\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/setuptools/setuptools-59.6.0-py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2IwLzNhLzg4YjIxMGRiNjhlNTY4NTRkMGJjZjRiMzhlMTY1ZTAzYmUzNzdlMTM5MDc3NDZmODI1NzkwZjNkZjViZi9zZXR1cHRvb2xzLTU5LjYuMC1weTMtbm9uZS1hbnkud2hsI3NoYTI1Nj00Y2U5MmYxZTFmOGYwMTIzM2VlOTk1MmMwNGY2YjgxZDFlMDI5MzlkNmUxYjQ4ODQyODE1NDk3NGE0ZDA3ODNl (952 kB)\nCollecting llvmlite<0.37,>=0.36.0rc1\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/llvmlite/llvmlite-0.36.0-cp36-cp36m-manylinux2010_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzRkLzVhLzcwN2NjN2UwNzJkNzFiYzE5ODY5ZDA5M2U1Y2Y5YjdiZTk4Y2I0MmQyMzk4NDg5NDY1NDc0ZDAwN2NlOC9sbHZtbGl0ZS0wLjM2LjAtY3AzNi1jcDM2bS1tYW55bGludXgyMDEwX3g4Nl82NC53aGwjc2hhMjU2PTc3Njg2NTg2NDZjNDE4YjliM2JlY2NiNzA0NDI3N2E2MDhiYzhjNjJiODJhODVlNzNjN2U1YzA2NWU0MTU3YzI= (25.3 MB)\nCollecting python-dateutil>=2.7.3\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/python-dateutil/python_dateutil-2.8.2-py2.py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzM2LzdhLzg3ODM3ZjM5ZDAyOTZlNzIzYmI5YjYyYmJiMjU3ZDAzNTVjN2Y2MTI4ODUzYzc4OTU1ZjU3MzQyYTU2ZC9weXRob25fZGF0ZXV0aWwtMi44LjItcHkyLnB5My1ub25lLWFueS53aGwjc2hhMjU2PTk2MWQwM2RjMzQ1M2ViYmM1OWRiZGVhOWU0ZTExYzU2NTE1MjBhODc2ZDBmNGRiMTYxZTg2NzRhYWU5MzVkYTk= (247 kB)\nCollecting pytz>=2017.2\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/pytz/pytz-2022.4-py2.py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2Q4LzY2LzMwOTU0NTQxMzE2MmJjODI3MWM3M2U2MjI0OTlhNDFjZGMzNzIxN2I0OTlmZWJlMjYxNTVmZjlmOTNlZC9weXR6LTIwMjIuNC1weTIucHkzLW5vbmUtYW55LndobCNzaGEyNTY9MmMwNzg0NzQ3MDcxNDAyYzZlOTlmMGJhZmRiN2RhMGZhMjI2NDVmMDY1NTRjN2FlMDZiZjYzNTg4OTdlOWM5MQ== (500 kB)\nCollecting six>=1.5\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/six/six-1.16.0-py2.py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2Q5LzVhL2U3YzMxYWRiZTg3NWYyYWJiYjkxYmQ4NGNmMmRjNTJkNzkyYjVhMDE1MDY3ODFkYmNmMjVjOTFkYWYxMS9zaXgtMS4xNi4wLXB5Mi5weTMtbm9uZS1hbnkud2hsI3NoYTI1Nj04YWJiMmYxZDg2ODkwYTJkZmI5ODlmOWE3N2NmY2ZkM2U0N2MyYTM1NGIwMTExMTc3MTMyNmY4YWEyNmUwMjU0 (11 kB)\nCollecting joblib>=0.11\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/joblib/joblib-1.1.1-py2.py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzdjLzkxL2QzYmEwNDAxZTYyZDdlNDI4MTZiYzdkOTdiODJkMTljOTVjMTY0YjNlMTQ5YTg3YzBhMWMwMjZhNzM1ZS9qb2JsaWItMS4xLjEtcHkyLnB5My1ub25lLWFueS53aGwjc2hhMjU2PWY5ZDZjM2NkZjJhNzc3OGU5MDU4ZTEwZTlkYmEwMjhlNDc3NzFhMWEzNTVlNTc2OGY0NjcwNGJmMDUzNDJlYmE= (309 kB)\nCollecting threadpoolctl>=2.0.0\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/threadpoolctl/threadpoolctl-3.1.0-py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzYxL2NmLzZlMzU0MzA0YmNiOWM2NDEzYzRlMDJhNzQ3YjYwMDA2MWMyMWQzOGJhNTFlN2U1NDRhYzdiYzY2YWVjYy90aHJlYWRwb29sY3RsLTMuMS4wLXB5My1ub25lLWFueS53aGwjc2hhMjU2PThiOTlhZGRhMjY1ZmViNjc3MzI4MGRmNDFlZWNlN2IyZTY1NjFiNzcyZDIxZmZkNTJlMzcyZjk5OTAyNDkwN2I= (14 kB)\nCollecting scipy>=0.19.1\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/scipy/scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2M4Lzg5LzYzMTcxMjI4ZDVjZWQxNDhmNWNlZDUwMzA1Yzg5ZTg1NzZmZmM2OTVhOTBiNThmZTViYjYwMmI5MTBjMi9zY2lweS0xLjUuNC1jcDM2LWNwMzZtLW1hbnlsaW51eDFfeDg2XzY0LndobCNzaGEyNTY9MzY4YzBmNjlmOTMxODYzMDllMWI0YmViOGUyNmQ1MWRkNmY1MDEwYjc5MjY0YzBmMWU5Y2EwMGNkOTJlYThjOQ== (25.9 MB)\nCollecting patsy>=0.5\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/patsy/patsy-0.5.3-py2.py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzJhL2U0L2IzMjYzYjBlMzUzZjJiZTdiMTRmMDQ0ZDU3ODc0NDkwYzljZWYxNzk4YTQzNWYwMzg2ODNhY2VhNWM5OC9wYXRzeS0wLjUuMy1weTIucHkzLW5vbmUtYW55LndobCNzaGEyNTY9N2ViNTM0OTc1NGVkNmFhOTgyYWY4MWY2MzY0NzliMWI4ZGI5ZDViMWE2ZTk1N2E2MDE2ZWMwNTM0YjVjODZiNw== (233 kB)\nInstalling collected packages: six, pytz, python-dateutil, numpy, wrapt, threadpoolctl, setuptools, scipy, patsy, pandas, llvmlite, joblib, wheel, statsmodels, scikit-learn, numba, deprecated, sktime\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndistributed 1.21.8 requires msgpack, which is not installed.\npmdarima 1.2.1 requires scipy<1.3,>=1.2, but you have scipy 1.5.4 which is incompatible.\u001b[0m\nSuccessfully installed deprecated-1.2.13 joblib-1.1.1 llvmlite-0.36.0 numba-0.53.1 numpy-1.19.5 pandas-1.1.5 patsy-0.5.3 python-dateutil-2.8.2 pytz-2022.4 scikit-learn-0.24.2 scipy-1.5.4 setuptools-59.6.0 six-1.16.0 sktime-0.9.0 statsmodels-0.12.2 threadpoolctl-3.1.0 wheel-0.37.1 wrapt-1.14.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nLooking in indexes: https://repository.walmart.com/repository/pypi-proxy/simple/\nRequirement already satisfied: delayed in /opt/conda/anaconda/lib/python3.6/site-packages (0.11.0b1)\nRequirement already satisfied: redis in /opt/conda/anaconda/lib/python3.6/site-packages (from delayed) (4.3.4)\nRequirement already satisfied: hiredis in /opt/conda/anaconda/lib/python3.6/site-packages (from delayed) (2.0.0)\nRequirement already satisfied: deprecated>=1.2.3 in /opt/conda/anaconda/lib/python3.6/site-packages (from redis->delayed) (1.2.13)\nRequirement already satisfied: async-timeout>=4.0.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from redis->delayed) (4.0.2)\nRequirement already satisfied: packaging>=20.4 in /opt/conda/anaconda/lib/python3.6/site-packages (from redis->delayed) (21.3)\nRequirement already satisfied: typing-extensions in /opt/conda/anaconda/lib/python3.6/site-packages (from redis->delayed) (3.10.0.2)\nRequirement already satisfied: importlib-metadata>=1.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from redis->delayed) (3.10.1)\nRequirement already satisfied: wrapt<2,>=1.10 in /opt/conda/anaconda/lib/python3.6/site-packages (from deprecated>=1.2.3->redis->delayed) (1.14.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from importlib-metadata>=1.0->redis->delayed) (3.6.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from packaging>=20.4->redis->delayed) (3.0.7)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nLooking in indexes: https://repository.walmart.com/repository/pypi-proxy/simple/\nRequirement already satisfied: fbprophet in /opt/conda/anaconda/lib/python3.6/site-packages (0.7.1)\nRequirement already satisfied: Cython>=0.22 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (0.29.32)\nRequirement already satisfied: cmdstanpy==0.9.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (0.9.5)\nRequirement already satisfied: pystan>=2.14 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (2.19.1.1)\nRequirement already satisfied: numpy>=1.15.4 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (1.19.5)\nRequirement already satisfied: pandas>=1.0.4 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (1.1.5)\nRequirement already satisfied: matplotlib>=2.0.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (2.2.4)\nRequirement already satisfied: LunarCalendar>=0.0.9 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (0.0.9)\nRequirement already satisfied: convertdate>=2.1.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (2.3.2)\nRequirement already satisfied: holidays>=0.10.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (0.13)\nRequirement already satisfied: setuptools-git>=1.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (1.2)\nRequirement already satisfied: python-dateutil>=2.8.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (2.8.2)\nRequirement already satisfied: tqdm>=4.36.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (4.64.1)\nRequirement already satisfied: pymeeus<=1,>=0.3.13 in /opt/conda/anaconda/lib/python3.6/site-packages (from convertdate>=2.1.2->fbprophet) (0.5.11)\nRequirement already satisfied: pytz>=2014.10 in /opt/conda/anaconda/lib/python3.6/site-packages (from convertdate>=2.1.2->fbprophet) (2022.4)\nRequirement already satisfied: korean-lunar-calendar in /opt/conda/anaconda/lib/python3.6/site-packages (from holidays>=0.10.2->fbprophet) (0.3.1)\nRequirement already satisfied: hijri-converter in /opt/conda/anaconda/lib/python3.6/site-packages (from holidays>=0.10.2->fbprophet) (2.2.4)\nRequirement already satisfied: ephem>=3.7.5.3 in /opt/conda/anaconda/lib/python3.6/site-packages (from LunarCalendar>=0.0.9->fbprophet) (4.1.3)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/anaconda/lib/python3.6/site-packages (from matplotlib>=2.0.0->fbprophet) (0.10.0)\nRequirement already satisfied: six>=1.10 in /opt/conda/anaconda/lib/python3.6/site-packages (from matplotlib>=2.0.0->fbprophet) (1.16.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from matplotlib>=2.0.0->fbprophet) (1.0.1)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from matplotlib>=2.0.0->fbprophet) (3.0.7)\nRequirement already satisfied: setuptools in /opt/conda/anaconda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->fbprophet) (59.6.0)\nRequirement already satisfied: importlib-resources in /opt/conda/anaconda/lib/python3.6/site-packages (from tqdm>=4.36.1->fbprophet) (5.4.0)\nRequirement already satisfied: zipp>=3.1.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from importlib-resources->tqdm>=4.36.1->fbprophet) (3.6.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nLooking in indexes: https://repository.walmart.com/repository/pypi-proxy/simple/\nRequirement already satisfied: tbats in /opt/conda/anaconda/lib/python3.6/site-packages (1.1.1)\nRequirement already satisfied: scikit-learn in /opt/conda/anaconda/lib/python3.6/site-packages (from tbats) (0.24.2)\nRequirement already satisfied: pmdarima in /opt/conda/anaconda/lib/python3.6/site-packages (from tbats) (1.2.1)\nRequirement already satisfied: scipy in /opt/conda/anaconda/lib/python3.6/site-packages (from tbats) (1.5.4)\nRequirement already satisfied: numpy in /opt/conda/anaconda/lib/python3.6/site-packages (from tbats) (1.19.5)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima->tbats) (1.1.1)\nRequirement already satisfied: Cython>=0.29 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima->tbats) (0.29.32)\nCollecting scipy\n  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/scipy/scipy-1.2.3-cp36-cp36m-manylinux1_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2M3L2Q2LzY2MWNmNGZiMzJlY2RhNDBmYzAzNjg1ZWYwOTk1ZGIzZDMzMDBjNjE5ZjQ2MTc1MTYyMWY2NDZhYjNmNi9zY2lweS0xLjIuMy1jcDM2LWNwMzZtLW1hbnlsaW51eDFfeDg2XzY0LndobCNzaGEyNTY9YzA4MDEzZTBmZTE1NTQzNzJkYTkzMTJkNWJhZDU4ODQwMmY3MWMwNjM2ZjBmODZhOWI5YjYxZDUwN2M1OWJhYw== (24.8 MB)\nRequirement already satisfied: six>=1.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima->tbats) (1.16.0)\nRequirement already satisfied: pandas>=0.19 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima->tbats) (1.1.5)\nRequirement already satisfied: statsmodels>=0.9.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima->tbats) (0.12.2)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/anaconda/lib/python3.6/site-packages (from pandas>=0.19->pmdarima->tbats) (2.8.2)\nRequirement already satisfied: pytz>=2017.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from pandas>=0.19->pmdarima->tbats) (2022.4)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from scikit-learn->tbats) (3.1.0)\nRequirement already satisfied: patsy>=0.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from statsmodels>=0.9.0->pmdarima->tbats) (0.5.3)\nInstalling collected packages: scipy\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.5.4\n    Uninstalling scipy-1.5.4:\n      Successfully uninstalled scipy-1.5.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsktime 0.9.0 requires numpy<=1.19.3, but you have numpy 1.19.5 which is incompatible.\nsktime 0.9.0 requires statsmodels<=0.12.1, but you have statsmodels 0.12.2 which is incompatible.\u001b[0m\nSuccessfully installed scipy-1.2.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nLooking in indexes: https://repository.walmart.com/repository/pypi-proxy/simple/\nRequirement already satisfied: workalendar in /opt/conda/anaconda/lib/python3.6/site-packages (16.4.0)\nRequirement already satisfied: pyluach in /opt/conda/anaconda/lib/python3.6/site-packages (from workalendar) (1.4.2)\nRequirement already satisfied: convertdate in /opt/conda/anaconda/lib/python3.6/site-packages (from workalendar) (2.3.2)\nRequirement already satisfied: importlib-metadata in /opt/conda/anaconda/lib/python3.6/site-packages (from workalendar) (3.10.1)\nRequirement already satisfied: lunardate in /opt/conda/anaconda/lib/python3.6/site-packages (from workalendar) (0.2.0)\nRequirement already satisfied: python-dateutil in /opt/conda/anaconda/lib/python3.6/site-packages (from workalendar) (2.8.2)\nRequirement already satisfied: backports.zoneinfo in /opt/conda/anaconda/lib/python3.6/site-packages (from workalendar) (0.2.1)\nRequirement already satisfied: importlib-resources in /opt/conda/anaconda/lib/python3.6/site-packages (from backports.zoneinfo->workalendar) (5.4.0)\nRequirement already satisfied: pytz>=2014.10 in /opt/conda/anaconda/lib/python3.6/site-packages (from convertdate->workalendar) (2022.4)\nRequirement already satisfied: pymeeus<=1,>=0.3.13 in /opt/conda/anaconda/lib/python3.6/site-packages (from convertdate->workalendar) (0.5.11)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/anaconda/lib/python3.6/site-packages (from importlib-metadata->workalendar) (3.10.0.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from importlib-metadata->workalendar) (3.6.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from python-dateutil->workalendar) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"}], "source": "!pip install optuna\n!pip install pmdarima\n!pip install pmdarima scipy==1.2 -Uqq\n!pip install yellowbrick==0.9.1 scikit-learn==0.22.2\n!pip install sktime --ignore-installed\n!pip install delayed\n!pip install fbprophet\n!pip install tbats\n!pip install workalendar\n\nimport optuna\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n# os.chdir(\"../../..\")\nimport pandas as pd\nimport numpy as np\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import *\nfrom pyspark import SparkConf\nfrom pyspark.sql import SparkSession\n\nimport sys, os, time\nimport matplotlib.cm as cm\n\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nfrom workalendar.core import Calendar\nfrom workalendar.registry import registry\nfrom pandas.tseries.offsets import BDay\nimport numpy as np\nfrom pandas.tseries.holiday import (\n    AbstractHolidayCalendar, Holiday, DateOffset, \\\n    SU, MO, TU, WE, TH, FR, SA, \\\n    next_monday, nearest_workday, sunday_to_monday,\n    EasterMonday, GoodFriday, Easter\n)\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error#, mean_absolute_percentage_error\n\n\nfrom datetime import datetime\nimport statistics\n\nimport random\n\n\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom dateutil.relativedelta import relativedelta\n\nimport re\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn import preprocessing\nfrom torch.utils.data import Dataset, DataLoader\n\nspark = SparkSession \\\n    .builder \\\n    .appName(\"WMX - impr forecast - KT\") \\\n    .enableHiveSupport()\\\n    .config(\"hive.exec.dynamic.partition\", \"true\")\\\n    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\\\n    .config(\"spark.sql.catalogImplementation\",\"hive\")\\\n    .config(\"spark.sql.hive.convertMetastoreOrc\", \"false\")\\\n    .config(\"spark.executor.memory\", \"3G\")\\\n    .config(\"spark.driver.memory\", \"3G\")\\\n    .config(\"spark.network.timeout\", \"43200\")\\\n    .config(\"spark.rdd.compress\", \"true\")\\\n    .config(\"spark.cores.max\", \"512\")\\\n    .config(\"spark.default.parallelism\", \"256\")\\\n    .config(\"spark.dynamicAllocation.minExecutors\", \"64\")\\\n    .config(\"spark.dynamicAllocation.maxExecutors\", \"2048\")\\\n    .config(\"spark.executor.memoryOverhead\", \"1G\")\\\n    .config(\"spark.sql.hive.convertMetastoreOrc\", \"false\")\\\n    .config(\"spark.sql.execution.arrow.enabled\",\"true\")\\\n    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n    .config(\"spark.driver.maxResultSize\",\"0\")\\\n    .config(\"spark.memory.offHeap.enabled\",\"true\") \\\n    .config(\"spark.memory.offHeap.size\",\"1g\")\\\n    .config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest_2.11.jar')\\\n    .getOrCreate()"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "cuda:0\n1\nTesla V100-SXM2-16GB\n"}], "source": "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nprint(torch.cuda.device_count())\nprint(torch.cuda.get_device_name(torch.cuda.current_device()))\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n# os.chdir(\"../../..\")\nimport pandas as pd\nimport numpy as np\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import *\nfrom pyspark import SparkConf\nfrom pyspark.sql import SparkSession\n\nimport sys, os, time\nimport matplotlib.cm as cm\n\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error#, mean_absolute_percentage_error\n\n\nfrom datetime import datetime\nimport statistics\n\nimport random\n\n\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom dateutil.relativedelta import relativedelta\n\nimport re\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn import preprocessing\nfrom torch.utils.data import Dataset, DataLoader\n\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n# os.chdir(\"../../..\")\nimport pandas as pd\nimport numpy as np\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import *\nfrom pyspark import SparkConf\nfrom pyspark.sql import SparkSession\nimport os,sys\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\nfrom dateutil.relativedelta import relativedelta\nimport numpy as np\nimport pandas as pd\nimport re\nimport os\nimport sys, os, time\nimport matplotlib.cm as cm\n\nimport matplotlib.pyplot as plt\n\nimport matplotlib.ticker as mticker\n\nimport torch\nimport torch.nn as nn\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error#, mean_absolute_percentage_error\n\n\nfrom datetime import datetime\nimport statistics\n\nimport random\nimport time\nfrom sklearn.preprocessing import MinMaxScaler\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\n\nimport time\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport numpy as np\n\n\nimport time\nfrom torch.optim.lr_scheduler import ExponentialLR\n\nimport sys, os, time\nimport matplotlib.cm as cm\n\nimport torch\nimport torch.nn as nn\n\nfrom datetime import datetime\nimport statistics\n\nimport random\nimport time\nimport optuna\nfrom optuna.trial import TrialState\nimport torch.nn.functional as F\n\nimport os,sys\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\nfrom dateutil.relativedelta import relativedelta\nimport re\nimport os\nfrom sklearn.preprocessing import MinMaxScaler\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\nimport torch\nfrom sklearn import preprocessing\nfrom dateutil.relativedelta import relativedelta\n# from ray import tune\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n#le = preprocessing.LabelEncoder()\nfrom sklearn.preprocessing import MinMaxScaler\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n# from ray import tune\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": "import pmdarima as pmd\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport numpy as np\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n#import statsmodels.api as sm\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.arima_model import ARIMA\nimport sktime\nimport delayed\nfrom sktime.forecasting.naive import NaiveForecaster\nfrom sktime.forecasting.ets import AutoETS\nfrom warnings import simplefilter\n\nfrom sktime.forecasting.base import ForecastingHorizon\nfrom sktime.forecasting.compose import (\n    EnsembleForecaster,\n    MultiplexForecaster,\n    TransformedTargetForecaster,\n    make_reduction,\n)\nfrom sktime.forecasting.exp_smoothing import ExponentialSmoothing\nfrom sktime.forecasting.model_evaluation import evaluate\nfrom sktime.forecasting.model_selection import (\n    ExpandingWindowSplitter,\n    ForecastingGridSearchCV,\n    SlidingWindowSplitter,\n    temporal_train_test_split,\n)\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom sktime.forecasting.theta import ThetaForecaster\nfrom sktime.forecasting.trend import PolynomialTrendForecaster\nfrom sktime.transformations.series.detrend import Deseasonalizer, Detrender\nfrom sktime.forecasting.theta import ThetaForecaster\nfrom statsmodels.tsa.api import STLForecast\nfrom sktime.forecasting.bats import BATS\nfrom sktime.forecasting.tbats import TBATS\nfrom sktime.forecasting.croston import Croston\nfrom statsmodels.tsa.exponential_smoothing.ets import ETSModel\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n\n"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "Importing plotly failed. Interactive plots will not work.\n"}], "source": "\nfrom __future__ import print_function\nimport sys, os, time\nimport matplotlib.cm as cm\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error#, mean_absolute_percentage_error\n\n\nfrom datetime import datetime\nimport statistics\n\nimport random\nimport time\n\nfrom tqdm.auto import tqdm\n\n# import darts\n# from darts import TimeSeries\n# from darts.metrics import *\n# from darts.utils.missing_values import fill_missing_values\n\nimport warnings\nfrom pyspark.sql import Window\nimport pyspark.sql.functions as f\n\nimport os,sys\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\nfrom dateutil.relativedelta import relativedelta\nimport numpy as np\nimport pandas as pd\nimport re\nimport os\nfrom sklearn.preprocessing import MinMaxScaler\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, TimeDistributed, Conv1D, MaxPooling1D, Flatten, Bidirectional, Input, Flatten, Activation, Reshape, RepeatVector, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nimport torch\nfrom sktime.forecasting.fbprophet import Prophet"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": "from sklearn import linear_model\n\nimport sys, os, time\nimport matplotlib.cm as cm\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error#, mean_absolute_percentage_error\n\nfrom sklearn import linear_model\nfrom datetime import datetime\nimport statistics\n\nimport random\nimport time\n\nfrom tqdm.auto import tqdm\n\n# import darts\n# from darts import TimeSeries\n# from darts.metrics import *\n# from darts.utils.missing_values import fill_missing_values\n\nimport warnings\nfrom pyspark.sql import Window\nimport pyspark.sql.functions as f\n\nimport os,sys\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\nfrom dateutil.relativedelta import relativedelta\nimport numpy as np\nimport pandas as pd\nimport re\nimport os\nfrom sklearn.preprocessing import MinMaxScaler\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, TimeDistributed, Conv1D, MaxPooling1D, Flatten, Bidirectional, Input, Flatten, Activation, Reshape, RepeatVector, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nimport torch\n"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": "class dataprep:\n    def __init__(self, df, input_window=90, output_window=30, stride=1, start='2020-11-01'):\n        # le = preprocessing.LabelEncoder()\n        df['ds'] = pd.to_datetime(df['ds'], format='%Y-%m-%d')\n        start = pd.to_datetime(start, format='%Y-%m-%d') #+ relativedelta(months=months)\n        end = start + relativedelta(days=365 + 30) #+ relativedelta(months=months)\n        # start1=df.ds.sort_values()[0]\n        # end1=df.ds.sort_values()[len(df_panda)-1]\n        # if end1<end:end=end1\n        # if start1>start:start=start1\n        # print(start,end)\n        df = df[(df.ds >= start) & (df.ds <= end)]\n\n\n        df['query_idx'] = df['query_idx'].map(str)\n        df.sort_values('ds', inplace=True)\n        if df['ds'].iloc[-1]!=end:\n            last_date=df.iloc[-1,:]\n            last_date['ds']=end\n            last_date['daily_supply'] = 0\n            df = df.append(last_date)\n        if df['ds'].iloc[0]!=start:\n            first_date=df.iloc[0,:]\n            first_date['ds']=end\n            first_date['daily_supply'] = 0\n            df = df.append(first_date)\n        df.sort_values('ds', inplace=True)\n        df['grp'] = df.apply(\n            lambda x: '%s_%s_%s_%s' % (x['group_id'], x['query_idx'], x['SearchKeyValue'], x['LocationKeyValue']),\n            axis=1)\n        dts = (end - start).days + 1\n\n        qr = df.groupby(['grp'])['daily_supply'].count()\n        self.df_test=df[['ds','daily_supply','grp']]\n##############################\n        qrs = list(qr[(dts - qr) <= 50].index)\n\n        df = df.loc[df['grp'].isin(qrs)]\n##############################\n\n        df.sort_values('ds', inplace=True)\n        # df.set_index(['ds', 'grp'], append=True,inplace=True)\n        # df= df.unstack(fill_value=0)\\\n        # df=df.asfreq('D', fill_value=0).stack(). \\\n        #     sort_index(level=1).reset_index()\n\n\n        try:\n            df = df.set_index(['ds', 'grp']).unstack(fill_value=0). \\\n            asfreq('D', fill_value=0).stack(). \\\n            sort_index(level=1).reset_index()\n        except:\n            df = df.groupby(['ds', 'grp']).agg({'daily_supply':['sum'],'group_id':['first'],'query_idx':['first'], 'SearchKeyValue':['first'], 'LocationKeyValue':['first']})\n            df = df.unstack(fill_value=0). \\\n                asfreq('D', fill_value=0).stack(). \\\n                sort_index(level=1).reset_index()\n            df.columns = df.columns.get_level_values(0)\n        #cls = ['year', 'month', 'week', 'quarter', 'dayofweek', 'isWeekend', 'Trend', 'weeklyChange', 'priceMov',\n        #       'Season', 'isHoliday', 'isPromotion', 'dayofyear_sin', 'dayofyear_cos', 'month_sin', 'month_cos']\n        df.sort_values('ds', inplace=True)\n        #df[cls] = df[cls].replace(to_replace=0, method='ffill')  # **********\n        #df[cls] = df[cls].replace(to_replace=0, method='bfill')\n        self.df = df\n        self.test_split_date = pd.to_datetime((end + relativedelta(days=-30)).strftime('%Y-%m-%d'))\n        ####################################################################################\n        self.val_split_date = pd.to_datetime((self.test_split_date + relativedelta(days=-90)).strftime('%Y-%m-%d'))\n\n        self.df = self.df.drop(['group_id', 'query_idx', 'SearchKeyValue', 'LocationKeyValue'], axis=1)\n\n        self.df_trains = self.df[self.df.ds <= self.val_split_date]\n        self.df_tests = self.df[self.df.ds > self.test_split_date]\n        df_train_temp = self.df[self.df.ds <= self.test_split_date]\n\n        self.df_test_org = pd.pivot_table(self.df_tests, values='daily_supply', index=['ds'], columns=['grp'])\n        self.df_train_org = pd.pivot_table(df_train_temp, values='daily_supply', index=['ds'],columns=['grp'])  # ,aggfunc=np.sum\n\n        self.df_train_org.sort_index(inplace=True)\n        # print(self.df_train_org.iloc[:,0])\n        # print(self.df_train_org.iloc[-14:,0])\n\n        # $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n        self.q_train = self.df_train_org.iloc[-14:, :].sum(axis=0) / np.sum(np.sum(self.df_train_org.iloc[-14:, :]))\n        self.q_test = self.df_test_org.sum(axis=0) / np.sum(np.sum(self.df_test_org))\n        del self.df_train_org\n        del self.df_trains\n        del self.df_tests\n        del df_train_temp\n        self.df = self.df.groupby('ds').agg(\n            {'daily_supply': 'sum'\n             #,'shifted_annual_daily_supply': 'sum', 'year': lambda x: x.iloc[0],\n             #'month': lambda x: x.iloc[0],\n             #'week': lambda x: x.iloc[0], 'quarter': lambda x: x.iloc[0], 'dayofweek': lambda x: x.iloc[0],\n             #'isWeekend': lambda x: x.iloc[0], 'Trend': lambda x: x.value_counts().index[0], 'Trend_Var': 'sum',\n             #'weeklyChange': lambda x: x.value_counts().index[0], 'priceMov': lambda x: x.value_counts().index[0],\n             #'Season': lambda x: x.iloc[0],\n             #'isHoliday': lambda x: x.iloc[0], 'isPromotion': lambda x: x.iloc[0], 'dayofyear_sin': lambda x: x.iloc[0],\n             #'dayofyear_cos': lambda x: x.iloc[0], 'month_sin': lambda x: x.iloc[0], 'month_cos': lambda x: x.iloc[0]\n             })\n\n        self.inp = input_window\n        self.out = output_window\n        self.stride = stride\n\n    def _get_data(self):\n        # strCol = ['year', 'month', 'week', 'quarter', 'dayofweek', 'isWeekend', 'Trend', 'weeklyChange', 'priceMov',\n        #           'Season', 'isHoliday',\n        #           'isPromotion']  # *******************************************************************\n        # for cols in strCol:\n        #     encoder = OneHotEncoder()\n        #     encoder_df = pd.DataFrame(encoder.fit_transform(self.df[[cols]]).toarray())\n        #     column_name = encoder.get_feature_names([cols])\n        #     encoder_df.columns = column_name\n        #     encoder_df.index = self.df.index\n        #     self.df = self.df.join(encoder_df)\n        #     self.df = self.df.drop(cols, axis=1)\n\n        Cols_minmax = ['daily_supply',\n                       # 'shifted_annual_daily_supply', 'Trend_Var', 'dayofyear_sin', 'dayofyear_cos',\n                       # 'month_sin', 'month_cos'\n                       ]\n        for cols in Cols_minmax:\n            if cols != 'shifted_annual_daily_supply':\n                self.scaler_fz = MinMaxScaler(feature_range=(-1, 1))\n                self.scaler_fz = self.scaler_fz.fit(self.df[cols].values.reshape(-1, 1))\n            if cols == 'daily_supply':\n                self.scaler_f = self.scaler_fz\n            self.df[cols] = self.scaler_fz.transform(self.df[cols].values.reshape(-1, 1))\n\n        self.df_train = self.df[self.df.index <= self.val_split_date]\n        self.df_val = self.df[(self.df.index > self.val_split_date) & (self.df.index <= self.test_split_date)]\n        self.df_test = self.df[self.df.index > self.test_split_date]\n\n        train_date, test_date = list(self.df_train.index), list(self.df_test.index)\n\n        average = 0\n\n        try: val_shape = self.df_val['daily_supply'].shape[0]\n        except:val_shape = self.df_val.shape[0]\n        self.df_train.reset_index(inplace=True, drop=True)\n        self.df_val.reset_index(inplace=True, drop=True)\n        self.df_test.reset_index(inplace=True, drop=True)\n\n\n        if self.inp > val_shape:\n            df_test = np.concatenate([self.df_train.iloc[-(self.inp - val_shape):], self.df_val, self.df_test], axis=0)\n            df_val = np.concatenate([self.df_train.iloc[-self.inp:], self.df_val], axis=0)\n        else:\n            df_test = np.concatenate([self.df_val.iloc[:, :][-self.inp:], self.df_test], axis=0)#$$$$$$need to change for more features\n            df_val = np.concatenate([self.df_train.iloc[-self.inp:], self.df_val], axis=0)#$$$$$$need to change for more features\n        self.df_test, self.df_val = df_test, df_val\n        df_test = pd.DataFrame(df_test)\n        try:df_test.columns = self.df_train.columns\n        except:df_test.name = self.df_train.name\n\n        df_val = pd.DataFrame(df_val)\n        try:df_val.columns = self.df_train.columns\n        except:df_val.name = self.df_train.name\n\n        self.df_val, self.df_test = df_val, df_test\n\n        X_train, Y_train = self.sliding_windows(self.df_train, seq_length_x=self.inp, seq_length_y=self.out,\n                                                time_step=self.stride)\n        X_val, Y_val = self.sliding_windows(self.df_val, seq_length_x=self.inp, seq_length_y=self.out,\n                                            time_step=self.stride)\n        X_test, Y_test = self.sliding_windows(self.df_test, seq_length_x=self.inp, seq_length_y=self.out,\n                                              time_step=self.stride)\n        return X_train, Y_train, X_val, Y_val, X_test, Y_test, self.df_test_org.values, self.q_train, self.q_test\n\n    def sliding_windows(self, data, seq_length_x, seq_length_y, time_step):\n        x = []\n        y = []\n        for i in range(0, len(data) - seq_length_x - seq_length_y + 1, time_step):\n            try:\n                _x = data.iloc[i:(i + seq_length_x), :].values\n                _y = data.loc[i + seq_length_x:i + seq_length_x + seq_length_y - 1,'daily_supply'].values\n            except:\n                _x = data.iloc[i:(i + seq_length_x)].values\n                _y = data.loc[i + seq_length_x:i + seq_length_x + seq_length_y - 1].values\n              # data.loc[i + seq_length_x:i + seq_length_x + seq_length_y,'daily_supply'].values\n            x.append(_x)\n            y.append(_y)\n        return np.array(x), np.array(y)\n"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": "class Data(Dataset):\n    def __init__(self, x, y):\n        np.random.seed(int(43))\n        super(Data, self).__init__()\n        # store the raw tensors\n        self._x = x\n        self._y = y\n\n    def __len__(self):\n    # a DataSet must know it size\n        return self._x.shape[0]\n\n    def __getitem__(self, index):\n        x = self._x[index, :]\n        y = self._y[index, :]\n        return x, y\n    \nclass LSTM(nn.Module):\n\n    def __init__(self, output_size, input_size=1, hidden_size=4, num_layers=1, dropout=.3,relu='False'):\n        super(LSTM, self).__init__()\n\n        self.output_size = output_size\n        self.num_layers = num_layers\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        self.relu=relu\n        # self.dp = nn.Dropout(dropout)\n        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n                            num_layers=num_layers, batch_first=True, dropout=dropout)  # .25\n        # self.batchnorm=nn.BatchNorm1d(hidden_size)\n        self.rel=nn.LeakyReLU(hidden_size)\n        # self.fc = nn.Linear(hidden_size, hidden_size)\n        # self.dp = nn.Dropout(.5)\n        self.fc=nn.Linear(hidden_size, output_size)\n    def forward(self, x):\n        for it in self.lstm.parameters():\n            w = it\n            break\n\n        h_0 = w.new_zeros(\n            self.num_layers, x.size(0), self.hidden_size, requires_grad=w.requires_grad).to(self.device)\n\n        c_0 = w.new_zeros(\n            self.num_layers, x.size(0), self.hidden_size, requires_grad=w.requires_grad).to(self.device)\n\n        # Propagate input through LSTM\n\n        h_out, _ = self.lstm(x, (h_0, c_0))\n        #h_out.requires_grad_()\n        h_out = h_out[:, -1, :]\n        h_out = h_out.reshape(-1, self.hidden_size)\n        #out = self.dp(h_out)\n\n        #out = self.batchnorm(h_out)\n        if self.relu=='True':\n            h_out=  self.rel(h_out)\n        # out=  self.fc(h_out)\n        # out = self.dp(out)\n        out = self.fc(h_out)\n        #out.requires_grad_()\n        return out\nclass LSTM3(nn.Module):\n\n    def __init__(self, output_size, input_size=1, hidden_size=4, num_layers=1,dropout=.1):\n        super(LSTM, self).__init__()\n\n        self.output_size = output_size\n        self.num_layers = num_layers\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n                            num_layers=num_layers, batch_first=True,dropout = .2)\n        self.batchnorm=nn.BatchNorm1d(hidden_size)\n        self.dp=nn.Dropout(dropout)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n\n    def forward(self, x):\n        for it in self.lstm.parameters():\n            w = it\n            break\n\n        h_0 = w.new_zeros(\n            self.num_layers, x.size(0), self.hidden_size, requires_grad=w.requires_grad).to(self.device)\n\n        c_0 = w.new_zeros(\n            self.num_layers, x.size(0), self.hidden_size, requires_grad=w.requires_grad).to(self.device)\n\n        # Propagate input through LSTM\n\n        h_out, _ = self.lstm(x, (h_0, c_0))\n        h_out.requires_grad_()\n        h_out = h_out[:, -1, :]\n        h_out = h_out.reshape(-1, self.hidden_size)\n        out=self.batchnorm(h_out)\n        out=self.dp(out)\n        out = self.fc(out)\n\n        out.requires_grad_()\n        return out\n    \n    \n\nclass LSTM3(nn.Module):\n\n    def __init__(self, output_size, input_size=1, hidden_size=4, num_layers=1,dropout=.1):\n        super(LSTM, self).__init__()\n\n        self.output_size = output_size\n        self.num_layers = num_layers\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n                            num_layers=num_layers, batch_first=True,dropout = dropout)\n        self.lstm2 = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size,\n                            num_layers=num_layers, batch_first=True,dropout = dropout)\n        self.batchnorm=nn.BatchNorm1d(hidden_size)\n        self.dp=nn.Dropout(dropout)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n\n    def forward(self, x):\n        for it in self.lstm1.parameters():\n            w = it\n            break\n\n        h_0 = w.new_zeros(\n            self.num_layers, x.size(0), self.hidden_size, requires_grad=w.requires_grad).to(self.device)\n\n        c_0 = w.new_zeros(\n            self.num_layers, x.size(0), self.hidden_size, requires_grad=w.requires_grad).to(self.device)\n        for it in self.lstm2.parameters():\n            w1 = it\n            break\n        h_1 = w1.new_zeros(\n            self.num_layers, x.size(0), self.hidden_size, requires_grad=w1.requires_grad).to(self.device)\n\n        c_1 = w1.new_zeros(\n            self.num_layers, x.size(0), self.hidden_size, requires_grad=w1.requires_grad).to(self.device)\n\n        torch.nn.init.xavier_normal_(h_0)\n        torch.nn.init.xavier_normal_(c_0)\n        torch.nn.init.xavier_normal_(h_1)\n        torch.nn.init.xavier_normal_(c_1)\n        # Propagate input through LSTM\n\n        h_out, _ = self.lstm1(x, (h_0, c_0))\n        h_out.requires_grad_()\n        #h_out = h_out[:, -1, :]\n        #h_out = h_out.reshape(-1, self.hidden_size)\n        \n        h_out1, _ = self.lstm2(h_out, (h_0, c_0))\n        h_out1.requires_grad_()\n        h_out1 = h_out1[:, -1, :]\n        h_out1 = h_out1.reshape(-1, self.hidden_size)\n        \n        out=self.batchnorm(h_out1)\n        out=self.dp(out)\n        out = self.fc(out)\n\n        out.requires_grad_()\n        return out\n"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": "def _init_fn(worker_id):\n    np.random.seed(int(43))\nclass trainLSTM:\n    def __init__(self, num_epochs, learning_rate, output_size, input_size, hidden_size, num_layers, drop_out, gamma,\n                 optimizer_name, relu=False):\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        self.num_epochs = num_epochs\n        self.learning_rate = learning_rate\n        self.output_size = output_size\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.drop_out = drop_out\n        self.lstm = LSTM(output_size, input_size, hidden_size, num_layers, drop_out, relu)\n        self.lstm.to(self.device)\n        self.criterion = torch.nn.MSELoss()  # mean-squared error for regression: MSELoss  #L1Loss\n        self.gamma = gamma\n        self.optimizer_name = optimizer_name\n        self.optimizer = getattr(torch.optim, optimizer_name)(self.lstm.parameters(), lr=learning_rate)\n        self.val_loss_ep = []\n        self.train_loss_ep = []\n        self.test_loss_ep = []\n        # torch.optim.Adam(self.lstm.parameters(), lr=learning_rate)\n\n    def train(self, trainX, trainY, valX, valY, testX, testY, batch_size=32, n_epochs_stop=10):\n        # Train the model\n        # self.lstm.train()\n        trainX, trainY = trainX.to(self.device), trainY.to(self.device)\n        valX, valY = valX.to(self.device), valY.to(self.device)\n        DF = Data(trainX, trainY)\n        trainloader = DataLoader(dataset=DF, batch_size=batch_size, shuffle=True, num_workers=0, worker_init_fn=_init_fn)  ####\n        # Early stopping\n        last_loss = 1e10\n        patience = n_epochs_stop\n        trigger_times = 0\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', patience=75, factor=self.gamma)  #ExponentialLR(self.optimizer, gamma=self.gamma)#\n        clip = 5\n        PATH = \"best_mod_agg.pt\"\n        for epoch in range(self.num_epochs):\n            train_losses = 0\n            self.lstm.train()\n            start1 = time.time()\n            for x, y in trainloader:\n                # trainX.requires_grad_()\n                # print(x[0,:5,:])\n                outputs = self.lstm(x)\n\n                # obtain the loss function\n                loss = self.criterion(outputs.reshape(-1, 1),\n                                      y.reshape(-1, 1))  # myloss(outputs.reshape(-1, 1), y.reshape(-1, 1))#.\n                # loss.requires_grad_()\n                self.optimizer.zero_grad()\n                loss.backward()  # retain_graph=True\n                # nn.utils.clip_grad_norm_(self.lstm.parameters(), clip)\n                self.optimizer.step()\n                train_losses += loss.item()  # *x.size(0)\n            train_losses = train_losses / len(DF)\n            # if epoch % 5 == 0:\n            #    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item() / batch_size))\n            self.train_loss_ep.append(train_losses)\n\n            current_loss = self.eval(valX, valY)  # Estimate(valY.data.cpu().numpy(), output.data.cpu().numpy())\n            scheduler.step(current_loss)\n            self.val_loss_ep.append(current_loss)\n            test_loss = self.eval(testX, testY)\n            self.test_loss_ep.append(test_loss)\n            if current_loss > last_loss and epoch >= 10:\n                trigger_times += 1\n                # print('Trigger Times:', trigger_times)\n                if trigger_times >= patience:\n                    print('Early stopping!\\nStart to test process.', epoch)\n                    break\n            else:\n                trigger_times = 0\n                # torch.save({\n                #     'epoch': epoch,\n                #     'model_state_dict': self.lstm.state_dict(),\n                #     'optimizer_state_dict': self.optimizer.state_dict(),\n                #     'loss': train_losses,\n                #     'val_loss':current_loss\n                # }, PATH)\n                # print(self.lstm.state_dict()['lstm.bias_hh_l4'][:5])\n                torch.save({'model': self.lstm, 'val_loss': current_loss}, PATH)\n                # print(self.lstm.state_dict())\n            last_loss = current_loss\n            # print('Epoach : ', epoch, ' train_loss: ', train_losses, ' valid_loss: ', current_loss, 'test loss',\n            #      test_loss, 'time taken', time.time() - start1)\n        checkpoint = torch.load(PATH)\n        self.lstm = checkpoint['model']\n        # self.lstm.load_state_dict(checkpoint['model_state_dict'])\n        # self.lstm=LSTM(self.output_size, self.input_size, self.hidden_size, self.num_layers, self.drop_out)\n        # self.lstm.load_state_dict(checkpoint['model_state_dict'])\n        # self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        # print('lets check the model poaram')\n        # print(self.lstm.state_dict()['lstm.bias_hh_l4'][:5])\n        current_loss = checkpoint['val_loss']\n        return current_loss\n\n    def eval(self, valX, valY, batch_size=32):\n        valX, valY = valX.to(self.device), valY.to(self.device)\n        DFv = Data(valX, valY)\n        val_loader = torch.utils.data.DataLoader(dataset=DFv, batch_size=batch_size, shuffle=False, worker_init_fn=_init_fn)\n        val_loss = 0\n        self.lstm.eval()  # prep model for evaluation\n        with torch.no_grad():\n            for x, y in val_loader:\n                outputs = self.lstm(x)\n                loss = self.criterion(y, outputs)  # Estimate(valY.data.cpu().numpy(), outputs.data.cpu().numpy())\n                val_loss += loss.item()  # *batch_size#*x.size(0)\n            current_loss = val_loss / len(DFv)\n        return current_loss\n\n    def valid(self, testX, testY, not_test=True, pred_is_req=False, df_test_org=None):\n        # evaluate the model\n        self.lstm.eval()\n        testX, testY = testX.to(self.device), testY.to(self.device)\n        test_predict = self.lstm(testX)\n        data_predict = test_predict.data.cpu().numpy()\n        dataY_plot = testY.data.cpu().numpy()  # to(self.device)\n        data_predict1 = prep.scaler_f.inverse_transform(data_predict.T)\n        try:\n            dataY_plot = prep.scaler_f.inverse_transform(dataY_plot.T)\n        except:\n            dataY_plot = dataY_plot.reshape(dataY_plot.shape[0], dataY_plot.shape[1])\n            dataY_plot = prep.scaler_f.inverse_transform(dataY_plot.T)\n        data_predict1[data_predict1 < 0] = 0\n        if not_test:\n            smape, wape, pe = Estimate(dataY_plot, data_predict1, metric='smape'), Estimate(dataY_plot, data_predict1, metric='wape'), Estimate(dataY_plot, data_predict1, metric='pe')\n            #print('wape is :', smape, wape, pe)\n            return data_predict1[:,-1], dataY_plot, smape, wape, pe\n        else:\n            smape_agg, wape_agg, pe_agg = Estimate(dataY_plot, data_predict1, metric='smape'), Estimate(dataY_plot,data_predict1, metric='wape'), Estimate(\n                dataY_plot, data_predict1, metric='pe')\n            #print('smape is: ', smape_agg, 'wape is: ', wape_agg, 'pe is:', pe_agg)\n            data_predict11 = np.tile(data_predict1.T, (q.shape[0], 1)).T\n            data_predict2 = data_predict11 * q.values.reshape(-1, 1).T\n            Wsmape, Wwape, Wpe = estimate(df_test_org, data_predict2, wq)\n            smape_dis, wape_dis, pe_dis = metric_dist(df_test_org, data_predict2, wq, metric='smape'), metric_dist( df_test_org, data_predict2, wq, metric='wape'), metric_dist(df_test_org, data_predict2, wq, metric='pe')\n#         if pred_is_req:\n#             data_predict_2 = data_predict.T\n#             data_predict_2 = prep.scaler_f.inverse_transform(data_predict_2)\n#             data_predict_2[data_predict_2 < 0] = 0\n        return data_predict2, dataY_plot, Wsmape, Wwape, Wpe, smape_dis, wape_dis, pe_dis, smape_agg[\n            0], wape_agg, pe_agg\n"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": "def train_lstm(config,save_model=False):\n    num_epochs = config['num_epochs']\n    learning_rate = config['learning_rate']\n    window_size = 30\n    output_size = 30\n    relu=config['relu']\n    input_size = train_set_in.shape[2]\n    hidden_size = config['hidden_size']\n    drop_out = config['dropout']\n    batch_size=config['batch_size']\n    gamma=config['gamma']\n    optimizer_name=config['optimizer_name']\n    num_layers=config['num_layer']\n    #dropout1=config['dropout1']\n    lstm = trainLSTM(num_epochs, learning_rate, output_size, input_size, hidden_size, num_layers, drop_out,gamma,optimizer_name,relu)\n    lstm.train(train_set_in, train_set_out,val_set_in,val_set_out,test_in_set,test_out_set,batch_size=batch_size)\n    val_pred,_, valid_smape, valid_wape,valid_pe= lstm.valid(val_set_in, val_set_out)\n    testing_predict, testing_truth, Wsmape, Wape, Wpe, smape_dis, wape_dis, pe_dis, smape_agg, wape_agg,pe_agg = lstm.valid(test_in_set, test_out_set,not_test=False,pred_is_req=True,df_test_org=df_test_org)\n    print('for sanity check', valid_smape[0], valid_wape,valid_pe, 'the mean is: ',(valid_smape[0]+valid_wape+valid_pe)/3)\n    return [Wsmape, Wape, Wpe, smape_dis, wape_dis, pe_dis, smape_agg, wape_agg,pe_agg,lstm,val_pred.reshape(-1,),valid_wape]#[training_smape*100,valid_smape*100, testing_smape*100,training_wape*100,valid_wape*100, testing_wape*100,lstm]\ndef running_lstm(config,save_model=False):\n    num_epochs = config['num_epochs']\n    learning_rate = config['learning_rate']\n    window_size = 30\n    output_size = 30\n    relu=config['relu']\n    input_size = train_set_in.shape[2]\n    hidden_size = config['hidden_size']\n    drop_out = config['dropout']\n    batch_size=config['batch_size']\n    gamma=config['gamma']\n    optimizer_name=config['optimizer_name']\n    num_layers=config['num_layer']\n    #dropout1=config['dropout1']\n    lstm = trainLSTM(num_epochs, learning_rate, output_size, input_size, hidden_size, num_layers, drop_out,gamma,optimizer_name,relu)\n    #lstm.to(Device)\n    loss=lstm.train(train_set_in, train_set_out,val_set_in,val_set_out,test_in_set,test_out_set,batch_size=batch_size)\n    _,_, Wsmape, Wwape,Wpe = lstm.valid(val_set_in, val_set_out)\n    #testing_predict, testing_truth, testing_smape,testing_wape,smape_agg,wape_agg = lstm.valid(val_set_in, val_set_out)\n    return Wsmape[0],Wwape,Wpe,lstm.lstm\n"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": "#top 50 clusters\ntest_group_id_list = [\n'/3944/1060825/447913',\n'/976759/976787/1001392/9358664',\n'/976759/976787/1001391/2392263',\n'/1115193/1071966/1072133',\n'/2636/3475115/2762884',\n'/976759/976791/6259087/1001411',\n'/3944/3951',\n'/91083/1074765',\n'/4125/546956',\n'/976759/976794/5403011/9712965',\n'/976759/976783/8102529/4007132',\n'/4125/1081404/1230089/2768744',\n'/1085666/7876574/4034645',\n'/4171/4186/1105635/2927326',\n'/976759/9176907/6053481',\n'/976759/976794/3029941/9677176',\n'/976759/976791/9551235/5459614',\n'/2637/615760',\n'/976759/976779/8399244/6065814',\n'/976760/1005863/1637840',\n'/4171/4187/133126',\n'/976759/976789/5428795/3333999',\n'/976759/976791/6259087/7801454',\n'/1005862/1071969/1074364',\n'/976759/9176907/1001468/9603576',\n'/2637/1042319',\n'/976759/9176907/1001468/4800400',\n'/5428/91416/3013177/1225084',\n'/976759/976791/5624760/1001424',\n'/976759/1071964',\n'/4171/3318550/7016108/9012469',\n'/91083/1077064/2130360',\n'/976759/9569500/1001443/8161643',\n'/4171/4187',\n'/976759/976787/1001390/5517955',\n'/976759/976782',\n'/2636/4646529/2002476',\n'/976759/9176907/1001469/2181160',\n'/2636/9206773/4782003/1224995',\n'/4171/1111647',\n'/1005862/1071969/2566660',\n'/976759/976782/1001659/7866501',\n'/976759/976794/7981173/5580286',\n'/2636/4646529/4234943',\n'/976759/976787',\n'/976760/1005863/1091528',\n'/976759/1096070/1224976/3761246',\n'/976759/976783/8438428/2725989',\n'/3944/3951/1089430/1230457',\n'/3944/1060825']"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": "# # # Tail clusters\n# test_group_id_list = [\n#  '/1005862/1007220/1044020/3392907',\n#  '/3944/77622/8375901/1230415/1230417',\n#  '/4171/645779/2165088/3723286',\n#  '/1334134/5899871/7445675/5456260',\n#  '/5428/1102183/8617474/1564368',\n#  '/2637/5249422/5222147',\n#  '/1334134/6355365/1285843/3783885',\n#  '/976759/976787/1001390/2523039/2119183',\n#  '/91083/2962826/8564995/2827548',\n#  '/4125/4134/1026285/9642310/1251682',\n#  '/1005862/1007220/1007399/6461098',\n#  '/5427/133283/9095197',\n#  '/1115193/1071967/1149380/5135111',\n#  '/976759/976794/5403011/9731488',\n#  '/4125/4134/2350251/1078395',\n#  '/5428/8018905/5374562/7304919/6954191',\n#  '/6197502/5702707/9458810/9028847',\n#  '/91083/1074769/6350088',\n#  '/5440/1001299/6148116',\n#  '/1334134/6172404/7659444/3836217',\n#  '/3920/1987289/582374/8324310/1974555',\n#  '/4125/4161/4165/1075707',\n#  '/976760/1414629/6578740/1310637',\n#  '/91083/1074765/9183778/3753296',\n#  '/3920/582053/584177/585913',\n#  '/976760/1005863/8121112',\n#  '/1105910/133161/1072306/5297453/5115891',\n#  '/976760/1005860/4157476/8398146/6630246',#\n#  '/2637/6749799/2053705',\n#  '/1072864/1067612/6801317/3181011/4259773',\n#  '/1085666/5859906/3311357',\n#  '/976760/9575239/2981737',\n#  '/2637/9489599/2275107',\n#  '/4125/4134/1078384/1078388',\n#  '/5440/202073/1749780/3053202/2610736',\n#  '/1105910/133161/1103334/3123349/9119436',\n#  '/1334134/6172404/4854691/2291474',\n#  '/4171/7357538/6711451/4147327',\n#  '/1334134/8495017/8541719',\n#  '/4125/546956/4128/4547850/2751930',\n#  '/1072864/1067618/1230774/8250798/6489704',\n#  '/1072864/1808984/4037404/9537015',#\n#  '/1085666/3316357/6852893/3607949',\n#  '/1229749/6562796/6709176',\n#  '/91083/1077064/8595272/2238009',#\n#  '/5440/9672181',\n#  '/1072864/1067618/1230774/8250798/6489704',\n#  '/1085666/3147628/8028415/6121950',#\n#  '/4044/103150/4038/7389568/2455597',#\n#  '/976760/1876667/7219872']"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": "start_month=['2020-09-01','2020-10-01','2020-11-01','2020-12-01','2021-01-01','2021-02-01','2021-03-01',\n            '2021-04-01','2021-05-01','2021-06-01','2021-07-01','2021-08-01',]"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "September  :  2020-09-01\nOctober  :  2020-10-01\nNovember  :  2020-11-01\nDecember  :  2020-12-01\nJanuary  :  2021-01-01\nFebruary  :  2021-02-01\nMarch  :  2021-03-01\nApril  :  2021-04-01\nMay  :  2021-05-01\nJune  :  2021-06-01\nJuly  :  2021-07-01\nAugust  :  2021-08-01\n"}], "source": "name=['September','October','November','December','January','February','March','April','May','June','July','August']\nfor i in range(len(name)):\n    print(name[i],' : ',start_month[i])"}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": "def SMAPE(A, F):\n    smape_mean = 0\n    for i in range(A.shape[0]):\n        if A[i] == F[i]:\n            continue\n        smape_mean += 2 * np.abs(F[i] - A[i]) / (np.abs(A[i]) + np.abs(F[i]))\n    smape_mean = 1 / A.shape[0] * smape_mean\n    return smape_mean\ndef SMAPE2(A, F):\n    smape_mean = 0\n    for i in range(A.shape[0]):\n        if A[i] == F[i]:\n            continue\n        smape_mean += 2 * np.abs(F[i] - A[i]) / (np.abs(A[i]) + np.abs(F[i]))\n    smape_mean = 1 / A.shape[0] * smape_mean\n    return smape_mean\ndef WAPE(A, F):\n    return np.sum((np.abs(F - A))/np.sum(A))\ndef weightedSMAPE(A,F,wq):\n    res=0\n    print('number of smape: ',len(wq))\n    for i in range(len(wq)):\n        res+=wq[i]*SMAPE(A[:,i],F[:,i])\n    return res\ndef WAPE_fun(actual,predicted,w):\n    #w=query percentage in the cluster\n    wape_tot=0\n    #print('number of smape: ', len(w))\n    for i in range(len(w)):\n        if np.sum(actual[:,i].reshape(-1,1))<=0:\n            continue\n        wape_tot+=w[i]*WAPE(actual[:,i].reshape(-1,1),predicted[:,i].reshape(-1,1))\n    return wape_tot\ndef PE(A, F):\n    return np.abs(np.sum(F)-np.sum(A))/np.sum(A)\ndef PE_fun(actual,predicted,w):\n    wape_tot = 0\n    for i in range(len(w)):\n        if np.sum(actual[:, i].reshape(-1, 1)) <= 0:\n            continue\n        wape_tot += w[i] * PE(actual[:, i].reshape(-1, 1), predicted[:, i].reshape(-1, 1))\n    return wape_tot\ndef estimate(actual,pred,q):\n    if type(q)==pd.core.series.Series:q=q.values\n    #pred=pred * q.reshape(-1, )\n    smape_error = weightedSMAPE(actual, pred, q)\n    wape=WAPE_fun(actual,pred,q)\n    pe=PE_fun(actual,pred,q)\n    return smape_error,wape,pe\ndef metric_dist(A,F,q,metric='wape'):\n    res=[]\n    if metric=='wape':\n        for i in range(len(q)):\n            if np.sum(A[:, i].reshape(-1, 1)) <= 0:\n                continue\n            res.append(WAPE(A[:, i].reshape(-1, 1), F[:, i].reshape(-1, 1)))\n    elif metric=='smape':\n        for i in range(len(q)):\n            if np.sum(A[:, i].reshape(-1, 1)) <= 0:\n                continue\n            res.append(SMAPE(A[:, i].reshape(-1, 1), F[:, i].reshape(-1, 1)))\n    else:\n        for i in range(len(q)):\n            if np.sum(A[:, i].reshape(-1, 1)) <= 0:\n                continue\n            res.append(PE(A[:, i].reshape(-1, 1), F[:, i].reshape(-1, 1)))\n    return [np.percentile(res,50),np.percentile(res,80),np.percentile(res,90)]\n\n\ndef Estimate(actual,pred,metric='wape'):\n    if metric=='pe':return PE(actual,pred)\n    elif metric=='wape':return WAPE(actual.reshape(-1,1),pred.reshape(-1,1))\n    elif metric=='smape':return SMAPE(actual.reshape(-1,1),pred.reshape(-1,1))\n    else:\n        raise ValueError('incorrect metric is defined')\ndef myloss(A,F):\n    #loss=torch.mean(np.abs((actual.detach().numpy() - predict.detach().numpy()) / actual))\n    loss=torch.mean(1 / A.size(dim=0)* torch.sum(2 * torch.abs(F - A) / (torch.abs(A) + torch.abs(F))))\n    return loss#torch.from_numpy(np.asarray(loss))"}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": "def objective(trial):\n    params={\n        #\"num_layers\":trial.suggest_int(\"num_layer\",1,2),\n        'optimizer_name':trial.suggest_categorical(\"optimizer_name\", [\"Adam\",'RMSprop','AdamW','SGD']),#\"Adam\",'RMSprop',\n        \"hidden_size\":trial.suggest_int('hidden_size',8,20),\n        'dropout':trial.suggest_uniform(\"dropout\",0,0.10),\n        #'dropout1':trial.suggest_uniform(\"dropout1\",0,0.3),\n        'learning_rate':trial.suggest_loguniform('learning_rate',1e-4,1e-0),\n        'num_epochs':trial.suggest_int('num_epochs',100,100),\n        'batch_size':trial.suggest_int('batch_size',32,64),\n        'gamma':trial.suggest_uniform(\"gamma\",0.8,1),\n        'num_layer':trial.suggest_int('num_layer',1,3),\n        'relu':trial.suggest_categorical('relu',['True','False'])}\n    temp_loss=running_lstm(params,save_model=False)\n    return temp_loss\n"}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": "from torch.optim.lr_scheduler import ExponentialLR\n\nTimesL=[]\nSMAPEsL=[]\nPEsL=[]\nWAPEsL=[]\nAggSmapeL=[]\nAggWapeL=[]\nAggPeL=[]\ntimes_org=[]\nmedian_smape,median_wape,median_pe=[],[],[]\nEighty_per_smape,Eighty_per_wape,Eighty_per_pe=[],[],[]\nNinty_per_smape,Ninty_per_wape,Ninty_per_pe=[],[],[]\nwape_agg_lstm=np.zeros(50, dtype=np.float64)\ndf_lstm=np.zeros((30,50), dtype=np.float64)"}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [], "source": "def seed_torch(seed=43):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\nseed_torch()\n"}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": "def check_lstm(config,model):\n    num_epochs = config['num_epochs']\n    learning_rate = config['learning_rate']\n    window_size = 30\n    output_size = 30\n    relu=config['relu']\n    input_size = train_set_in.shape[2]\n    hidden_size = config['hidden_size']\n    drop_out = config['dropout']\n    batch_size=config['batch_size']\n    gamma=config['gamma']\n    optimizer_name=config['optimizer_name']\n    num_layers=config['num_layer']\n    #dropout1=config['dropout1']\n    lstm = trainLSTM(num_epochs, learning_rate, output_size, input_size, hidden_size, num_layers, drop_out,gamma,optimizer_name,relu)\n    #lstm.to(Device)\n    lstm.lstm=model\n    #_,_, Wsmape, Wwape,Wpe = lstm.valid(val_set_in, val_set_out)\n    val_pred,_, valid_smape, valid_wape,valid_pe= lstm.valid(val_set_in, val_set_out)\n    testing_predict, testing_truth, Wsmape, Wape, Wpe, smape_dis, wape_dis, pe_dis, smape_agg, wape_agg,pe_agg = lstm.valid(test_in_set, test_out_set,not_test=False,pred_is_req=True,df_test_org=df_test_org)\n    print('for sanity check', valid_smape[0], valid_wape,valid_pe, 'the mean is: ',(valid_smape[0]+valid_wape+valid_pe)/3)\n    return [Wsmape, Wape, Wpe, smape_dis, wape_dis, pe_dis, smape_agg, wape_agg,pe_agg,lstm,val_pred.reshape(-1,),valid_wape]#[training_smape*100,valid_smape*100, testing_smape*100,training_wape*100,valid_wape*100, testing_wape*100,lstm]"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Running for the month of : September\ncluster id:  1\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.14864657819271088 wape is 0.14020881056785583 and pe is 0.06416800618171692 and total is 0.11767446001370747\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\ntrial run:  4\ntrial run:  5\nthe  smape is 0.1619293987751007 wape is 0.15053604543209076 and pe is 0.028799233958125114 and total is 0.11375488837560017\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  751\nfor sanity check 0.1619294 0.15053605 0.028799234 the mean is:  0.11375488837560017\nval wape is:  0.15053604543209076\ntotal time  -191.39099884033203\ntest Wsmape:  0.2975160837302321  test Wape:  0.2961820364407199  test Wpe:  0.19537138530423412\nSmape dis [0.5609963028325597, 0.7998104372051873, 0.9387321932604608] Wape dis [0.5427279683100066, 0.7897630161604843, 0.9266831406524474] PE dis [0.2318303961112487, 0.4879603846417363, 0.7151413938236394]\nAgg SMAPE:  0.18129113 Agg WAPE:  0.18456796 Agg PE:  0.10312165\ntotal time  -191.39099884033203\n===============================================\ncluster id:  2\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.19480480253696442 wape is 0.18839184939861298 and pe is 0.08274734020233154 and total is 0.15531466404596964\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\nthe  smape is 0.18984441459178925 wape is 0.1836245357990265 and pe is 0.08819037675857544 and total is 0.15388643741607666\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\nnumber of smape:  262\nfor sanity check 0.18984441 0.18362454 0.08819038 the mean is:  0.15388643741607666\nval wape is:  0.1836245357990265\ntotal time  -186.80035281181335\ntest Wsmape:  0.9517995760998587  test Wape:  0.6901325930334411  test Wpe:  0.6740341813008532\nSmape dis [0.7443376070999033, 0.9253911318018133, 1.037017681498811] Wape dis [0.6247616428966513, 0.7256592711585479, 0.77689311840247] PE dis [0.5112922459213256, 0.6802366227453446, 0.7379929867094573]\nAgg SMAPE:  0.90916866 Agg WAPE:  0.670097 Agg PE:  0.670097\ntotal time  -186.80035281181335\n===============================================\ncluster id:  3\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.1342654824256897 wape is 0.13024607300758362 and pe is 0.07818040251731873 and total is 0.11423065265019734\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  497\nfor sanity check 0.13426548 0.13024607 0.0781804 the mean is:  0.11423065265019734\nval wape is:  0.13024607300758362\ntotal time  -186.55605792999268\ntest Wsmape:  0.692831808176359  test Wape:  0.5520904100376333  test Wpe:  0.5049056099429929\nSmape dis [0.6611682592321555, 0.8401092869726376, 0.9962022714247235] Wape dis [0.578603127483893, 0.6947835530758372, 0.7607736392941296] PE dis [0.3985654077309583, 0.5945216808797941, 0.6999961715635583]\nAgg SMAPE:  0.5821601 Agg WAPE:  0.47697294 Agg PE:  0.47115707\ntotal time  -186.55605792999268\n===============================================\ncluster id:  4\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.25708359479904175 wape is 0.2421969622373581 and pe is 0.19759875535964966 and total is 0.23229310909907022\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\nthe  smape is 0.22327455878257751 wape is 0.2115412801504135 and pe is 0.1724718064069748 and total is 0.20242921511332193\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  3\ntrial run:  4\ntrial run:  5\nthe  smape is 0.22674345970153809 wape is 0.21261224150657654 and pe is 0.16206538677215576 and total is 0.20047370592753092\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\nEarly stopping!\nStart to test process. 46\nthe  smape is 0.22625267505645752 wape is 0.2151115983724594 and pe is 0.007483364082872868 and total is 0.14961588382720947\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  414\nfor sanity check 0.22625268 0.2151116 0.007483364 the mean is:  0.14961588382720947\nval wape is:  0.2151115983724594\ntotal time  -181.9619607925415\ntest Wsmape:  0.4124451768476899  test Wape:  0.36992699413210983  test Wpe:  0.29194599354149026\nSmape dis [0.5239184318310555, 0.7440541750414166, 0.8385606691151082] Wape dis [0.49189284807981165, 0.672472769132755, 0.7840772211652337] PE dis [0.2255768764012969, 0.4334342956632178, 0.5850632706455594]\nAgg SMAPE:  0.18745206 Agg WAPE:  0.17538524 Agg PE:  0.14382638\ntotal time  -181.9619607925415\n===============================================\ncluster id:  5\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.8735124468803406 wape is 1.1546149253845215 and pe is 0.3966533839702606 and total is 0.8082602818806967\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\nthe  smape is 0.8009129762649536 wape is 1.088086485862732 and pe is 0.41998255252838135 and total is 0.7696606318155924\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  3\ntrial run:  4\nthe  smape is 0.7756369709968567 wape is 1.0533684492111206 and pe is 0.39128175377845764 and total is 0.7400957743326823\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  5\ntrial run:  6\nthe  smape is 0.8073059916496277 wape is 0.881227433681488 and pe is 0.02832154557108879 and total is 0.572284976641337\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\nthe  smape is 0.4770084619522095 wape is 0.6067104339599609 and pe is 0.3447098433971405 and total is 0.4761429230372111\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\nnumber of smape:  11\nfor sanity check 0.47700846 0.60671043 0.34470984 the mean is:  0.4761429230372111\nval wape is:  0.6067104339599609\ntotal time  -186.2498950958252\ntest Wsmape:  1.2552534620218747  test Wape:  0.9470156252864107  test Wpe:  0.9211623537586227\nSmape dis [0.8450375598356804, 0.9751814469575949, 1.2799856837360788] Wape dis [0.9076921113594492, 0.9595254393599582, 1.043976387607475] PE dis [0.3742816682843032, 0.6659339223619051, 0.9442715166988115]\nAgg SMAPE:  1.1922519 Agg WAPE:  0.9371351 Agg PE:  0.9202667\ntotal time  -186.2498950958252\n===============================================\ncluster id:  6\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.15536299347877502 wape is 0.1489751935005188 and pe is 0.02771545760333538 and total is 0.11068454384803772\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\nthe  smape is 0.14476104080677032 wape is 0.13885949552059174 and pe is 0.012443460524082184 and total is 0.09868799646695454\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  3\ntrial run:  4\nthe  smape is 0.13772346079349518 wape is 0.13196468353271484 and pe is 0.005268510431051254 and total is 0.09165221452713013\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\nEarly stopping!\nStart to test process. 99\nthe  smape is 0.13840357959270477 wape is 0.1325949877500534 and pe is 3.3550081752764527e-06 and total is 0.09033397833506267\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  202\nfor sanity check 0.13840358 0.13259499 3.3550082e-06 the mean is:  0.09033397833506267\nval wape is:  0.1325949877500534\ntotal time  -187.5533788204193\ntest Wsmape:  1.0845748873897518  test Wape:  0.7584016411756946  test Wpe:  0.7505240952684394\nSmape dis [0.9268463071871067, 1.118049691719625, 1.2839226631356713] Wape dis [0.7110541659746721, 0.7954343065526014, 0.8489651986330305] PE dis [0.6853686161917598, 0.7844450809433191, 0.8356167513419249]\nAgg SMAPE:  1.0586249 Agg WAPE:  0.7469659 Agg PE:  0.74696594\ntotal time  -187.5533788204193\n===============================================\ncluster id:  7\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.2232007086277008 wape is 0.2472226321697235 and pe is 0.2360864281654358 and total is 0.23550325632095337\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\nthe  smape is 0.19949959218502045 wape is 0.21823027729988098 and pe is 0.2064867913722992 and total is 0.20807222525278726\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  4\nthe  smape is 0.1604103296995163 wape is 0.17436519265174866 and pe is 0.13117769360542297 and total is 0.15531773368517557\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\nthe  smape is 0.11559387296438217 wape is 0.11968053877353668 and pe is 0.035147007554769516 and total is 0.09014047185579936\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\nnumber of smape:  901\nfor sanity check 0.11559387 0.11968054 0.035147008 the mean is:  0.09014047185579936\nval wape is:  0.11968053877353668\ntotal time  -190.80856323242188\ntest Wsmape:  0.7190090594447431  test Wape:  0.6679775961029542  test Wpe:  0.3309157223012169\nSmape dis [0.818242524054085, 1.0017042964681677, 1.1045752302557648] Wape dis [0.7300973341081253, 0.928340561243481, 1.1224752033291827] PE dis [0.27618041196649523, 0.5187659042193853, 0.682170341961486]\nAgg SMAPE:  0.19425587 Agg WAPE:  0.19470479 Agg PE:  0.08357436\ntotal time  -190.80856323242188\n===============================================\ncluster id:  8\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.1388513445854187 wape is 0.14531932771205902 and pe is 0.13057105243206024 and total is 0.13824725151062012\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\ntrial run:  4\ntrial run:  5\ntrial run:  6\nthe  smape is 0.12601076066493988 wape is 0.12995898723602295 and pe is 0.1160903126001358 and total is 0.12402002016703288\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\nthe  smape is 0.11541831493377686 wape is 0.11933279037475586 and pe is 0.1134251281619072 and total is 0.11605874697367351\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\nnumber of smape:  786\nfor sanity check 0.115418315 0.11933279 0.11342513 the mean is:  0.11605874697367351\nval wape is:  0.11933279037475586\ntotal time  -187.4871847629547\ntest Wsmape:  0.7339880118401066  test Wape:  0.9372335769021725  test Wpe:  0.6907015912458557\nSmape dis [0.8638721587331211, 1.1749334422956546, 1.3200920273042418] Wape dis [0.9574390324002704, 1.6655635123394485, 2.2808190844428458] PE dis [0.6427168276517221, 1.3786269875776624, 2.096742087111955]\nAgg SMAPE:  0.46773827 Agg WAPE:  0.59905845 Agg PE:  0.5990582\ntotal time  -187.4871847629547\n===============================================\ncluster id:  9\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.13514550030231476 wape is 0.14058978855609894 and pe is 0.1138777881860733 and total is 0.1298710306485494\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\nthe  smape is 0.12298239767551422 wape is 0.12749513983726501 and pe is 0.08192085474729538 and total is 0.11079947153727214\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  3\ntrial run:  4\nthe  smape is 0.09029349684715271 wape is 0.09228822588920593 and pe is 0.058369215577840805 and total is 0.08031698067982991\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  5\ntrial run:  6\nthe  smape is 0.07214955240488052 wape is 0.07266178727149963 and pe is 0.018289851024746895 and total is 0.054367060462633766\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\nnumber of smape:  743\nfor sanity check 0.07214955 0.07266179 0.018289851 the mean is:  0.054367060462633766\nval wape is:  0.07266178727149963\ntotal time  -188.06559085845947\ntest Wsmape:  0.6723234979805491  test Wape:  0.7176513797177058  test Wpe:  0.42751602879606054\nSmape dis [0.8053591505305961, 1.0449494814517268, 1.2187914231913635] Wape dis [0.7865194120761066, 1.2195693144012472, 1.553287145210414] PE dis [0.37329546124420376, 0.8377599135590001, 1.2177587755954276]\nAgg SMAPE:  0.2714877 Agg WAPE:  0.28974026 Agg PE:  0.22333132\ntotal time  -188.06559085845947\n===============================================\ncluster id:  10\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.1238894909620285 wape is 0.12077831476926804 and pe is 0.07010394334793091 and total is 0.10492391387621562\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\nthe  smape is 0.10755270719528198 wape is 0.10592412948608398 and pe is 0.028261957690119743 and total is 0.08057959874471028\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  3\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\nthe  smape is 0.11013906449079514 wape is 0.10876245051622391 and pe is 0.00240292027592659 and total is 0.07376814385255177\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\nthe  smape is 0.1051798090338707 wape is 0.1037934422492981 and pe is 0.0005350019200704992 and total is 0.06983608504136403\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\nnumber of smape:  305\nfor sanity check 0.10517981 0.10379344 0.0005350019 the mean is:  0.06983608504136403\nval wape is:  0.1037934422492981\ntotal time  -189.3646523952484\ntest Wsmape:  0.868232189906101  test Wape:  0.6612106834628276  test Wpe:  0.6404077337542503\nSmape dis [0.7391909793013006, 0.99564534878048, 1.109712782738943] Wape dis [0.6233658320592131, 0.7548279447292243, 0.7958204371406481] PE dis [0.510907127043798, 0.7282805113876338, 0.7829518085893936]\nAgg SMAPE:  0.7763762 Agg WAPE:  0.60849524 Agg PE:  0.60849524\ntotal time  -189.3646523952484\n===============================================\ncluster id:  11\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.1341545283794403 wape is 0.13244730234146118 and pe is 0.0025663459673523903 and total is 0.08972272276878357\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\nthe  smape is 0.12374832481145859 wape is 0.12219397723674774 and pe is 0.0010643542045727372 and total is 0.08233555157979329\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\nnumber of smape:  306\nfor sanity check 0.123748325 0.12219398 0.0010643542 the mean is:  0.08233555157979329\nval wape is:  0.12219397723674774\ntotal time  -189.23898100852966\ntest Wsmape:  0.9294815328351423  test Wape:  0.6708704130930944  test Wpe:  0.6458693252551657\nSmape dis [0.6700093630952193, 0.8744917865648797, 1.057515801493842] Wape dis [0.585415914691552, 0.7109674103575941, 0.7916619741073431] PE dis [0.46610708567409675, 0.6536974416185806, 0.7570126561007469]\nAgg SMAPE:  0.83045286 Agg WAPE:  0.63056076 Agg PE:  0.6305607\ntotal time  -189.23898100852966\n===============================================\ncluster id:  12\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.16189400851726532 wape is 0.16602814197540283 and pe is 0.08523333072662354 and total is 0.13771849870681763\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\ntrial run:  4\ntrial run:  5\ntrial run:  6\nthe  smape is 0.16159756481647491 wape is 0.1599055528640747 and pe is 0.028571616858243942 and total is 0.11669156948725383\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  7\nthe  smape is 0.14745840430259705 wape is 0.14956901967525482 and pe is 0.0435086265206337 and total is 0.11351200938224792\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  8\nthe  smape is 0.1647862046957016 wape is 0.16547000408172607 and pe is 0.0023938496597111225 and total is 0.11088335514068604\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  9\ntrial run:  10\ntrial run:  11\nthe  smape is 0.140436589717865 wape is 0.1409597247838974 and pe is 0.016490628942847252 and total is 0.0992956558863322\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\nEarly stopping!\nStart to test process. 136\ntrial run:  19\ntrial run:  20\ntrial run:  21\nthe  smape is 0.1129867434501648 wape is 0.11255903542041779 and pe is 0.04119989648461342 and total is 0.08891522884368896\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  215\nfor sanity check 0.11298674 0.112559035 0.041199896 the mean is:  0.08891522884368896\nval wape is:  0.11255903542041779\ntotal time  -184.2036623954773\ntest Wsmape:  0.7227296038078813  test Wape:  1.1278696402372455  test Wpe:  1.0527837147624628\nSmape dis [0.8967028198577659, 1.098717953591028, 1.2146551971900772] Wape dis [1.2102227559156207, 1.778649324880059, 2.06305480417748] PE dis [0.9162323688066067, 1.5384004545750927, 1.8727859015346437]\nAgg SMAPE:  0.6887028 Agg WAPE:  1.078634 Agg PE:  1.0301983\ntotal time  -184.2036623954773\n===============================================\ncluster id:  13\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.13794149458408356 wape is 0.1309662163257599 and pe is 0.080286405980587 and total is 0.11639804641405742\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\nthe  smape is 0.10705476999282837 wape is 0.10495404899120331 and pe is 0.047305501997470856 and total is 0.08643810947736104\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\nEarly stopping!\nStart to test process. 32\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  517\nfor sanity check 0.10705477 0.10495405 0.047305502 the mean is:  0.08643810947736104\nval wape is:  0.10495404899120331\ntotal time  -182.4704315662384\ntest Wsmape:  0.46484658054784456  test Wape:  0.39098395394847685  test Wpe:  0.3256690642778302\nSmape dis [0.5102648917893631, 0.766290743765839, 0.9582530288367963] Wape dis [0.48476589932426545, 0.6833489802005505, 0.8110016717893551] PE dis [0.21527947134088124, 0.47746160745716243, 0.7151366542286506]\nAgg SMAPE:  0.24269068 Agg WAPE:  0.22097763 Agg PE:  0.21711935\ntotal time  -182.4704315662384\n===============================================\ncluster id:  14\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.1463516503572464 wape is 0.14338171482086182 and pe is 0.10072695463895798 and total is 0.1301534374554952\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\nthe  smape is 0.13760118186473846 wape is 0.13615325093269348 and pe is 0.061479367315769196 and total is 0.11174459258715312\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  4\ntrial run:  5\nthe  smape is 0.13982181251049042 wape is 0.13876628875732422 and pe is 0.04576975852251053 and total is 0.10811928908030193\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\nthe  smape is 0.1284344494342804 wape is 0.12758100032806396 and pe is 0.056873518973588943 and total is 0.10429632663726807\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  462\nfor sanity check 0.12843445 0.127581 0.05687352 the mean is:  0.10429632663726807\nval wape is:  0.12758100032806396\ntotal time  -187.75834894180298\ntest Wsmape:  0.7040126061976242  test Wape:  0.5747624257419327  test Wpe:  0.4671239216440135\nSmape dis [0.6399481381017194, 0.8463415580071236, 0.9435708738109972] Wape dis [0.6345042914790358, 0.8646655879859566, 1.0262162958407621] PE dis [0.3085051317860171, 0.5688030858162457, 0.8201817062105021]\nAgg SMAPE:  0.31713033 Agg WAPE:  0.28792307 Agg PE:  0.2879231\ntotal time  -187.75834894180298\n===============================================\ncluster id:  15\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.4245480000972748 wape is 0.3684135377407074 and pe is 0.32970404624938965 and total is 0.3742218812306722\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\nthe  smape is 0.36470699310302734 wape is 0.3265421986579895 and pe is 0.2691372334957123 and total is 0.32012879848480225\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\nnumber of smape:  38\nfor sanity check 0.364707 0.3265422 0.26913723 the mean is:  0.32012879848480225\nval wape is:  0.3265421986579895\ntotal time  -187.32640528678894\ntest Wsmape:  1.198052611591336  test Wape:  0.8104471572442892  test Wpe:  0.8094195927284132\nSmape dis [1.0215864709891254, 1.2188407374497494, 1.2804198962521642] Wape dis [0.7623211718076116, 0.8237224282426098, 0.8426745328970017] PE dis [0.7344231285132261, 0.8196372479397466, 0.8381260568421005]\nAgg SMAPE:  1.1946248 Agg WAPE:  0.80941963 Agg PE:  0.80941963\ntotal time  -187.32640528678894\n===============================================\ncluster id:  16\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.12627452611923218 wape is 0.12558141350746155 and pe is 0.04804234951734543 and total is 0.09996609886487325\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\nthe  smape is 0.11074607074260712 wape is 0.10865120589733124 and pe is 0.020034851506352425 and total is 0.07981070876121521\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  441\nfor sanity check 0.11074607 0.108651206 0.020034852 the mean is:  0.07981070876121521\nval wape is:  0.10865120589733124\ntotal time  -187.29383754730225\ntest Wsmape:  0.96169418718733  test Wape:  0.6892540102099447  test Wpe:  0.6591772721143437\nSmape dis [0.7964634903353782, 1.069544005194346, 1.194751530086793] Wape dis [0.652479499224424, 0.7736699225380661, 0.8320623366245777] PE dis [0.5567568549089658, 0.7473876794276338, 0.8172712729421262]\nAgg SMAPE:  0.871863 Agg WAPE:  0.64835507 Agg PE:  0.64835495\ntotal time  -187.29383754730225\n===============================================\ncluster id:  17\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.14348433911800385 wape is 0.13939066231250763 and pe is 0.09351630508899689 and total is 0.12546376387278238\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\ntrial run:  4\ntrial run:  5\nthe  smape is 0.1348820924758911 wape is 0.13140738010406494 and pe is 0.07385573536157608 and total is 0.11338173349698384\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\nthe  smape is 0.1320079267024994 wape is 0.12915121018886566 and pe is 0.07610145211219788 and total is 0.11242019136746724\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\nthe  smape is 0.1279466450214386 wape is 0.12591487169265747 and pe is 0.04800090566277504 and total is 0.1006208062171936\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  22\ntrial run:  23\nthe  smape is 0.1339370310306549 wape is 0.1328049600124359 and pe is 0.020595824345946312 and total is 0.09577926993370056\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  199\nfor sanity check 0.13393703 0.13280496 0.020595824 the mean is:  0.09577926993370056\nval wape is:  0.1328049600124359\ntotal time  -187.18598341941833\ntest Wsmape:  0.742189897578649  test Wape:  0.5944494551675806  test Wpe:  0.5717351465995281\nSmape dis [0.6544059371817383, 0.8990248530007395, 0.9800424564897385] Wape dis [0.5631560384063736, 0.7016421262384229, 0.7425823727434925] PE dis [0.4909243551966169, 0.6629351095107754, 0.7131513290347954]\nAgg SMAPE:  0.6837027 Agg WAPE:  0.5641842 Agg PE:  0.5619536\ntotal time  -187.18598341941833\n===============================================\ncluster id:  18\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.1751999706029892 wape is 0.1804744154214859 and pe is 0.008607033640146255 and total is 0.12142713864644368\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\nthe  smape is 0.15321725606918335 wape is 0.15452632308006287 and pe is 0.028032595291733742 and total is 0.1119253933429718\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  3\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\nthe  smape is 0.14965002238750458 wape is 0.15189233422279358 and pe is 0.023614551872015 and total is 0.10838563243548076\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\nEarly stopping!\nStart to test process. 70\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  114\nfor sanity check 0.14965002 0.15189233 0.023614552 the mean is:  0.10838563243548076\nval wape is:  0.15189233422279358\ntotal time  -182.99367260932922\ntest Wsmape:  1.088597489630758  test Wape:  0.7459941064880978  test Wpe:  0.721778841666879\nSmape dis [0.9164293035444917, 1.1704168296764355, 1.2526979988714113] Wape dis [0.7140692965533236, 0.7977947254976305, 0.8228754812240384] PE dis [0.6496326235504952, 0.7738988280599342, 0.8126122201670195]\nAgg SMAPE:  1.0746644 Agg WAPE:  0.71890193 Agg PE:  0.71890193\ntotal time  -182.99367260932922\n===============================================\ncluster id:  19\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.32937106490135193 wape is 0.30805137753486633 and pe is 0.05293629318475723 and total is 0.23011958599090576\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\nthe  smape is 0.3243998885154724 wape is 0.30361437797546387 and pe is 0.03533914312720299 and total is 0.2211177945137024\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  4\ntrial run:  5\nthe  smape is 0.3197728097438812 wape is 0.29998689889907837 and pe is 0.04322841390967369 and total is 0.22099602222442627\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  52\nfor sanity check 0.3197728 0.2999869 0.043228414 the mean is:  0.22099602222442627\nval wape is:  0.29998689889907837\ntotal time  -190.8888065814972\ntest Wsmape:  1.0746741933553077  test Wape:  0.7662950113038015  test Wpe:  0.7642501309050308\nSmape dis [0.9678498605204879, 1.0944925292024978, 1.1259610184171345] Wape dis [0.7288698566242793, 0.7838658836123323, 0.7968787369331671] PE dis [0.7002305392262632, 0.7768806260529512, 0.7883786896434618]\nAgg SMAPE:  1.0662749 Agg WAPE:  0.76425016 Agg PE:  0.7642501\ntotal time  -190.8888065814972\n===============================================\ncluster id:  20\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.4812327027320862 wape is 0.4242566227912903 and pe is 0.4177503287792206 and total is 0.44107989470163983\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\nthe  smape is 0.43261390924453735 wape is 0.3988959789276123 and pe is 0.3905477523803711 and total is 0.4073525269826253\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  3\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\nthe  smape is 0.4302070438861847 wape is 0.39705604314804077 and pe is 0.3899315297603607 and total is 0.40573155879974365\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\nEarly stopping!\nStart to test process. 32\ntrial run:  18\nEarly stopping!\nStart to test process. 29\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\nthe  smape is 0.4177776575088501 wape is 0.38837265968322754 and pe is 0.38254332542419434 and total is 0.396231214205424\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\nnumber of smape:  24\nfor sanity check 0.41777766 0.38837266 0.38254333 the mean is:  0.396231214205424\nval wape is:  0.38837265968322754\ntotal time  -174.4357132911682\ntest Wsmape:  1.0135122952562865  test Wape:  0.6931288278250997  test Wpe:  0.688612016846456\nSmape dis [0.8332140287221519, 0.9976148722040172, 1.0864793175511436] Wape dis [0.6441052483570622, 0.7339788535298757, 0.8059842543812019] PE dis [0.6201303484626974, 0.721325085005952, 0.765064429860133]\nAgg SMAPE:  0.9502467 Agg WAPE:  0.6501283 Agg PE:  0.6501283\ntotal time  -174.4357132911682\n===============================================\ncluster id:  21\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.12318529933691025 wape is 0.12115193903446198 and pe is 0.039875805377960205 and total is 0.09473767876625061\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\nthe  smape is 0.11972712725400925 wape is 0.11839757859706879 and pe is 0.024340534582734108 and total is 0.08748841285705566\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\nnumber of smape:  266\nfor sanity check 0.11972713 0.11839758 0.024340535 the mean is:  0.08748841285705566\nval wape is:  0.11839757859706879\ntotal time  -186.6455910205841\ntest Wsmape:  0.7052878312887855  test Wape:  0.5857168332326755  test Wpe:  0.48244070515267345\nSmape dis [0.6040737747032603, 0.8342879169344808, 1.007554377955253] Wape dis [0.5832713103206184, 0.7922942405908907, 0.9393961766953003] PE dis [0.2214537240311219, 0.46777088483462975, 0.7711760277150039]\nAgg SMAPE:  0.3115867 Agg WAPE:  0.2863996 Agg PE:  0.28375876\ntotal time  -186.6455910205841\n===============================================\ncluster id:  22\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.2751123607158661 wape is 0.24988317489624023 and pe is 0.179158017039299 and total is 0.23471786578496298\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\nthe  smape is 0.27317312359809875 wape is 0.247367262840271 and pe is 0.17231020331382751 and total is 0.23095019658406576\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  4\nthe  smape is 0.2262769490480423 wape is 0.2097446173429489 and pe is 0.09946498274803162 and total is 0.17849552631378174\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  5\nthe  smape is 0.21406175196170807 wape is 0.19981369376182556 and pe is 0.05109730362892151 and total is 0.1549909214178721\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  65\nfor sanity check 0.21406175 0.1998137 0.051097304 the mean is:  0.1549909214178721\nval wape is:  0.19981369376182556\ntotal time  -188.15374207496643\ntest Wsmape:  1.158637686448772  test Wape:  0.8175276507191105  test Wpe:  0.8117135414406031\nSmape dis [1.1354941948366437, 1.2539219692318089, 1.311524038642437] Wape dis [0.8085983372330642, 0.8578297931506103, 0.8801816364678879] PE dis [0.7960018147727314, 0.8453226857154361, 0.876998317232563]\nAgg SMAPE:  1.1404376 Agg WAPE:  0.81314826 Agg PE:  0.8117135\ntotal time  -188.15374207496643\n===============================================\ncluster id:  23\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.33800575137138367 wape is 0.29939979314804077 and pe is 0.2960060238838196 and total is 0.31113717953364056\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\nthe  smape is 0.31534528732299805 wape is 0.2826547920703888 and pe is 0.2763213515281677 and total is 0.2914404670397441\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  3\nthe  smape is 0.28733858466148376 wape is 0.2610837519168854 and pe is 0.2504219710826874 and total is 0.2662814458211263\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  151\nfor sanity check 0.28733858 0.26108375 0.25042197 the mean is:  0.2662814458211263\nval wape is:  0.2610837519168854\ntotal time  -188.16310358047485\ntest Wsmape:  1.249820709395745  test Wape:  0.8126569270053955  test Wpe:  0.7986975007648563\nSmape dis [1.0597877056622158, 1.2255407440552077, 1.295648916213469] Wape dis [0.7661644313453942, 0.8408952561240036, 0.8756179860019832] PE dis [0.7580101959957156, 0.8323546767852816, 0.8655319235220157]\nAgg SMAPE:  1.1833847 Agg WAPE:  0.7910785 Agg PE:  0.7910785\ntotal time  -188.16310358047485\n===============================================\ncluster id:  24\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.540978729724884 wape is 0.4395085871219635 and pe is 0.41631731390953064 and total is 0.4656015634536743\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\nthe  smape is 0.5435117483139038 wape is 0.4196915030479431 and pe is 0.39331233501434326 and total is 0.45217188199361164\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  3\ntrial run:  4\ntrial run:  5\ntrial run:  6\nthe  smape is 0.2974683940410614 wape is 0.23724044859409332 and pe is 0.13784854114055634 and total is 0.22418580452601114\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\nEarly stopping!\nStart to test process. 20\nthe  smape is 0.241514191031456 wape is 0.20910266041755676 and pe is 0.033842794597148895 and total is 0.161486546198527\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  18\nEarly stopping!\nStart to test process. 22\nthe  smape is 0.23306597769260406 wape is 0.20368334650993347 and pe is 0.02000388130545616 and total is 0.15225107471148172\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  19\nEarly stopping!\nStart to test process. 22\ntrial run:  20\nEarly stopping!\nStart to test process. 26\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\nnumber of smape:  432\nfor sanity check 0.23306598 0.20368335 0.020003881 the mean is:  0.15225107471148172\nval wape is:  0.20368334650993347\ntotal time  -160.4605360031128\ntest Wsmape:  0.5582052920075492  test Wape:  0.41629248725546314  test Wpe:  0.3421595649098705\nSmape dis [0.5277394112961478, 0.6983241136950588, 0.820179040267953] Wape dis [0.4897687134516478, 0.6289023997510849, 0.7357994440258608] PE dis [0.22095162023398957, 0.4180921883978913, 0.5265853221282305]\nAgg SMAPE:  0.33553633 Agg WAPE:  0.29200906 Agg PE:  0.29200912\ntotal time  -160.4605360031128\n===============================================\ncluster id:  25\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.37642157077789307 wape is 0.33242425322532654 and pe is 0.3080237805843353 and total is 0.3389565547307332\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  54\nfor sanity check 0.37642157 0.33242425 0.30802378 the mean is:  0.3389565547307332\nval wape is:  0.33242425322532654\ntotal time  -188.5884256362915\ntest Wsmape:  1.37377118129461  test Wape:  0.8721214384635751  test Wpe:  0.8715946117728668\nSmape dis [1.2891260754251928, 1.4111715461819856, 1.4365993036038067] Wape dis [0.8597723410564776, 0.8953142779057772, 0.9036079401327841] PE dis [0.8565535876790003, 0.8953142779057772, 0.903607940132784]\nAgg SMAPE:  1.3753049 Agg WAPE:  0.8715908 Agg PE:  0.8715908\ntotal time  -188.5884256362915\n===============================================\ncluster id:  26\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.1058458462357521 wape is 0.1067519411444664 and pe is 0.05110224708914757 and total is 0.08790001273155212\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\nthe  smape is 0.09128831326961517 wape is 0.09138317406177521 and pe is 0.048307545483112335 and total is 0.0769930084546407\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  3\nthe  smape is 0.08396312594413757 wape is 0.08363142609596252 and pe is 0.018885869532823563 and total is 0.062160139282544456\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  4\ntrial run:  5\ntrial run:  6\nthe  smape is 0.09209413081407547 wape is 0.09139002859592438 and pe is 0.0004821943584829569 and total is 0.06132212281227112\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\nthe  smape is 0.09199392050504684 wape is 0.09121772646903992 and pe is 0.00043261758401058614 and total is 0.06121475497881571\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  18\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\nEarly stopping!\nStart to test process. 149\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  213\nfor sanity check 0.09199392 0.09121773 0.00043261758 the mean is:  0.06121475497881571\nval wape is:  0.09121772646903992\ntotal time  -186.54992961883545\ntest Wsmape:  0.7377010985522745  test Wape:  0.7977610185603352  test Wpe:  0.43099862432080865\nSmape dis [0.7541981809040518, 0.9549572753928957, 1.061573135658119] Wape dis [0.7353483077603917, 0.9802768370300218, 1.3565759790588159] PE dis [0.2825044113085666, 0.6283003077913711, 1.0069324247011924]\nAgg SMAPE:  0.26930195 Agg WAPE:  0.29657945 Agg PE:  0.2799835\ntotal time  -186.54992961883545\n===============================================\ncluster id:  27\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.31990528106689453 wape is 0.289295494556427 and pe is 0.2370755523443222 and total is 0.2820921142896016\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\nthe  smape is 0.2775956392288208 wape is 0.2560470998287201 and pe is 0.19270092248916626 and total is 0.24211456378300986\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  3\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\nthe  smape is 0.2602701783180237 wape is 0.24089309573173523 and pe is 0.15999680757522583 and total is 0.22038668394088745\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  51\nfor sanity check 0.26027018 0.2408931 0.15999681 the mean is:  0.22038668394088745\nval wape is:  0.24089309573173523\ntotal time  -191.9509789943695\ntest Wsmape:  1.2546385636206083  test Wape:  0.8442505763541833  test Wpe:  0.8429937558409638\nSmape dis [1.2269213562190922, 1.3323894840717054, 1.3663576932387265] Wape dis [0.8389193552938221, 0.8792192357525416, 0.8917614714784504] PE dis [0.831677863924085, 0.8787654005385116, 0.8898441449710134]\nAgg SMAPE:  1.2465256 Agg WAPE:  0.84299374 Agg PE:  0.84299374\ntotal time  -191.9509789943695\n===============================================\ncluster id:  28\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.6821539402008057 wape is 1.014443278312683 and pe is 1.0087991952896118 and total is 0.9017988046010336\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\nthe  smape is 0.6439673900604248 wape is 0.9231642484664917 and pe is 0.922967791557312 and total is 0.8300331433614095\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  3\nthe  smape is 0.6043251752853394 wape is 0.8527134656906128 and pe is 0.8424862623214722 and total is 0.7665082613627116\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  4\ntrial run:  5\nthe  smape is 0.4896295964717865 wape is 0.6021614074707031 and pe is 0.5992209911346436 and total is 0.5636706749598185\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  6\ntrial run:  7\nthe  smape is 0.45208507776260376 wape is 0.547100305557251 and pe is 0.4266907274723053 and total is 0.47529204686482746\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\nEarly stopping!\nStart to test process. 26\nthe  smape is 0.28896576166152954 wape is 0.2998872399330139 and pe is 0.1474754810333252 and total is 0.2454428275426229\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  18\nEarly stopping!\nStart to test process. 21\ntrial run:  19\nEarly stopping!\nStart to test process. 26\ntrial run:  20\nEarly stopping!\nStart to test process. 22\ntrial run:  21\nEarly stopping!\nStart to test process. 22\nthe  smape is 0.28854021430015564 wape is 0.2980850338935852 and pe is 0.10366159677505493 and total is 0.23009560505549112\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  22\nEarly stopping!\nStart to test process. 19\nthe  smape is 0.27071455121040344 wape is 0.2738061845302582 and pe is 0.10319210588932037 and total is 0.2159042755762736\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  23\nEarly stopping!\nStart to test process. 21\ntrial run:  24\nEarly stopping!\nStart to test process. 19\nthus best config is {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\nnumber of smape:  234\nfor sanity check 0.27071455 0.27380618 0.103192106 the mean is:  0.2159042755762736\nval wape is:  0.2738061845302582\ntotal time  -135.59119939804077\ntest Wsmape:  0.6631044700431815  test Wape:  0.90501322326964  test Wpe:  0.7556075036792333\nSmape dis [0.9185028114139084, 1.1203818712777307, 1.2350409214253406] Wape dis [1.2429995782282797, 1.8041162310864811, 2.223664981920967] PE dis [1.0157245840142437, 1.5936424976004444, 2.0191348070132142]\nAgg SMAPE:  0.57135475 Agg WAPE:  0.7692821 Agg PE:  0.709856\ntotal time  -135.59119939804077\n===============================================\ncluster id:  29\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.22704105079174042 wape is 0.21015658974647522 and pe is 0.1293996423482895 and total is 0.18886576096216837\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\nthe  smape is 0.2205478549003601 wape is 0.2045731544494629 and pe is 0.12829753756523132 and total is 0.1844728390375773\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  4\nthe  smape is 0.20782925188541412 wape is 0.1917569786310196 and pe is 0.11110929399728775 and total is 0.17023183902104697\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\nnumber of smape:  100\nfor sanity check 0.20782925 0.19175698 0.111109294 the mean is:  0.17023183902104697\nval wape is:  0.1917569786310196\ntotal time  -187.15501308441162\ntest Wsmape:  1.1672777516724093  test Wape:  0.8119398789132249  test Wpe:  0.8069283955739864\nSmape dis [1.1114671202245208, 1.288439695343414, 1.3335055638573983] Wape dis [0.7953656817911732, 0.864303686656243, 0.8786931888142104] PE dis [0.773361710512467, 0.8560508082440885, 0.8745971052624147]\nAgg SMAPE:  1.1440444 Agg WAPE:  0.806579 Agg PE:  0.806579\ntotal time  -187.15501308441162\n===============================================\ncluster id:  30\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.21610893309116364 wape is 0.23325029015541077 and pe is 0.21694011986255646 and total is 0.22209978103637695\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\nthe  smape is 0.18598401546478271 wape is 0.19495123624801636 and pe is 0.14575758576393127 and total is 0.17556428909301758\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  3\nthe  smape is 0.12230026721954346 wape is 0.12372817099094391 and pe is 0.05388098582625389 and total is 0.09996980428695679\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  4\ntrial run:  5\nthe  smape is 0.12988001108169556 wape is 0.12625232338905334 and pe is 0.024379676207900047 and total is 0.0935040016969045\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\nthe  smape is 0.10152067989110947 wape is 0.10255500674247742 and pe is 0.00045661855256184936 and total is 0.06817743678887685\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  10\ntrial run:  11\nEarly stopping!\nStart to test process. 50\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\nEarly stopping!\nStart to test process. 85\ntrial run:  16\ntrial run:  17\nEarly stopping!\nStart to test process. 155\ntrial run:  18\ntrial run:  19\nEarly stopping!\nStart to test process. 136\ntrial run:  20\ntrial run:  21\ntrial run:  22\nEarly stopping!\nStart to test process. 41\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  68\nfor sanity check 0.10152068 0.10255501 0.00045661855 the mean is:  0.06817743678887685\nval wape is:  0.10255500674247742\ntotal time  -168.1005916595459\ntest Wsmape:  0.7868149070225877  test Wape:  0.6645940470366817  test Wpe:  0.5733764986505909\nSmape dis [0.6962885002244106, 0.8926745119294498, 1.0819635974244661] Wape dis [0.6193741302473688, 0.8994254081602466, 1.2349675799137025] PE dis [0.42214953922398524, 0.7516356884688689, 0.9249288200470739]\nAgg SMAPE:  0.38509274 Agg WAPE:  0.3443419 Agg PE:  0.3293821\ntotal time  -168.1005916595459\n===============================================\ncluster id:  31\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.14929437637329102 wape is 0.15700620412826538 and pe is 0.05747391656041145 and total is 0.12125816941261292\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\nthe  smape is 0.13754209876060486 wape is 0.14430665969848633 and pe is 0.030052507296204567 and total is 0.10396709044774373\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  221\nfor sanity check 0.1375421 0.14430666 0.030052507 the mean is:  0.10396709044774373\nval wape is:  0.14430665969848633\ntotal time  -189.1288924217224\ntest Wsmape:  0.9651531191209015  test Wape:  0.6824614202441307  test Wpe:  0.616836077338456\nSmape dis [0.5875282175944833, 0.8275311070612865, 1.0166198942442075] Wape dis [0.5810247464704242, 0.8275168466145346, 0.9521383869821575] PE dis [0.28273548369503415, 0.6606479977712371, 0.8736205114696752]\nAgg SMAPE:  0.50263625 Agg WAPE:  0.43840066 Agg PE:  0.43840066\ntotal time  -189.1288924217224\n===============================================\ncluster id:  32\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.07277112454175949 wape is 0.07279004901647568 and pe is 0.019930552691221237 and total is 0.05516390999158224\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\nthe  smape is 0.07251816242933273 wape is 0.0725729763507843 and pe is 0.019972195848822594 and total is 0.055021112163861595\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  9\ntrial run:  10\ntrial run:  11\nthe  smape is 0.07125747203826904 wape is 0.07130539417266846 and pe is 0.015745364129543304 and total is 0.05276941259702047\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  545\nfor sanity check 0.07125747 0.071305394 0.015745364 the mean is:  0.05276941259702047\nval wape is:  0.07130539417266846\ntotal time  -188.83188724517822\ntest Wsmape:  0.5531049673434201  test Wape:  0.5423207232860755  test Wpe:  0.41185991263898303\nSmape dis [0.6862289106108098, 0.8769342826648803, 1.0155813229156303] Wape dis [0.6696133322007785, 0.9075203001161056, 1.1477412888687364] PE dis [0.30923720587209813, 0.6367201989551909, 0.8610081109583392]\nAgg SMAPE:  0.0629457 Agg WAPE:  0.062052134 Agg PE:  0.02445737\ntotal time  -188.83188724517822\n===============================================\ncluster id:  33\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.2694357931613922 wape is 0.2438785433769226 and pe is 0.18825379014015198 and total is 0.2338560422261556\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\nthe  smape is 0.20409700274467468 wape is 0.19187362492084503 and pe is 0.09438257664442062 and total is 0.16345107555389404\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\nthe  smape is 0.19381394982337952 wape is 0.18283018469810486 and pe is 0.0950329378247261 and total is 0.1572256882985433\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  26\nfor sanity check 0.19381395 0.18283018 0.09503294 the mean is:  0.1572256882985433\nval wape is:  0.18283018469810486\ntotal time  -188.41914343833923\ntest Wsmape:  1.0796775809936423  test Wape:  0.7724134304473158  test Wpe:  0.7709700117591054\nSmape dis [0.981195980588972, 1.1572349484325692, 1.2212116155830965] Wape dis [0.7330839147247714, 0.8109220883838107, 0.8311392892386436] PE dis [0.7207754821901828, 0.7964872298930704, 0.8225603261697287]\nAgg SMAPE:  1.0728611 Agg WAPE:  0.77097 Agg PE:  0.77097005\ntotal time  -188.41914343833923\n===============================================\ncluster id:  34\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.12273251265287399 wape is 0.12139937281608582 and pe is 0.04816785454750061 and total is 0.09743324915568034\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\ntrial run:  4\ntrial run:  5\nthe  smape is 0.12120620161294937 wape is 0.12322437763214111 and pe is 0.016251804307103157 and total is 0.08689412474632263\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  6\ntrial run:  7\nthe  smape is 0.11886091530323029 wape is 0.12020087242126465 and pe is 0.00029170914785936475 and total is 0.07978449761867523\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\nthe  smape is 0.10993502289056778 wape is 0.11064110696315765 and pe is 0.013894297182559967 and total is 0.07815680901209514\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  12\nthe  smape is 0.11068758368492126 wape is 0.11172458529472351 and pe is 0.004528483841568232 and total is 0.07564688225587209\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\nthe  smape is 0.10972791910171509 wape is 0.11086671054363251 and pe is 0.003032415872439742 and total is 0.07454234858353932\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\nnumber of smape:  327\nfor sanity check 0.10972792 0.11086671 0.0030324159 the mean is:  0.07454234858353932\nval wape is:  0.11086671054363251\ntotal time  -188.82486200332642\ntest Wsmape:  0.7186752821341749  test Wape:  0.743349256318125  test Wpe:  0.39828581800395374\nSmape dis [0.7844536501894523, 0.9716115194507725, 1.073234155131288] Wape dis [0.7279557189837264, 1.0139737339383321, 1.1954503984693008] PE dis [0.30228834946398425, 0.6809222180161768, 0.9661293497553329]\nAgg SMAPE:  0.16967304 Agg WAPE:  0.17710644 Agg PE:  0.16508377\ntotal time  -188.82486200332642\n===============================================\ncluster id:  35\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.34033578634262085 wape is 0.3117700219154358 and pe is 0.16517692804336548 and total is 0.2724275787671407\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\ntrial run:  4\ntrial run:  5\nthe  smape is 0.32759013772010803 wape is 0.3037858009338379 and pe is 0.11577901989221573 and total is 0.2490516503651937\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  66\nfor sanity check 0.32759014 0.3037858 0.11577902 the mean is:  0.2490516503651937\nval wape is:  0.3037858009338379\ntotal time  -187.69266819953918\ntest Wsmape:  1.2108943933132577  test Wape:  0.838274206330701  test Wpe:  0.8254129436782095\nSmape dis [0.8681696827947185, 1.0308316891302511, 1.1793651235585747] Wape dis [0.6930641111182015, 0.7551413888166204, 0.8166822774342739] PE dis [0.5960265437572405, 0.7238417890269206, 0.7826320639443636]\nAgg SMAPE:  1.1615742 Agg WAPE:  0.8277543 Agg PE:  0.82384235\ntotal time  -187.69266819953918\n===============================================\ncluster id:  36\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.06030457094311714 wape is 0.05965372920036316 and pe is 0.029605763033032417 and total is 0.04985468586285909\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\nthe  smape is 0.058348849415779114 wape is 0.057618431746959686 and pe is 0.005071297287940979 and total is 0.04034619281689326\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  3\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\nthe  smape is 0.05788389593362808 wape is 0.05709986016154289 and pe is 0.004533736500889063 and total is 0.039839163422584534\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\nnumber of smape:  273\nfor sanity check 0.057883896 0.05709986 0.0045337365 the mean is:  0.039839163422584534\nval wape is:  0.05709986016154289\ntotal time  -189.24378561973572\ntest Wsmape:  0.6790066940738786  test Wape:  0.6675277546186354  test Wpe:  0.3881161067485341\nSmape dis [0.6933225200672036, 0.8663841430572876, 0.9577488604268701] Wape dis [0.6430927212776439, 0.8569912886686817, 1.0371221873554661] PE dis [0.2814313585294092, 0.5868788931564857, 0.8326899280878575]\nAgg SMAPE:  0.08780744 Agg WAPE:  0.09381598 Agg PE:  0.019783624\ntotal time  -189.24378561973572\n===============================================\ncluster id:  37\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.6873278617858887 wape is 0.5813894271850586 and pe is 0.03882390633225441 and total is 0.43584708372751874\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\nthe  smape is 0.6368739008903503 wape is 0.5411644577980042 and pe is 0.09685210883617401 and total is 0.42496347427368164\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\nthe  smape is 0.6051562428474426 wape is 0.5252687931060791 and pe is 0.06635349243879318 and total is 0.39892613887786865\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  43\nfor sanity check 0.60515624 0.5252688 0.06635349 the mean is:  0.39892613887786865\nval wape is:  0.5252687931060791\ntotal time  -189.0604190826416\ntest Wsmape:  0.6746757947421075  test Wape:  0.46108494342968953  test Wpe:  0.450370511240148\nSmape dis [0.7847144182938397, 1.065971665695065, 1.133239668691434] Wape dis [0.6848457564762244, 0.9062596275532231, 0.9661723746083439] PE dis [0.3891127876988436, 0.5163597754810112, 0.759200897025226]\nAgg SMAPE:  0.66274774 Agg WAPE:  0.44848564 Agg PE:  0.44681606\ntotal time  -189.0604190826416\n===============================================\ncluster id:  38\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.17894378304481506 wape is 0.17017224431037903 and pe is 0.10401926934719086 and total is 0.15104510386784872\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\nthe  smape is 0.17500931024551392 wape is 0.16696560382843018 and pe is 0.09216578304767609 and total is 0.14471356074015299\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  3\ntrial run:  4\nthe  smape is 0.16785921156406403 wape is 0.16017746925354004 and pe is 0.05095008760690689 and total is 0.12632891535758972\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  5\ntrial run:  6\ntrial run:  7\nthe  smape is 0.14876320958137512 wape is 0.143896222114563 and pe is 0.0021494461689144373 and total is 0.09826962153116862\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  8\nthe  smape is 0.13511629402637482 wape is 0.13009627163410187 and pe is 0.00829283893108368 and total is 0.09116846323013306\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\nnumber of smape:  37\nfor sanity check 0.1351163 0.13009627 0.008292839 the mean is:  0.09116846323013306\nval wape is:  0.13009627163410187\ntotal time  -187.90461087226868\ntest Wsmape:  1.1381031176777603  test Wape:  0.7920694474611011  test Wpe:  0.7909104960001491\nSmape dis [1.0045381277783634, 1.2359188420346034, 1.3073344402740112] Wape dis [0.7362370118717498, 0.8293179246773597, 0.8627937233261178] PE dis [0.7113093850987533, 0.825860907474428, 0.8627937233261178]\nAgg SMAPE:  1.1299752 Agg WAPE:  0.7909105 Agg PE:  0.7909105\ntotal time  -187.90461087226868\n===============================================\ncluster id:  39\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.782989501953125 wape is 1.2066259384155273 and pe is 0.9417693018913269 and total is 0.977128267288208\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\nthe  smape is 0.7583990097045898 wape is 1.1381893157958984 and pe is 0.9143239259719849 and total is 0.9369707107543945\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  3\nthe  smape is 0.6854677200317383 wape is 0.7281373739242554 and pe is 0.1639127880334854 and total is 0.5258392890294393\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  4\nthe  smape is 0.4867445230484009 wape is 0.6574113965034485 and pe is 0.2935373783111572 and total is 0.4792311191558838\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\nthe  smape is 0.42043641209602356 wape is 0.4985208511352539 and pe is 0.01129074115306139 and total is 0.31008265415827435\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\nnumber of smape:  63\nfor sanity check 0.4204364 0.49852085 0.011290741 the mean is:  0.31008265415827435\nval wape is:  0.4985208511352539\ntotal time  -186.73775386810303\ntest Wsmape:  0.6124775372910485  test Wape:  0.70905408793073  test Wpe:  0.21311886815067124\nSmape dis [0.6820313767357861, 0.8469657980590652, 1.0119316778126342] Wape dis [0.5922124485244027, 0.8127293208942904, 1.0833798940886061] PE dis [0.29663045073593597, 0.5553287842284637, 0.6006934109381604]\nAgg SMAPE:  0.40319565 Agg WAPE:  0.5197639 Agg PE:  0.013681516\ntotal time  -186.73775386810303\n===============================================\ncluster id:  40\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.11323598027229309 wape is 0.11393509805202484 and pe is 0.01053699478507042 and total is 0.07923602561155955\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\nthe  smape is 0.11277661472558975 wape is 0.11290755867958069 and pe is 0.007289006374776363 and total is 0.0776577244202296\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  9\ntrial run:  10\nthe  smape is 0.10494466871023178 wape is 0.10543099790811539 and pe is 0.007977321743965149 and total is 0.07278432945410411\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  11\ntrial run:  12\nthe  smape is 0.10415486246347427 wape is 0.10457530617713928 and pe is 0.005370127037167549 and total is 0.07136676212151845\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  13\ntrial run:  14\ntrial run:  15\nthe  smape is 0.10379906743764877 wape is 0.1040596216917038 and pe is 0.005420316942036152 and total is 0.07109299798806508\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  264\nfor sanity check 0.10379907 0.10405962 0.005420317 the mean is:  0.07109299798806508\nval wape is:  0.1040596216917038\ntotal time  -187.3940670490265\ntest Wsmape:  0.6373279253414088  test Wape:  0.6113651957874733  test Wpe:  0.33055473604722635\nSmape dis [0.7924206888307058, 0.9580880191297139, 1.050874156587916] Wape dis [0.7282585437444336, 0.9296209049719009, 1.1274280539760666] PE dis [0.27845504903781065, 0.5575502775810776, 0.7768214801962008]\nAgg SMAPE:  0.10216217 Agg WAPE:  0.10075865 Agg PE:  0.062245592\ntotal time  -187.3940670490265\n===============================================\ncluster id:  41\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.11559340357780457 wape is 0.1196870356798172 and pe is 0.08127214014530182 and total is 0.10551752646764119\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\nthe  smape is 0.0844009593129158 wape is 0.08310647308826447 and pe is 0.037295106798410416 and total is 0.06826751430829366\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  3\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\nthe  smape is 0.09184585511684418 wape is 0.09026238322257996 and pe is 0.017110934481024742 and total is 0.06640638907750447\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  11\ntrial run:  12\nthe  smape is 0.0880732536315918 wape is 0.08718801289796829 and pe is 0.0003695850900840014 and total is 0.058543612559636436\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nEarly stopping!\nStart to test process. 118\nthus best config is {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\nnumber of smape:  508\nfor sanity check 0.08807325 0.08718801 0.0003695851 the mean is:  0.058543612559636436\nval wape is:  0.08718801289796829\ntotal time  -184.01538014411926\ntest Wsmape:  0.4529338462722793  test Wape:  0.3839991981115493  test Wpe:  0.2892762773899094\nSmape dis [0.5593588142036068, 0.7488813749440566, 0.857665895756587] Wape dis [0.5200803266281717, 0.6704405294191191, 0.7907365434246774] PE dis [0.18865339677009277, 0.40274232611540983, 0.5298269440667533]\nAgg SMAPE:  0.2189459 Agg WAPE:  0.20104514 Agg PE:  0.19688718\ntotal time  -184.01538014411926\n===============================================\ncluster id:  42\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.3311547040939331 wape is 0.3337962031364441 and pe is 0.2631465792655945 and total is 0.30936582883199054\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\nthe  smape is 0.33664876222610474 wape is 0.3392459750175476 and pe is 0.23912076652050018 and total is 0.3050051728884379\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  3\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\nnumber of smape:  70\nfor sanity check 0.33664876 0.33924598 0.23912077 the mean is:  0.3050051728884379\nval wape is:  0.3392459750175476\ntotal time  -187.88607931137085\ntest Wsmape:  1.3720665632467441  test Wape:  0.8425538297019812  test Wpe:  0.8397178697062297\nSmape dis [1.0354065582306418, 1.2950994096746085, 1.3538884739073076] Wape dis [0.7294182541808956, 0.8364340568885698, 0.8660571032844123] PE dis [0.6987433416220364, 0.8095436346113694, 0.8518128760488959]\nAgg SMAPE:  1.3576949 Agg WAPE:  0.8395829 Agg PE:  0.8395828\ntotal time  -187.88607931137085\n===============================================\ncluster id:  43\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.26868417859077454 wape is 0.24371042847633362 and pe is 0.22728265821933746 and total is 0.24655908346176147\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\nthe  smape is 0.2485462874174118 wape is 0.22799542546272278 and pe is 0.2054944634437561 and total is 0.22734538714090982\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  3\nthe  smape is 0.21540993452072144 wape is 0.20029817521572113 and pe is 0.1616232991218567 and total is 0.19244380791982016\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  187\nfor sanity check 0.21540993 0.20029818 0.1616233 the mean is:  0.19244380791982016\nval wape is:  0.20029817521572113\ntotal time  -185.82454347610474\ntest Wsmape:  0.9968440174105152  test Wape:  0.7265018040536068  test Wpe:  0.7125525541449343\nSmape dis [0.8015866306280423, 1.032276990064368, 1.1410008176325885] Wape dis [0.6519950379708948, 0.76423862870519, 0.8147287459125478] PE dis [0.5990245126114722, 0.7393317044196153, 0.7871364271613105]\nAgg SMAPE:  0.94983983 Agg WAPE:  0.7068073 Agg PE:  0.7068073\ntotal time  -185.82454347610474\n===============================================\ncluster id:  44\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.2881031036376953 wape is 0.3270932137966156 and pe is 0.3016347289085388 and total is 0.3056103587150574\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\nthe  smape is 0.254986971616745 wape is 0.286178857088089 and pe is 0.21350477635860443 and total is 0.2515568733215332\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  3\nthe  smape is 0.1920194923877716 wape is 0.19866935908794403 and pe is 0.07022909820079803 and total is 0.1536393165588379\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  277\nfor sanity check 0.19201949 0.19866936 0.0702291 the mean is:  0.1536393165588379\nval wape is:  0.19866935908794403\ntotal time  -187.26987552642822\ntest Wsmape:  0.4385335817820704  test Wape:  0.48775125789104307  test Wpe:  0.35130351573492663\nSmape dis [0.6546358555796439, 0.8613218530795514, 0.9723819186135523] Wape dis [0.6763622168526615, 0.9163013882836601, 1.2074613943536752] PE dis [0.3300208597250598, 0.6485331639740153, 0.9698793278844853]\nAgg SMAPE:  0.29899696 Agg WAPE:  0.33554754 Agg PE:  0.24995025\ntotal time  -187.26987552642822\n===============================================\ncluster id:  45\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.35160884261131287 wape is 0.436116099357605 and pe is 0.4193589985370636 and total is 0.40236131350199383\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\nthe  smape is 0.18224431574344635 wape is 0.19883741438388824 and pe is 0.17638258635997772 and total is 0.18582143386205038\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  3\nthe  smape is 0.1550329476594925 wape is 0.16342124342918396 and pe is 0.15245474874973297 and total is 0.15696964661280313\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\nthe  smape is 0.13932591676712036 wape is 0.14753608405590057 and pe is 0.1132943257689476 and total is 0.13338544964790344\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\nEarly stopping!\nStart to test process. 163\ntrial run:  16\ntrial run:  17\ntrial run:  18\nEarly stopping!\nStart to test process. 66\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\nthe  smape is 0.1308550238609314 wape is 0.13590840995311737 and pe is 0.11333340406417847 and total is 0.1266989509264628\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\nnumber of smape:  94\nfor sanity check 0.13085502 0.13590841 0.113333404 the mean is:  0.1266989509264628\nval wape is:  0.13590840995311737\ntotal time  -179.27339816093445\ntest Wsmape:  0.72836758519549  test Wape:  0.701578669628021  test Wpe:  0.45771285587375216\nSmape dis [0.7495176594286284, 0.9813991714173502, 1.1011031599787269] Wape dis [0.6625432403807633, 1.0415033452575173, 1.3606508304300264] PE dis [0.3535791755096092, 0.7583515516398679, 1.0698216349711893]\nAgg SMAPE:  0.17806068 Agg WAPE:  0.17462328 Agg PE:  0.065663815\ntotal time  -179.27339816093445\n===============================================\ncluster id:  46\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.17507724463939667 wape is 0.16264893114566803 and pe is 0.1479007452726364 and total is 0.16187564531962076\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\nthe  smape is 0.07773833721876144 wape is 0.07654822617769241 and pe is 0.028017926961183548 and total is 0.06076816221078237\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  3\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\nthe  smape is 0.06896089762449265 wape is 0.06829722225666046 and pe is 0.008165175095200539 and total is 0.04847443103790283\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\nEarly stopping!\nStart to test process. 163\ntrial run:  24\nEarly stopping!\nStart to test process. 76\nthus best config is {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\nnumber of smape:  890\nfor sanity check 0.0689609 0.06829722 0.008165175 the mean is:  0.04847443103790283\nval wape is:  0.06829722225666046\ntotal time  -180.34643077850342\ntest Wsmape:  0.3304534266026132  test Wape:  0.31905342965370376  test Wpe:  0.17980293069790854\nSmape dis [0.5214693274462701, 0.7051296806916151, 0.8150435151738884] Wape dis [0.49408113418059724, 0.654555786991688, 0.7553093316729067] PE dis [0.17361214061008376, 0.3806252480145982, 0.49660768863187466]\nAgg SMAPE:  0.08933321 Agg WAPE:  0.08691503 Agg PE:  0.042339843\ntotal time  -180.34643077850342\n===============================================\ncluster id:  47\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.14475436508655548 wape is 0.15068955719470978 and pe is 0.13475367426872253 and total is 0.14339919884999594\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\nthe  smape is 0.14857971668243408 wape is 0.15355223417282104 and pe is 0.12172947078943253 and total is 0.14128713806470236\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\nthe  smape is 0.13508206605911255 wape is 0.13798874616622925 and pe is 0.10777631402015686 and total is 0.12694904208183289\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\nnumber of smape:  312\nfor sanity check 0.13508207 0.13798875 0.107776314 the mean is:  0.12694904208183289\nval wape is:  0.13798874616622925\ntotal time  -186.7046022415161\ntest Wsmape:  0.8758016375297817  test Wape:  0.6304181668476856  test Wpe:  0.5918877769021875\nSmape dis [0.6575955164295922, 0.8789599032232565, 0.9754821589403754] Wape dis [0.5738008989644721, 0.7012753297992218, 0.7739168583762001] PE dis [0.42880767210317594, 0.6292646363753831, 0.7160128911783297]\nAgg SMAPE:  0.74092144 Agg WAPE:  0.5711519 Agg PE:  0.571152\ntotal time  -186.7046022415161\n===============================================\ncluster id:  48\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.16306722164154053 wape is 0.15349440276622772 and pe is 0.12480871379375458 and total is 0.14712345600128174\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\nthe  smape is 0.15267342329025269 wape is 0.14781582355499268 and pe is 0.11469683051109314 and total is 0.1383953591187795\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  3\ntrial run:  4\ntrial run:  5\nthe  smape is 0.14661750197410583 wape is 0.1410122513771057 and pe is 0.11940301209688187 and total is 0.13567758599917093\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  6\ntrial run:  7\nthe  smape is 0.14322006702423096 wape is 0.13784846663475037 and pe is 0.11779021471738815 and total is 0.13295291860898337\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\nthe  smape is 0.1463727355003357 wape is 0.14164307713508606 and pe is 0.09283612668514252 and total is 0.12695064147313437\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\nnumber of smape:  188\nfor sanity check 0.14637274 0.14164308 0.09283613 the mean is:  0.12695064147313437\nval wape is:  0.14164307713508606\ntotal time  -186.47635412216187\ntest Wsmape:  0.8805465468857253  test Wape:  0.6401337443951617  test Wpe:  0.6274576870005036\nSmape dis [0.7943955032966543, 1.0294671824063044, 1.127811563432972] Wape dis [0.6291540664293587, 0.7478248348104121, 0.8028915417584404] PE dis [0.5614959228767961, 0.7131595806194846, 0.7690891768834591]\nAgg SMAPE:  0.8524585 Agg WAPE:  0.62494296 Agg PE:  0.6249429\ntotal time  -186.47635412216187\n===============================================\ncluster id:  49\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.302394300699234 wape is 0.32231059670448303 and pe is 0.3000308573246002 and total is 0.3082452615102132\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\nthe  smape is 0.30990365147590637 wape is 0.31217288970947266 and pe is 0.12933321297168732 and total is 0.2504699031511943\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  9\nEarly stopping!\nStart to test process. 161\ntrial run:  10\nEarly stopping!\nStart to test process. 103\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\nthe  smape is 0.25674736499786377 wape is 0.2282080203294754 and pe is 0.018715471029281616 and total is 0.16789027055104574\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  16\ntrial run:  17\nEarly stopping!\nStart to test process. 144\ntrial run:  18\nEarly stopping!\nStart to test process. 128\ntrial run:  19\ntrial run:  20\ntrial run:  21\nEarly stopping!\nStart to test process. 165\ntrial run:  22\ntrial run:  23\nEarly stopping!\nStart to test process. 122\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  174\nfor sanity check 0.25674736 0.22820802 0.018715471 the mean is:  0.16789027055104574\nval wape is:  0.2282080203294754\ntotal time  -173.9629349708557\ntest Wsmape:  0.6819107023073514  test Wape:  0.5312595806526221  test Wpe:  0.4664330781579982\nSmape dis [0.6256993957670753, 0.8486812939049553, 0.9881642166488682] Wape dis [0.5629430763636241, 0.72394770919621, 0.8334027307561109] PE dis [0.2634607654959389, 0.44796959279574494, 0.653116335457321]\nAgg SMAPE:  0.48359603 Agg WAPE:  0.3820075 Agg PE:  0.3820075\ntotal time  -173.9629349708557\n===============================================\ncluster id:  50\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.20704112946987152 wape is 0.22486841678619385 and pe is 0.15896542370319366 and total is 0.19695832331975302\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\nthe  smape is 0.1988109052181244 wape is 0.21272709965705872 and pe is 0.12353277951478958 and total is 0.17835692564646402\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  460\nfor sanity check 0.1988109 0.2127271 0.12353278 the mean is:  0.17835692564646402\nval wape is:  0.21272709965705872\ntotal time  -186.11059951782227\ntest Wsmape:  0.7946418323424775  test Wape:  0.9649103068074721  test Wpe:  0.6110163981104872\nSmape dis [0.8872844345977977, 1.0995171344814711, 1.2387415917034614] Wape dis [0.8933858787308875, 1.5067332406187106, 1.8991566228685854] PE dis [0.47285108005023635, 1.1724441149556784, 1.6435720386821877]\nAgg SMAPE:  0.4784595 Agg WAPE:  0.5917092 Agg PE:  0.5127261\ntotal time  -186.11059951782227\n===============================================\n"}], "source": "month=0\nprint('Running for the month of :',name[month])\n################LSTM##########################\n################LSTM##########################\nfor i in range(50):\n    print('cluster id: ',i+1)\n    current_date = '20220831'\n    test_group_id = test_group_id_list[i]#'/3944/1060825/447913'\n    #for top cluster:\n    path = f\"gs://adtech-artifacts/ds/forecast/month_eval/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_tft_data_{current_date}.csv\"\n    #path = f\"gs://adtech-artifacts/ds/forecast/month_eval/tail_clusters/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_data_20220831.csv\"\n    # read in data\n    df_panda = spark.read.option(\"header\", True).csv(path).toPandas() # The usage should be the same as previous\n    df_panda=df_panda[['ds', 'group_id', 'query_idx', 'SearchKeyValue',  'LocationKeyValue', 'daily_supply']]\n    #df_panda=spark.read.csv(\"gs://adtech-artifacts/ds/forecast/query_taxonomy_sample_Addedfeatures_cluster_\"+str(i)+\".csv\",header=True).toPandas()\n    Cols=['daily_supply']#, 'shifted_annual_daily_supply','Trend_Var','dayofyear_sin', 'dayofyear_cos', 'month_sin', 'month_cos']\n    for cols in Cols:  \n        df_panda[cols]=df_panda[cols].astype(float,errors='raise')\n    start=time.time()\n    minError=1e10\n    param = {\n        \"learning_rate\": [.1,.01,.001],\n        \"hidden_size\": [10,20],\n        'batch_size': [32],\n        'dropout': [0.05,.1],\n        'gamma': [0.9],\n        'optimizer_name': ['AdamW'],\n        'num_layer': [1,2],\n        'relu': [False]\n    }\n    prep = dataprep(df_panda, input_window=90, output_window=30, stride=1,start=start_month[month])\n    print('Beginning the training...')\n    train_set_in, train_set_out, val_set_in, val_set_out, test_in_set, test_out_set,df_test_org,  q,wq = prep._get_data()\n    #print('print len of q, value and wq value',len(q),q[0],wq[0])\n    train_set_in = torch.from_numpy(train_set_in.astype(np.float64)).float()\n    train_set_out = torch.from_numpy(train_set_out.astype(np.float64)).float()\n    val_set_in = torch.from_numpy(val_set_in.astype(np.float64)).float()\n    val_set_out = torch.from_numpy(val_set_out.astype(np.float64)).float()\n    test_in_set = torch.from_numpy(test_in_set.astype(np.float64)).float()\n    test_out_set = torch.from_numpy(test_out_set.astype(np.float64)).float()\n    print('val shape is: ',val_set_in.shape, val_set_out.shape)\n    end1=time.time()\n    if len(train_set_in.shape) < 3:\n        train_set_in = train_set_in.reshape(train_set_in.shape[0], train_set_in.shape[1], 1)\n        val_set_in = val_set_in.reshape(val_set_in.shape[0], val_set_in.shape[1], 1)\n        test_in_set = test_in_set.reshape(test_in_set.shape[0], test_in_set.shape[1], 1)\n        test_out_set=test_out_set.reshape(test_out_set.shape[0], test_out_set.shape[1])\n    counter=1\n    for lr in param['learning_rate']:\n        for h_s in param['hidden_size']:\n            for b_s in param['batch_size']:\n                for dp in param['dropout']:\n                    for opt in param['optimizer_name']:\n                        for n_l in param['num_layer']:\n                            for act in param['relu']:\n                                for gm in param['gamma']:\n                                    config = {\n                                        \"num_epochs\": 200,\n                                        \"learning_rate\": lr,\n                                        \"hidden_size\": h_s,\n                                        \"window_size\": 30,\n                                        \"output_size\": 30,\n                                        'batch_size': b_s,\n                                        'dropout': dp,\n                                        'dropout1': 0,\n                                        'gamma': gm,\n                                        'optimizer_name': opt,\n                                        'num_layer': n_l,\n                                        'relu': act\n                                    }\n                                    print('trial run: ',counter)\n                                    counter+=1\n                                    e1,e2,e3,model = running_lstm(config, save_model=True)\n                                    if (e1+e2+e3)/3<minError:\n                                        minError=(e1+e2+e3)/3\n                                        best_config=config\n                                        best_model=model\n                                        print('the  smape is {} wape is {} and pe is {} and total is {}'.format(e1,e2,e3,(e1+e2+e3)/3))\n                                        print('the config is {}',config)\n    if minError>=.7:\n        param = {\n        \"learning_rate\": [.001,.0001],\n        \"hidden_size\": [15,30],\n        'batch_size': [32],\n        'dropout': [.05,.5],\n        'gamma': [0.9],\n        'optimizer_name': ['Adam','AdamW'],\n        'num_layer': [3,5],\n        'relu': [True]}\n        for lr in param['learning_rate']:\n            for h_s in param['hidden_size']:\n                for b_s in param['batch_size']:\n                    for dp in param['dropout']:\n                        for opt in param['optimizer_name']:\n                            for n_l in param['num_layer']:\n                                for act in param['relu']:\n                                    for gm in param['gamma']:\n                                        config = {\n                                            \"num_epochs\": 200,\n                                            \"learning_rate\": lr,\n                                            \"hidden_size\": h_s,\n                                            \"window_size\": 30,\n                                            \"output_size\": 30,\n                                            'batch_size': b_s,\n                                            'dropout': dp,\n                                            'dropout1': 0,\n                                            'gamma': gm,\n                                            'optimizer_name': opt,\n                                            'num_layer': n_l,\n                                            'relu': act\n                                        }\n                                        print('trial run: ',counter)\n                                        counter+=1\n                                        e1,e2,e3,model = running_lstm(config, save_model=True)\n                                        \n                                        #print('the config is {}',config)\n                                        if (e1+e2+e3)/3<minError:\n                                            minError=(e1+e2+e3)/3\n                                            best_config=config\n                                            best_model=model\n                                            print(best_model)\n                                            print('the  smape is {} wape is {} and pe is {} and total is {}'.format(e1,e2,e3,(e1+e2+e3)/3))\n                                            print('the config is {}',config)\n\n    end = time.time()\n    print('thus best config is', best_config)\n    Wsmape, Wape, Wpe, smape_dis, wape_dis, pe_dis, smape_agg, wape_agg, pe_agg, model, df_lstm[:,i],wape_agg_lstm[i] = check_lstm(best_config, best_model)\n    print('val wape is: ',wape_agg_lstm[i])\n    print('total time ', end1 - end)\n    print('test Wsmape: ', Wsmape, ' test Wape: ', Wape, ' test Wpe: ', Wpe)\n    print('Smape dis',smape_dis,'Wape dis',wape_dis,'PE dis',pe_dis)\n    print('Agg SMAPE: ',smape_agg,'Agg WAPE: ',wape_agg,'Agg PE: ',pe_agg)\n    print('total time ', end1 - end)\n\n    #print('time needed to run the model: ',end-start)\n    AggSmapeL.append(smape_agg)\n    AggWapeL.append(wape_agg)\n    AggPeL.append(pe_agg)\n    SMAPEsL.append(Wsmape)\n    WAPEsL.append(Wape)\n    PEsL.append(Wpe)\n    median_smape.append(smape_dis[0])\n    Eighty_per_smape.append(smape_dis[1])\n    Ninty_per_smape.append(smape_dis[2])\n    median_wape.append(wape_dis[0])\n    Eighty_per_wape.append(wape_dis[1])\n    Ninty_per_wape.append(wape_dis[2])\n    median_pe.append(pe_dis[0])\n    Eighty_per_pe.append(pe_dis[1])\n    Ninty_per_pe.append(pe_dis[2])\n    TimesL.append(end1-end)\n    times_org.append(end1-start)\n    print('===============================================')\n    \n    "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "TimesL= [-191.39099884033203, -186.80035281181335, -186.55605792999268, -181.9619607925415, -186.2498950958252, -187.5533788204193, -190.80856323242188, -187.4871847629547, -188.06559085845947, -189.3646523952484, -189.23898100852966, -184.2036623954773, -182.4704315662384, -187.75834894180298, -187.32640528678894, -187.29383754730225, -187.18598341941833, -182.99367260932922, -190.8888065814972, -174.4357132911682, -186.6455910205841, -188.15374207496643, -188.16310358047485, -160.4605360031128, -188.5884256362915, -186.54992961883545, -191.9509789943695, -135.59119939804077, -187.15501308441162, -168.1005916595459, -189.1288924217224, -188.83188724517822, -188.41914343833923, -188.82486200332642, -187.69266819953918, -189.24378561973572, -189.0604190826416, -187.90461087226868, -186.73775386810303, -187.3940670490265, -184.01538014411926, -187.88607931137085, -185.82454347610474, -187.26987552642822, -179.27339816093445, -180.34643077850342, -186.7046022415161, -186.47635412216187, -173.9629349708557, -186.11059951782227]\nSMAPEsL= [0.2975160837302321, 0.9517995760998587, 0.692831808176359, 0.4124451768476899, 1.2552534620218747, 1.0845748873897518, 0.7190090594447431, 0.7339880118401066, 0.6723234979805491, 0.868232189906101, 0.9294815328351423, 0.7227296038078813, 0.46484658054784456, 0.7040126061976242, 1.198052611591336, 0.96169418718733, 0.742189897578649, 1.088597489630758, 1.0746741933553077, 1.0135122952562865, 0.7052878312887855, 1.158637686448772, 1.249820709395745, 0.5582052920075492, 1.37377118129461, 0.7377010985522745, 1.2546385636206083, 0.6631044700431815, 1.1672777516724093, 0.7868149070225877, 0.9651531191209015, 0.5531049673434201, 1.0796775809936423, 0.7186752821341749, 1.2108943933132577, 0.6790066940738786, 0.6746757947421075, 1.1381031176777603, 0.6124775372910485, 0.6373279253414088, 0.4529338462722793, 1.3720665632467441, 0.9968440174105152, 0.4385335817820704, 0.72836758519549, 0.3304534266026132, 0.8758016375297817, 0.8805465468857253, 0.6819107023073514, 0.7946418323424775]\nPEsL= [0.19537138530423412, 0.6740341813008532, 0.5049056099429929, 0.29194599354149026, 0.9211623537586227, 0.7505240952684394, 0.3309157223012169, 0.6907015912458557, 0.42751602879606054, 0.6404077337542503, 0.6458693252551657, 1.0527837147624628, 0.3256690642778302, 0.4671239216440135, 0.8094195927284132, 0.6591772721143437, 0.5717351465995281, 0.721778841666879, 0.7642501309050308, 0.688612016846456, 0.48244070515267345, 0.8117135414406031, 0.7986975007648563, 0.3421595649098705, 0.8715946117728668, 0.43099862432080865, 0.8429937558409638, 0.7556075036792333, 0.8069283955739864, 0.5733764986505909, 0.616836077338456, 0.41185991263898303, 0.7709700117591054, 0.39828581800395374, 0.8254129436782095, 0.3881161067485341, 0.450370511240148, 0.7909104960001491, 0.21311886815067124, 0.33055473604722635, 0.2892762773899094, 0.8397178697062297, 0.7125525541449343, 0.35130351573492663, 0.45771285587375216, 0.17980293069790854, 0.5918877769021875, 0.6274576870005036, 0.4664330781579982, 0.6110163981104872]\nWAPEsL= [0.2961820364407199, 0.6901325930334411, 0.5520904100376333, 0.36992699413210983, 0.9470156252864107, 0.7584016411756946, 0.6679775961029542, 0.9372335769021725, 0.7176513797177058, 0.6612106834628276, 0.6708704130930944, 1.1278696402372455, 0.39098395394847685, 0.5747624257419327, 0.8104471572442892, 0.6892540102099447, 0.5944494551675806, 0.7459941064880978, 0.7662950113038015, 0.6931288278250997, 0.5857168332326755, 0.8175276507191105, 0.8126569270053955, 0.41629248725546314, 0.8721214384635751, 0.7977610185603352, 0.8442505763541833, 0.90501322326964, 0.8119398789132249, 0.6645940470366817, 0.6824614202441307, 0.5423207232860755, 0.7724134304473158, 0.743349256318125, 0.838274206330701, 0.6675277546186354, 0.46108494342968953, 0.7920694474611011, 0.70905408793073, 0.6113651957874733, 0.3839991981115493, 0.8425538297019812, 0.7265018040536068, 0.48775125789104307, 0.701578669628021, 0.31905342965370376, 0.6304181668476856, 0.6401337443951617, 0.5312595806526221, 0.9649103068074721]\nAggSmapeL= [0.18129113, 0.90916866, 0.5821601, 0.18745206, 1.1922519, 1.0586249, 0.19425587, 0.46773827, 0.2714877, 0.7763762, 0.83045286, 0.6887028, 0.24269068, 0.31713033, 1.1946248, 0.871863, 0.6837027, 1.0746644, 1.0662749, 0.9502467, 0.3115867, 1.1404376, 1.1833847, 0.33553633, 1.3753049, 0.26930195, 1.2465256, 0.57135475, 1.1440444, 0.38509274, 0.50263625, 0.0629457, 1.0728611, 0.16967304, 1.1615742, 0.08780744, 0.66274774, 1.1299752, 0.40319565, 0.10216217, 0.2189459, 1.3576949, 0.94983983, 0.29899696, 0.17806068, 0.08933321, 0.74092144, 0.8524585, 0.48359603, 0.4784595]\nAggWapeL= [0.18456796, 0.670097, 0.47697294, 0.17538524, 0.9371351, 0.7469659, 0.19470479, 0.59905845, 0.28974026, 0.60849524, 0.63056076, 1.078634, 0.22097763, 0.28792307, 0.80941963, 0.64835507, 0.5641842, 0.71890193, 0.76425016, 0.6501283, 0.2863996, 0.81314826, 0.7910785, 0.29200906, 0.8715908, 0.29657945, 0.84299374, 0.7692821, 0.806579, 0.3443419, 0.43840066, 0.062052134, 0.77097, 0.17710644, 0.8277543, 0.09381598, 0.44848564, 0.7909105, 0.5197639, 0.10075865, 0.20104514, 0.8395829, 0.7068073, 0.33554754, 0.17462328, 0.08691503, 0.5711519, 0.62494296, 0.3820075, 0.5917092]\nAggPeL= [0.10312165, 0.670097, 0.47115707, 0.14382638, 0.9202667, 0.74696594, 0.08357436, 0.5990582, 0.22333132, 0.60849524, 0.6305607, 1.0301983, 0.21711935, 0.2879231, 0.80941963, 0.64835495, 0.5619536, 0.71890193, 0.7642501, 0.6501283, 0.28375876, 0.8117135, 0.7910785, 0.29200912, 0.8715908, 0.2799835, 0.84299374, 0.709856, 0.806579, 0.3293821, 0.43840066, 0.02445737, 0.77097005, 0.16508377, 0.82384235, 0.019783624, 0.44681606, 0.7909105, 0.013681516, 0.062245592, 0.19688718, 0.8395828, 0.7068073, 0.24995025, 0.065663815, 0.042339843, 0.571152, 0.6249429, 0.3820075, 0.5127261]\ntimes_org= [12.383778095245361, 7.5017406940460205, 12.371533632278442, 8.789638042449951, 0.3542673587799072, 6.755014657974243, 54.67467403411865, 42.80170202255249, 42.697455167770386, 8.166824579238892, 8.799713850021362, 3.9071707725524902, 13.266402244567871, 9.203044414520264, 0.9089932441711426, 12.037763833999634, 5.964967250823975, 8.503984212875366, 1.3453102111816406, 0.87699294090271, 4.815322399139404, 1.8010976314544678, 5.236680269241333, 11.841272830963135, 1.7394015789031982, 19.04154133796692, 1.2748408317565918, 5.729899644851685, 3.9860925674438477, 4.930696964263916, 4.387789011001587, 13.881878137588501, 0.7340834140777588, 20.11744451522827, 1.9564063549041748, 25.311565399169922, 1.0567433834075928, 1.1602137088775635, 1.5472617149353027, 13.454665899276733, 11.725611209869385, 1.9247524738311768, 6.610523223876953, 5.222144603729248, 14.969639778137207, 16.568434715270996, 7.067835807800293, 5.97577428817749, 3.0322377681732178, 24.471866846084595]\nmedian_smape= [0.5609963028325597, 0.7443376070999033, 0.6611682592321555, 0.5239184318310555, 0.8450375598356804, 0.9268463071871067, 0.818242524054085, 0.8638721587331211, 0.8053591505305961, 0.7391909793013006, 0.6700093630952193, 0.8967028198577659, 0.5102648917893631, 0.6399481381017194, 1.0215864709891254, 0.7964634903353782, 0.6544059371817383, 0.9164293035444917, 0.9678498605204879, 0.8332140287221519, 0.6040737747032603, 1.1354941948366437, 1.0597877056622158, 0.5277394112961478, 1.2891260754251928, 0.7541981809040518, 1.2269213562190922, 0.9185028114139084, 1.1114671202245208, 0.6962885002244106, 0.5875282175944833, 0.6862289106108098, 0.981195980588972, 0.7844536501894523, 0.8681696827947185, 0.6933225200672036, 0.7847144182938397, 1.0045381277783634, 0.6820313767357861, 0.7924206888307058, 0.5593588142036068, 1.0354065582306418, 0.8015866306280423, 0.6546358555796439, 0.7495176594286284, 0.5214693274462701, 0.6575955164295922, 0.7943955032966543, 0.6256993957670753, 0.8872844345977977]\nmedian_wape= [0.5427279683100066, 0.6247616428966513, 0.578603127483893, 0.49189284807981165, 0.9076921113594492, 0.7110541659746721, 0.7300973341081253, 0.9574390324002704, 0.7865194120761066, 0.6233658320592131, 0.585415914691552, 1.2102227559156207, 0.48476589932426545, 0.6345042914790358, 0.7623211718076116, 0.652479499224424, 0.5631560384063736, 0.7140692965533236, 0.7288698566242793, 0.6441052483570622, 0.5832713103206184, 0.8085983372330642, 0.7661644313453942, 0.4897687134516478, 0.8597723410564776, 0.7353483077603917, 0.8389193552938221, 1.2429995782282797, 0.7953656817911732, 0.6193741302473688, 0.5810247464704242, 0.6696133322007785, 0.7330839147247714, 0.7279557189837264, 0.6930641111182015, 0.6430927212776439, 0.6848457564762244, 0.7362370118717498, 0.5922124485244027, 0.7282585437444336, 0.5200803266281717, 0.7294182541808956, 0.6519950379708948, 0.6763622168526615, 0.6625432403807633, 0.49408113418059724, 0.5738008989644721, 0.6291540664293587, 0.5629430763636241, 0.8933858787308875]\nmedian_pe= [0.2318303961112487, 0.5112922459213256, 0.3985654077309583, 0.2255768764012969, 0.3742816682843032, 0.6853686161917598, 0.27618041196649523, 0.6427168276517221, 0.37329546124420376, 0.510907127043798, 0.46610708567409675, 0.9162323688066067, 0.21527947134088124, 0.3085051317860171, 0.7344231285132261, 0.5567568549089658, 0.4909243551966169, 0.6496326235504952, 0.7002305392262632, 0.6201303484626974, 0.2214537240311219, 0.7960018147727314, 0.7580101959957156, 0.22095162023398957, 0.8565535876790003, 0.2825044113085666, 0.831677863924085, 1.0157245840142437, 0.773361710512467, 0.42214953922398524, 0.28273548369503415, 0.30923720587209813, 0.7207754821901828, 0.30228834946398425, 0.5960265437572405, 0.2814313585294092, 0.3891127876988436, 0.7113093850987533, 0.29663045073593597, 0.27845504903781065, 0.18865339677009277, 0.6987433416220364, 0.5990245126114722, 0.3300208597250598, 0.3535791755096092, 0.17361214061008376, 0.42880767210317594, 0.5614959228767961, 0.2634607654959389, 0.47285108005023635]\nEighty_per_smape= [0.7998104372051873, 0.9253911318018133, 0.8401092869726376, 0.7440541750414166, 0.9751814469575949, 1.118049691719625, 1.0017042964681677, 1.1749334422956546, 1.0449494814517268, 0.99564534878048, 0.8744917865648797, 1.098717953591028, 0.766290743765839, 0.8463415580071236, 1.2188407374497494, 1.069544005194346, 0.8990248530007395, 1.1704168296764355, 1.0944925292024978, 0.9976148722040172, 0.8342879169344808, 1.2539219692318089, 1.2255407440552077, 0.6983241136950588, 1.4111715461819856, 0.9549572753928957, 1.3323894840717054, 1.1203818712777307, 1.288439695343414, 0.8926745119294498, 0.8275311070612865, 0.8769342826648803, 1.1572349484325692, 0.9716115194507725, 1.0308316891302511, 0.8663841430572876, 1.065971665695065, 1.2359188420346034, 0.8469657980590652, 0.9580880191297139, 0.7488813749440566, 1.2950994096746085, 1.032276990064368, 0.8613218530795514, 0.9813991714173502, 0.7051296806916151, 0.8789599032232565, 1.0294671824063044, 0.8486812939049553, 1.0995171344814711]\nEighty_per_wape= [0.7897630161604843, 0.7256592711585479, 0.6947835530758372, 0.672472769132755, 0.9595254393599582, 0.7954343065526014, 0.928340561243481, 1.6655635123394485, 1.2195693144012472, 0.7548279447292243, 0.7109674103575941, 1.778649324880059, 0.6833489802005505, 0.8646655879859566, 0.8237224282426098, 0.7736699225380661, 0.7016421262384229, 0.7977947254976305, 0.7838658836123323, 0.7339788535298757, 0.7922942405908907, 0.8578297931506103, 0.8408952561240036, 0.6289023997510849, 0.8953142779057772, 0.9802768370300218, 0.8792192357525416, 1.8041162310864811, 0.864303686656243, 0.8994254081602466, 0.8275168466145346, 0.9075203001161056, 0.8109220883838107, 1.0139737339383321, 0.7551413888166204, 0.8569912886686817, 0.9062596275532231, 0.8293179246773597, 0.8127293208942904, 0.9296209049719009, 0.6704405294191191, 0.8364340568885698, 0.76423862870519, 0.9163013882836601, 1.0415033452575173, 0.654555786991688, 0.7012753297992218, 0.7478248348104121, 0.72394770919621, 1.5067332406187106]\nEighty_per_pe= [0.4879603846417363, 0.6802366227453446, 0.5945216808797941, 0.4334342956632178, 0.6659339223619051, 0.7844450809433191, 0.5187659042193853, 1.3786269875776624, 0.8377599135590001, 0.7282805113876338, 0.6536974416185806, 1.5384004545750927, 0.47746160745716243, 0.5688030858162457, 0.8196372479397466, 0.7473876794276338, 0.6629351095107754, 0.7738988280599342, 0.7768806260529512, 0.721325085005952, 0.46777088483462975, 0.8453226857154361, 0.8323546767852816, 0.4180921883978913, 0.8953142779057772, 0.6283003077913711, 0.8787654005385116, 1.5936424976004444, 0.8560508082440885, 0.7516356884688689, 0.6606479977712371, 0.6367201989551909, 0.7964872298930704, 0.6809222180161768, 0.7238417890269206, 0.5868788931564857, 0.5163597754810112, 0.825860907474428, 0.5553287842284637, 0.5575502775810776, 0.40274232611540983, 0.8095436346113694, 0.7393317044196153, 0.6485331639740153, 0.7583515516398679, 0.3806252480145982, 0.6292646363753831, 0.7131595806194846, 0.44796959279574494, 1.1724441149556784]\nNinty_per_smape= [0.9387321932604608, 1.037017681498811, 0.9962022714247235, 0.8385606691151082, 1.2799856837360788, 1.2839226631356713, 1.1045752302557648, 1.3200920273042418, 1.2187914231913635, 1.109712782738943, 1.057515801493842, 1.2146551971900772, 0.9582530288367963, 0.9435708738109972, 1.2804198962521642, 1.194751530086793, 0.9800424564897385, 1.2526979988714113, 1.1259610184171345, 1.0864793175511436, 1.007554377955253, 1.311524038642437, 1.295648916213469, 0.820179040267953, 1.4365993036038067, 1.061573135658119, 1.3663576932387265, 1.2350409214253406, 1.3335055638573983, 1.0819635974244661, 1.0166198942442075, 1.0155813229156303, 1.2212116155830965, 1.073234155131288, 1.1793651235585747, 0.9577488604268701, 1.133239668691434, 1.3073344402740112, 1.0119316778126342, 1.050874156587916, 0.857665895756587, 1.3538884739073076, 1.1410008176325885, 0.9723819186135523, 1.1011031599787269, 0.8150435151738884, 0.9754821589403754, 1.127811563432972, 0.9881642166488682, 1.2387415917034614]\nNinty_per_wape= [0.9266831406524474, 0.77689311840247, 0.7607736392941296, 0.7840772211652337, 1.043976387607475, 0.8489651986330305, 1.1224752033291827, 2.2808190844428458, 1.553287145210414, 0.7958204371406481, 0.7916619741073431, 2.06305480417748, 0.8110016717893551, 1.0262162958407621, 0.8426745328970017, 0.8320623366245777, 0.7425823727434925, 0.8228754812240384, 0.7968787369331671, 0.8059842543812019, 0.9393961766953003, 0.8801816364678879, 0.8756179860019832, 0.7357994440258608, 0.9036079401327841, 1.3565759790588159, 0.8917614714784504, 2.223664981920967, 0.8786931888142104, 1.2349675799137025, 0.9521383869821575, 1.1477412888687364, 0.8311392892386436, 1.1954503984693008, 0.8166822774342739, 1.0371221873554661, 0.9661723746083439, 0.8627937233261178, 1.0833798940886061, 1.1274280539760666, 0.7907365434246774, 0.8660571032844123, 0.8147287459125478, 1.2074613943536752, 1.3606508304300264, 0.7553093316729067, 0.7739168583762001, 0.8028915417584404, 0.8334027307561109, 1.8991566228685854]\nNinty_per_pe= [0.7151413938236394, 0.7379929867094573, 0.6999961715635583, 0.5850632706455594, 0.9442715166988115, 0.8356167513419249, 0.682170341961486, 2.096742087111955, 1.2177587755954276, 0.7829518085893936, 0.7570126561007469, 1.8727859015346437, 0.7151366542286506, 0.8201817062105021, 0.8381260568421005, 0.8172712729421262, 0.7131513290347954, 0.8126122201670195, 0.7883786896434618, 0.765064429860133, 0.7711760277150039, 0.876998317232563, 0.8655319235220157, 0.5265853221282305, 0.903607940132784, 1.0069324247011924, 0.8898441449710134, 2.0191348070132142, 0.8745971052624147, 0.9249288200470739, 0.8736205114696752, 0.8610081109583392, 0.8225603261697287, 0.9661293497553329, 0.7826320639443636, 0.8326899280878575, 0.759200897025226, 0.8627937233261178, 0.6006934109381604, 0.7768214801962008, 0.5298269440667533, 0.8518128760488959, 0.7871364271613105, 0.9698793278844853, 1.0698216349711893, 0.49660768863187466, 0.7160128911783297, 0.7690891768834591, 0.653116335457321, 1.6435720386821877]\n"}], "source": "print('TimesL=',TimesL)\nprint('SMAPEsL=',SMAPEsL)\nprint('PEsL=',PEsL)\nprint('WAPEsL=',WAPEsL)\nprint('AggSmapeL=',AggSmapeL)\nprint('AggWapeL=',AggWapeL)\nprint('AggPeL=',AggPeL)\nprint('times_org=',times_org)\nprint('median_smape=',median_smape)\nprint('median_wape=',median_wape)\nprint('median_pe=',median_pe)\nprint('Eighty_per_smape=',Eighty_per_smape)\nprint('Eighty_per_wape=',Eighty_per_wape)\nprint('Eighty_per_pe=',Eighty_per_pe)\nprint('Ninty_per_smape=',Ninty_per_smape)\nprint('Ninty_per_wape=',Ninty_per_wape)\nprint('Ninty_per_pe=',Ninty_per_pe)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "def _generate_train_test_data(df, window_size=30,val_req=False,start='2020-11-01'):\n    start = pd.to_datetime(start, format='%Y-%m-%d') #+ relativedelta(months=months)\n    end = start + relativedelta(days=365 + 30) #+ relativedelta(months=months)    \n    df3 = df.astype({'query_idx': 'string'})\n    df3.sort_values('ds', inplace=True)\n    df1 = pd.pivot_table(df3, values='daily_supply', index=['ds'],\n                         columns=['group_id', 'query_idx', 'SearchKeyValue', 'LocationKeyValue'])\n\n    df1.columns = ['_'.join(col) for col in df1.columns.values]\n    print('Total TS before filter: ', df1.shape[1])\n\n    df1.index = pd.to_datetime(df1.index)\n\n#     start = df1.index[0]#****\n#     end = df1.index[-1]#*****\n            \n    \n    #start,end=pd.to_datetime('2020-05-02',format='%Y-%m-%d'),pd.to_datetime('2022-05-02',format='%Y-%m-%d')\n#     start1=df1.index.sort_values()[0]\n#     end1=df1.index.sort_values()[len(df1)-1]\n#     #print(df1.index.sort_values())\n#     if end1<end:end=end1\n#     if start1>start:start=start1\n    df1=df1[(df1.index>=start) & (df1.index<=end)]\n    df1.sort_index(inplace=True)\n\n    train_test_split = end - pd.to_timedelta(30, unit='d')  ############30 needs to be a variable\n    # start_temp2 = pd.to_datetime((train_test_split + relativedelta(days=-4 * window_size)).strftime('%Y-%m-%d'))\n\n    end_train = pd.to_datetime((train_test_split).strftime('%Y-%m-%d'))\n    start_test = pd.to_datetime((train_test_split + relativedelta(days=1)).strftime('%Y-%m-%d'))\n    #print(df1.index.unique())\n    dates = pd.date_range(start=start, end=end, freq='D').format()\n    s = pd.Series(np.nan, index=pd.date_range(start, end, freq='D'))\n    #df1['ds']=pd.to_datetime(df_['ds'])\n    #df_.set_index('ds',inplace=True)\n    df1=pd.concat([df1,s[~s.index.isin(df1.index)]]).sort_index()\n    \n    #print(df1)\n    #del df1['0']\n    #dates = pd.date_range(start=start, end=end, freq='D').format()\n    #df1=df1.reindex(dates, fill_value=0)\n    df1.index = pd.to_datetime(df1.index)\n    df1 = df1.fillna(0)\n#########################    \n    df1 = df1.loc[:, df1.isin([0]).sum(axis=0) <= 50]\n########################\n    #     df1=df1.loc[:,df1.notna().sum()>90]\n    #     df1 = df1.fillna(0)\n    try:del df1[0]\n    except:pass\n    # impute_missing_time_modify(temp, train_test_split, end, 4, train=True)\n    df_train, df_test = df1[(df1.index <= end_train)], df1[df1.index >= start_test]\n    # temp_train, temp_test = temp[temp['ds'] < train_test_split], temp[temp['ds'] >= train_test_split]\n    print('Total TS after filter: ', df1.shape[1])\n    df_train.sort_index(inplace=True)\n    #print(df_train.iloc[-14:,:])\n    \n    #df_train_temp=df_train_temp[(df_train_temp.index <= end_train)], df_train_temp[df_train_temp.index >= start_test]\n    \n    qtrain = df_train.iloc[-14:,:].sum(axis=0) / np.sum(df_train.iloc[-14:,:].sum(axis=0))\n    qtest = df_test.sum(axis=0) / np.sum(df_test.sum(axis=0))  # df_train.iloc[-30:,].sum(axis=0)/np.sum(df_train.iloc[-30:,].sum(axis=0))*100\n    # average=df_train.mean(axis=0).values\n    # df_train+=average\n    # df_test+= average\n    train_date, test_date = df_train.index, df_test.index\n    df_train.reset_index(inplace=True, drop=True)\n    df_test.reset_index(inplace=True, drop=True)\n    # df.values.sum()\n    # q=df.sum(axis=1)/np.sum(np.sum(df))\n    # print(q)\n    if val_req:\n        df_train,df_val=df_train.iloc[:-30,:],df_train.iloc[-30:,:]\n        return df_train, df_val,df_test, train_date.values[:-30], test_date.values, qtrain, qtest\n    return df_train, df_test, train_date.values, test_date.values, qtrain, qtest\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# class statModels:\n#     def __init__(self):\n#         # self.df_train=df\n#         self.models = {}\n#         self.stores_smape = {}\n#         self.stores_times = {}\n#         self.store_wape = {}\n#         self.store_pe={}\n#         self.wape_dis={}\n#         self.smape={}\n#         self.pe_dis={}\n#         self.smape_dis={}\n#         self.sc = MinMaxScaler(feature_range=(0, 1))\n#         self.clus_wape={}\n#         self.clus_smape={}\n#         self.clus_pe={}\n#         self.wape_Weighted={}\n#         self.dfStore=None\n#         self.mape=[]\n#     def arimamodel(self, timeseriesarray):\n#         autoarima_model = pmd.auto_arima(timeseriesarray, start_p=0, d=0, trace=1,\n#                                          suppress_warnings=True, seasonal=True, m=12, D=1,\n#                                          start_q=0, error_action='ignore', stationary=True, njob=-1,\n#                                          test=\"adf\", stepwise=True)\n#         return autoarima_model\n\n#     def preprocessing(self, df_train):\n#         temp = df_train  # np.log(df_train)\n#         temp = temp.reshape((-1, 1))\n#         return self.sc.fit_transform(temp)\n\n#     def postprocess(self, prediction, df_test_org,dfx_test, q, wq, ensemble=False,Weighted=False):\n#         # prediction=np.exp(prediction)\n#         if type(prediction)==pd.core.frame.DataFrame:\n#             prediction = prediction.values.reshape(-1, 1)\n#         else:prediction = prediction.reshape(-1, 1)\n#         if not ensemble:\n#             prediction = self.sc.inverse_transform(prediction)\n#         re_elements = wq.shape[0]  # testY.shape[-1]\n#         testing_predict = prediction.reshape(-1, )\n#         #else:testing_predict = prediction\n#         clus_wape,clus_smape,clus_pe=WAPE(dfx_test,testing_predict),SMAPE(dfx_test,testing_predict),PE(dfx_test,testing_predict)\n\n#         if Weighted:\n#             return clus_wape,prediction\n        \n#         testing_predict = np.tile(testing_predict, (re_elements, 1))\n#         testing_predict = testing_predict.T\n#         # df_test_org=df_test_org.iloc[:,np.argsort(wq)[::-1][:no_ts]]\n#         # testing_predict=testing_predict[:,np.argsort(wq)[::-1][:no_ts]]\n#         # wq_mod=wq[np.argsort(wq)[::-1][:no_ts]]\n        \n        \n        \n#         testing_predict *= q\n\n#         # mape_error = np.abs(df_test_org.values.reshape(-1,) - testing_predict.reshape(-1,)) / df_test_org.values.reshape(-1,)\n#         #wape_error = WAPE_fun(df_test_org.values, testing_predict, wq)\n\n#         # testing_predict[testing_predict < 0] = 0\n\n#         # mape_mean = np.mean(mape_error)\n#         # print('shape of pred ',testing_predict.shape)\n#         # print('shape of True ',df_test_org.shape)\n#         Wsmape, Wwape,Wpe = estimate(df_test_org.values, testing_predict, wq)\n#         smape_dis,wape_dis,pe_dis=metric_dist(df_test_org.values, testing_predict, wq,metric='smape'),metric_dist(df_test_org.values, testing_predict, wq,metric='wape') ,metric_dist(df_test_org.values, testing_predict, wq,metric='pe')\n#         #smape_mean = weightedSMAPE(df_test_org.values, testing_predict, wq)\n#         #mape_mean = weightedMAPE(df_test_org.values, testing_predict, wq)\n\n#         # testing_predict = testing_predict - average[np.argsort(wq)[::-1][:no_ts]]\n#         # testing_predict[testing_predict < 0] = 0\n#         return Wsmape, Wwape,Wpe,smape_dis,wape_dis,pe_dis,clus_smape,clus_wape,clus_pe, prediction  # ,testing_predict\n#     def Mape(self,A,F):\n#         res=[]\n#         for i in range(A.shape[0]):\n#             res.append(np.abs(A[i]-F[i])/A[i])\n#         return res\n#     def training_infer(self, df_train, df_test, df_test_org,q, wq, forecast_period=30,lstm_pred=None):\n#         '''\n#         df_test: pandas (shape:(:,1))\n#         df_test_org: pandas (shape(:,no_ts))\n#         '''\n\n#         # self.df_train_log,self.df_test_log=test_train_spl(self.df,ratio=.8)\n#         ###\n#         fh = np.arange(forecast_period) + 1\n#         ###\n#         df_train['daily_supply'] = self.preprocessing(df_train.values)\n#         self.df_train = df_train\n#         # print(df_train['daily_supply'])\n#         df_store = df_test.to_frame()\n#         df_store = df_store.rename(columns={0: 'daily_supply'})\n#         ###################################################\n#         #df_store['lstm']=lstm_pred\n#         #print('naive')\n#         # naive\n#         start = time.time()\n#         forecaster=NaiveForecaster(strategy=\"last\")\n#         forecaster.fit(self.df_train)\n#         y_pred = forecaster.predict(fh)\n#         df_store['naive'] = y_pred.values\n#         # print(df_store['snaive'])\n#         # print(df_store)\n#         Wsmape, Wwape,Wpe,smape_dis,wape_dis,pe_dis,clus_smape,clus_wape,clus_pe,df_store['naive'] = self.postprocess(df_store['naive'].values, df_test_org,df_test,q, wq)\n#         self.mape.append(self.Mape(df_test,df_store['naive']))\n#         end = time.time()\n#         self.clus_smape['naive']=clus_smape\n#         self.clus_wape['naive']=clus_wape\n#         self.clus_pe['naive']=clus_pe\n#         #self.stores_mape['snaive'] = mape\n#         self.stores_smape['naive'] = Wsmape\n#         self.store_wape['naive'] = Wwape\n#         self.store_pe['naive']=Wpe\n#         self.wape_dis['naive']=wape_dis\n#         self.pe_dis['naive']=pe_dis\n#         self.smape_dis['naive']=smape_dis\n        \n        \n#         self.stores_times['naive'] = end - start\n#         self.models['naive'] = forecaster\n        \n\n#         #print('snaive')\n#         # snaive\n#         start = time.time()\n# #         y = self.pysnaive(self.df_train, 52, forecast_period)[1].iloc[:forecast_period]\n# #         df_store['snaive'] = y.values\n#         forecaster1=NaiveForecaster(strategy=\"last\",sp=12)\n#         forecaster1.fit(self.df_train)\n#         y_pred = forecaster1.predict(fh)\n#         Wsmape, Wwape,Wpe,smape_dis,wape_dis,pe_dis,clus_smape,clus_wape,clus_pe,df_store['snaive'] = self.postprocess(y_pred.values, df_test_org,df_test,q, wq)\n#         self.mape.append(self.Mape(df_test,df_store['snaive']))\n#         end = time.time()\n#         self.clus_smape['snaive']=clus_smape\n#         self.clus_wape['snaive']=clus_wape\n#         self.clus_pe['snaive']=clus_pe\n#         self.stores_smape['snaive'] = Wsmape\n#         self.store_wape['snaive'] = Wwape\n#         self.store_pe['snaive']=Wpe\n#         self.wape_dis['snaive']=wape_dis\n#         self.pe_dis['snaive']=pe_dis\n#         self.smape_dis['snaive']=smape_dis\n        \n#         self.stores_times['snaive'] = end - start\n#         self.models['snaive'] = forecaster1\n        \n#         #print('arima')\n#         start = time.time()\n#         try: \n#             model_fit = ARIMA(self.df_train, order=(1, 1, 1)).fit()\n#             df_store['arima'], _, _ = model_fit.forecast(forecast_period)  # 95% conf\n#             print('done')\n#         except (np.linalg.linalg.LinAlgError,ValueError):\n#             model_fit = self.arimamodel(self.df_train)\n#             df_store['arima']=model_fit.predict(forecast_period)\n#             print('done3')\n# #         else:\n# #             model_fit = ARIMA(self.df_train, order=(0, 0, 0)).fit()\n# #             df_store['arima']=model_fit.predict(forecast_period)\n# #             print('done3')\n            \n        \n#         #df_store['arima']= model_fit.forecast(forecast_period).values  # 95% conf\n#         Wsmape, Wwape,Wpe,smape_dis,wape_dis,pe_dis,clus_smape,clus_wape,clus_pe,df_store['arima'] = self.postprocess(df_store['arima'].values, df_test_org,df_test,q, wq)\n#         self.mape.append(self.Mape(df_test,df_store['arima']))\n#         end = time.time()\n\n#         self.models['arima'] = model_fit\n#         # self.stores_mape['arima'] = mape\n#         self.clus_smape['arima']=clus_smape\n#         self.clus_wape['arima']=clus_wape\n#         self.clus_pe['arima']=clus_pe\n#         self.stores_smape['arima'] = Wsmape\n#         self.store_wape['arima'] = Wwape\n#         self.store_pe['arima']=Wpe\n#         self.wape_dis['arima']=wape_dis\n#         self.pe_dis['arima']=pe_dis\n#         self.smape_dis['arima']=smape_dis\n        \n#         self.stores_times['arima'] = end - start\n    \n\n#         #print('ses')\n#         # ses\n#         start = time.time()\n#         fit3 = SimpleExpSmoothing(self.df_train, initialization_method=\"estimated\").fit()\n#         df_store['ses'] = fit3.forecast(forecast_period).values\n#         Wsmape, Wwape,Wpe,smape_dis,wape_dis,pe_dis,clus_smape,clus_wape,clus_pe,df_store['ses'] = self.postprocess(df_store['ses'].values, df_test_org,df_test,q, wq)\n#         self.mape.append(self.Mape(df_test,df_store['ses']))\n#         end = time.time()\n#         self.models['ses'] = fit3\n\n#         #self.stores_mape['ses'] = mape\n#         self.clus_smape['ses']=clus_smape\n#         self.clus_wape['ses']=clus_wape\n#         self.clus_pe['ses']=clus_pe\n#         self.stores_smape['ses'] = Wsmape\n#         self.store_wape['ses'] = Wwape\n#         self.store_pe['ses']=Wpe\n#         self.wape_dis['ses']=wape_dis\n#         self.pe_dis['ses']=pe_dis\n#         self.smape_dis['ses']=smape_dis\n#         self.stores_times['ses'] = end - start\n#         #print('holts')\n#         # holts\n#         start = time.time()\n#         try:\n#             fit3 = Holt(self.df_train, damped_trend=True, exponential=True, initialization_method=\"estimated\").fit(\n#                 smoothing_level=0.8, smoothing_trend=0.1)\n#         except (np.linalg.linalg.LinAlgError,ValueError):\n#             fit3 = Holt(self.df_train, damped_trend=True, exponential=False, initialization_method=\"estimated\").fit(\n#                 smoothing_level=0.8, smoothing_trend=0.1)\n#         df_store['holts'] = fit3.forecast(forecast_period).values\n#         Wsmape, Wwape,Wpe,smape_dis,wape_dis,pe_dis,clus_smape,clus_wape,clus_pe,df_store['holts'] = self.postprocess(df_store['holts'].values, df_test_org,df_test,q, wq)\n#         self.mape.append(self.Mape(df_test,df_store['holts']))\n#         end = time.time()\n\n#         self.models['holts'] = fit3\n#         #self.stores_mape['holts'] = mape\n#         self.clus_smape['holts']=clus_smape\n#         self.clus_wape['holts']=clus_wape\n#         self.clus_pe['holts']=clus_pe\n#         self.stores_smape['holts'] = Wsmape\n#         self.store_wape['holts'] = Wwape\n#         self.store_pe['holts']=Wpe\n#         self.wape_dis['holts']=wape_dis\n#         self.pe_dis['holts']=pe_dis\n#         self.smape_dis['holts']=smape_dis\n#         self.stores_times['holts'] = end - start\n#         #print('Exp_smoothing')\n#         # Exp_smoothing\n#         start = time.time()\n#         try:\n#             fit4 = ExponentialSmoothing(self.df_train, seasonal_periods=52, trend=\"add\", seasonal=\"add\",\n#                                         damped_trend=True, use_boxcox=True, initialization_method=\"estimated\", ).fit()\n#         except:\n#             fit4 = ExponentialSmoothing(self.df_train, seasonal_periods=52, trend=\"add\", seasonal=\"add\",\n#                                         damped_trend=True, use_boxcox=False, initialization_method=\"estimated\", ).fit()\n#         df_store['Es'] = fit4.forecast(forecast_period).values\n#         Wsmape, Wwape,Wpe,smape_dis,wape_dis,pe_dis,clus_smape,clus_wape,clus_pe,df_store['Es'] = self.postprocess(df_store['Es'].values, df_test_org,df_test,q, wq)\n#         self.mape.append(self.Mape(df_test,df_store['Es']))\n#         end = time.time()\n#         self.models['Es'] = fit4\n#         #self.stores_mape['Es'] = mape\n#         self.clus_smape['Es']=clus_smape\n#         self.clus_wape['Es']=clus_wape\n#         self.clus_pe['Es']=clus_pe\n#         self.stores_smape['Es'] = Wsmape\n#         self.store_wape['Es'] = Wwape\n#         self.store_pe['Es']=Wpe\n#         self.wape_dis['Es']=wape_dis\n#         self.pe_dis['Es']=pe_dis\n#         self.smape_dis['Es']=smape_dis\n#         self.stores_times['Es'] = end - start\n\n        \n#         #print('Auto_ETS')\n#         # Auto_ETS\n#         start = time.time()\n#         try: \n#             forecaster = AutoETS(auto=True, sp=12, n_jobs=-1, error='add', trend='True', damped_trend='True', seasonal='add')\n#             forecaster.fit(self.df_train)\n#         except (ValueError):\n#             forecaster = AutoETS(auto=False, sp=7, n_jobs=-1, error='add')\n#             forecaster.fit(self.df_train)\n#         df_store['auto_ets'] = forecaster.predict(fh).values\n#         Wsmape, Wwape,Wpe,smape_dis,wape_dis,pe_dis,clus_smape,clus_wape,clus_pe,df_store['auto_ets'] = self.postprocess(df_store['auto_ets'].values, df_test_org,df_test,q, wq)\n#         self.mape.append(self.Mape(df_test,df_store['auto_ets']))\n#         end = time.time()\n#         self.models['auto_ets'] = forecaster\n#         #self.stores_mape['auto_ets'] = mape\n#         self.clus_smape['auto_ets']=clus_smape\n#         self.clus_wape['auto_ets']=clus_wape\n#         self.clus_pe['auto_ets']=clus_pe\n#         self.stores_smape['auto_ets'] = Wsmape\n#         self.store_wape['auto_ets'] = Wwape\n#         self.store_pe['auto_ets']=Wpe\n#         self.wape_dis['auto_ets']=wape_dis\n#         self.pe_dis['auto_ets']=pe_dis\n#         self.smape_dis['auto_ets']=smape_dis\n#         self.stores_times['auto_ets'] = end - start\n        \n#         #theta\n#         start = time.time()\n#         forecaster4 = ThetaForecaster(sp=52, deseasonalize=False)\n#         forecaster4.fit(self.df_train)\n#         df_store['theta'] = forecaster4.predict(fh).values\n#         Wsmape, Wwape,Wpe,smape_dis,wape_dis,pe_dis,clus_smape,clus_wape,clus_pe,df_store['theta'] = self.postprocess(df_store['theta'].values, df_test_org,df_test,q, wq)\n#         self.mape.append(self.Mape(df_test,df_store['theta']))\n#         end = time.time()\n\n#         self.models['theta'] = forecaster4\n#         #self.stores_mape['theta'] = mape\n#         self.clus_smape['theta']=clus_smape\n#         self.clus_wape['theta']=clus_wape\n#         self.clus_pe['theta']=clus_pe\n#         self.stores_smape['theta'] = Wsmape\n#         self.store_wape['theta'] = Wwape\n#         self.store_pe['theta']=Wpe\n#         self.wape_dis['theta']=wape_dis\n#         self.pe_dis['theta']=pe_dis\n#         self.smape_dis['theta']=smape_dis\n#         self.stores_times['theta'] = end - start\n\n#         #print('Croston')\n#         # Croston\n#         start = time.time()\n#         forecaster5 = Croston(smoothing=.2)\n#         forecaster5.fit(self.df_train)\n#         df_store['croston'] = forecaster5.predict(fh).values\n#         Wsmape, Wwape,Wpe,smape_dis,wape_dis,pe_dis,clus_smape,clus_wape,clus_pe,df_store['croston'] = self.postprocess(df_store['croston'].values, df_test_org,df_test,q, wq)\n#         self.mape.append(self.Mape(df_test,df_store['croston']))\n#         end = time.time()\n\n#         self.models['croston'] = forecaster5\n#         #self.stores_mape['croston'] = mape\n#         self.clus_smape['croston']=clus_smape\n#         self.clus_wape['croston']=clus_wape\n#         self.clus_pe['croston']=clus_pe\n#         self.stores_smape['croston'] = Wsmape\n#         self.store_wape['croston'] = Wwape\n#         self.store_pe['croston']=Wpe\n#         self.wape_dis['croston']=wape_dis\n#         self.pe_dis['croston']=pe_dis\n#         self.smape_dis['croston']=smape_dis\n#         self.stores_times['croston'] = end - start\n\n#         #prophet\n#         start = time. time()\n#         df_train=self.df_train.squeeze()\n#         df_train.index.name='Period'\n#         df_train.index=df_train.index.to_timestamp()\n#         forecaster6 = Prophet(seasonality_mode='additive',n_changepoints=int(len(df_train) / 12),\n#             add_country_holidays={'country_name': 'US'},yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=True)\n#         forecaster6.fit(df_train)\n#         df_store['prophet'] = forecaster6.predict(fh).values\n#         Wsmape, Wwape,Wpe,smape_dis,wape_dis,pe_dis,clus_smape,clus_wape,clus_pe,df_store['prophet'] = self.postprocess(df_store['prophet'].values, df_test_org,df_test,q, wq)\n#         self.mape.append(self.Mape(df_test,df_store['prophet']))\n#         end = time.time()\n\n#         self.models['prophet'] = forecaster6\n#         # self.stores_mape['croston'] = mape\n#         self.clus_smape['prophet']=clus_smape\n#         self.clus_wape['prophet']=clus_wape\n#         self.clus_pe['prophet']=clus_pe\n#         self.stores_smape['prophet'] = Wsmape\n#         self.store_wape['prophet'] = Wwape\n#         self.store_pe['prophet']=Wpe\n#         self.wape_dis['prophet']=wape_dis\n#         self.pe_dis['prophet']=pe_dis\n#         self.smape_dis['prophet']=smape_dis\n#         self.stores_times['prophet'] = end - start\n\n#         #print('Average')\n#         # Average of stats models\n#         tog_time = np.sum(list(self.stores_times.values()))\n#         start = time.time()\n#         df_temp = df_store.drop(['daily_supply'], axis=1)  # df_store.loc[:, df_store.columns != 'daily_supply']\n#         df_store['Avg_ensemble'] = df_temp.mean(axis=1).values\n\n#         Wsmape, Wwape,Wpe,smape_dis,wape_dis,pe_dis,clus_smape,clus_wape,clus_pe,df_store['Avg_ensemble'] = self.postprocess(df_store['Avg_ensemble'].values, df_test_org,df_test,q, wq,ensemble=True)\n        \n#         end = time.time()\n#         #self.stores_mape['Avg_ensemble'] = mape\n#         self.clus_smape['Avg_ensemble']=clus_smape\n#         self.clus_wape['Avg_ensemble']=clus_wape\n#         self.clus_pe['Avg_ensemble']=clus_pe\n#         self.stores_smape['Avg_ensemble'] = Wsmape\n#         self.store_wape['Avg_ensemble'] = Wwape\n#         self.store_pe['Avg_ensemble']=Wpe\n#         self.wape_dis['Avg_ensemble']=wape_dis\n#         self.pe_dis['Avg_ensemble']=pe_dis\n#         self.smape_dis['Avg_ensemble']=smape_dis\n#         self.stores_times['Avg_ensemble'] = (end - start) + tog_time-self.stores_times['Lstm']#%%%%%%%%%%%%\n#         self.dfStore=df_store\n\n# #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@        \n# #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n#         #print('weighted')\n#         # weighted average\n#         #del df_store['daily_supply']\n#         subset = dfStore.drop(['daily_supply', 'Avg_ensemble'], axis=1)\n#         pred=np.zeros(subset.shape[0],1)\n#         for i in range(subset.shape[1]):\n#             res=[]\n#             w=self.mape[i]\n#             w=[1/q**2 for q in w]\n#             for j in range(subset.shape[0]):\n#                 pred[j,0]+=(subset.iloc[j,i]*w[j])\n        \n#         df_store['Weighted_ensemble'] = pred#(df_store * store_error).sum(1).values\n#         Wsmape, Wwape,Wpe,smape_dis,wape_dis,pe_dis,clus_smape,clus_wape,clus_pe,df_store['Weighted_ensemble'] = self.postprocess(df_store['Weighted_ensemble'].values, df_test_org,df_test,q, wq,ensemble=True)\n        \n#         end = time.time()\n#         self.clus_smape['Weighted_ensemble']=clus_smape\n#         self.clus_wape['Weighted_ensemble']=clus_wape\n#         self.clus_pe['Weighted_ensemble']=clus_pe\n#         self.stores_smape['Weighted_ensemble'] = Wsmape\n#         self.store_wape['Weighted_ensemble'] = Wwape\n#         self.store_pe['Weighted_ensemble']=Wpe\n#         self.wape_dis['Weighted_ensemble']=wape_dis\n#         self.pe_dis['Weighted_ensemble']=pe_dis\n#         self.smape_dis['Weighted_ensemble']=smape_dis\n#         self.stores_times['Weighted_ensemble'] = (end - start)#+self.stores_times['Lstm']\n#         self.dfStore['Weighted_ensemble']=df_store['Weighted_ensemble'].values\n    \n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "class statModels:\n    def __init__(self):\n        # self.df_train=df\n        self.models = {}\n        self.stores_smape = {}\n        self.stores_times = {}\n        self.store_wape = {}\n        self.store_pe={}\n        self.wape_dis={}\n        self.smape={}\n        self.pe_dis={}\n        self.smape_dis={}\n        self.sc = MinMaxScaler(feature_range=(0, 1))\n        self.clus_wape={}\n        self.clus_smape={}\n        self.clus_pe={}\n        self.wape_Weighted={}\n        self.dfStore=None\n    def arimamodel(self, timeseriesarray):\n        autoarima_model = pmd.auto_arima(timeseriesarray, start_p=0, d=0, trace=1,\n                                         suppress_warnings=True, seasonal=True, m=12, D=1,\n                                         start_q=0, error_action='ignore', stationary=True, njob=-1,\n                                         test=\"adf\", stepwise=True)\n        return autoarima_model\n\n    def preprocessing(self, df_train):\n        temp = df_train  # np.log(df_train)\n        temp = temp.reshape((-1, 1))\n        return self.sc.fit_transform(temp)\n\n    def postprocess(self, prediction, df_test_org,dfx_test, q, wq, ensemble=False,Weighted=False):\n        # prediction=np.exp(prediction)\n        if type(prediction)==pd.core.frame.DataFrame:\n            prediction = prediction.values.reshape(-1, 1)\n        else:prediction = prediction.reshape(-1, 1)\n        if not ensemble:\n            prediction = self.sc.inverse_transform(prediction)\n        re_elements = wq.shape[0]  # testY.shape[-1]\n        testing_predict = prediction.reshape(-1, )\n        #else:testing_predict = prediction\n        clus_wape,clus_smape,clus_pe=WAPE(dfx_test,testing_predict),SMAPE(dfx_test,testing_predict),PE(dfx_test,testing_predict)\n\n        if Weighted:\n            return clus_wape,prediction\n        \n        testing_predict = np.tile(testing_predict, (re_elements, 1))\n        testing_predict = testing_predict.T\n        # df_test_org=df_test_org.iloc[:,np.argsort(wq)[::-1][:no_ts]]\n        # testing_predict=testing_predict[:,np.argsort(wq)[::-1][:no_ts]]\n        # wq_mod=wq[np.argsort(wq)[::-1][:no_ts]]\n        \n        \n        \n        testing_predict *= q\n\n        # mape_error = np.abs(df_test_org.values.reshape(-1,) - testing_predict.reshape(-1,)) / df_test_org.values.reshape(-1,)\n        #wape_error = WAPE_fun(df_test_org.values, testing_predict, wq)\n\n        # testing_predict[testing_predict < 0] = 0\n\n        # mape_mean = np.mean(mape_error)\n        # print('shape of pred ',testing_predict.shape)\n        # print('shape of True ',df_test_org.shape)\n        Wsmape, Wwape,Wpe = estimate(df_test_org.values, testing_predict, wq)\n        smape_dis,wape_dis,pe_dis=metric_dist(df_test_org.values, testing_predict, wq,metric='smape'),metric_dist(df_test_org.values, testing_predict, wq,metric='wape') ,metric_dist(df_test_org.values, testing_predict, wq,metric='pe')\n        #smape_mean = weightedSMAPE(df_test_org.values, testing_predict, wq)\n        #mape_mean = weightedMAPE(df_test_org.values, testing_predict, wq)\n\n        # testing_predict = testing_predict - average[np.argsort(wq)[::-1][:no_ts]]\n        # testing_predict[testing_predict < 0] = 0\n        return Wsmape, Wwape,Wpe,smape_dis,wape_dis,pe_dis,clus_smape,clus_wape,clus_pe, prediction  # ,testing_predict\n\n    def training_infer(self, df_train, df_test, df_test_org,q, wq, forecast_period=30,lstm_pred=None):\n        '''\n        df_test: pandas (shape:(:,1))\n        df_test_org: pandas (shape(:,no_ts))\n        '''\n\n        # self.df_train_log,self.df_test_log=test_train_spl(self.df,ratio=.8)\n        ###\n        fh = np.arange(forecast_period) + 1\n        ###\n        df_train['daily_supply'] = self.preprocessing(df_train.values)\n        self.df_train = df_train\n        # print(df_train['daily_supply'])\n        df_store = df_test.to_frame()\n        df_store = df_store.rename(columns={0: 'daily_supply'})\n        ###################################################\n        #df_store['lstm']=lstm_pred\n        #print('naive')\n        # naive\n        start = time.time()\n        forecaster=NaiveForecaster(strategy=\"last\")\n        forecaster.fit(self.df_train)\n        y_pred = forecaster.predict(fh)\n        df_store['naive'] = y_pred.values\n        # print(df_store['snaive'])\n        # print(df_store)\n        Wsmape, Wwape,Wpe,smape_dis,wape_dis,pe_dis,clus_smape,clus_wape,clus_pe,df_store['naive'] = self.postprocess(df_store['naive'].values, df_test_org,df_test,q, wq)\n        \n        end = time.time()\n        self.clus_smape['naive']=clus_smape\n        self.clus_wape['naive']=clus_wape\n        self.clus_pe['naive']=clus_pe\n        #self.stores_mape['snaive'] = mape\n        self.stores_smape['naive'] = Wsmape\n        self.store_wape['naive'] = Wwape\n        self.store_pe['naive']=Wpe\n        self.wape_dis['naive']=wape_dis\n        self.pe_dis['naive']=pe_dis\n        self.smape_dis['naive']=smape_dis\n        \n        \n        self.stores_times['naive'] = end - start\n        self.models['naive'] = forecaster\n        \n\n        #print('snaive')\n        # snaive\n        start = time.time()\n#         y = self.pysnaive(self.df_train, 52, forecast_period)[1].iloc[:forecast_period]\n#         df_store['snaive'] = y.values\n        forecaster1=NaiveForecaster(strategy=\"last\",sp=12)\n        forecaster1.fit(self.df_train)\n        y_pred = forecaster1.predict(fh)\n        Wsmape, Wwape,Wpe,smape_dis,wape_dis,pe_dis,clus_smape,clus_wape,clus_pe,df_store['snaive'] = self.postprocess(y_pred.values, df_test_org,df_test,q, wq)\n        \n        end = time.time()\n        self.clus_smape['snaive']=clus_smape\n        self.clus_wape['snaive']=clus_wape\n        self.clus_pe['snaive']=clus_pe\n        self.stores_smape['snaive'] = Wsmape\n        self.store_wape['snaive'] = Wwape\n        self.store_pe['snaive']=Wpe\n        self.wape_dis['snaive']=wape_dis\n        self.pe_dis['snaive']=pe_dis\n        self.smape_dis['snaive']=smape_dis\n        \n        self.stores_times['snaive'] = end - start\n        self.models['snaive'] = forecaster1\n        \n        #print('arima')\n        start = time.time()\n        try: \n            model_fit = ARIMA(self.df_train, order=(1, 1, 1)).fit()\n            df_store['arima'], _, _ = model_fit.forecast(forecast_period)  # 95% conf\n            print('done')\n        except (np.linalg.linalg.LinAlgError,ValueError):\n            model_fit = self.arimamodel(self.df_train)\n            df_store['arima']=model_fit.predict(forecast_period)\n            print('done3')\n#         else:\n#             model_fit = ARIMA(self.df_train, order=(0, 0, 0)).fit()\n#             df_store['arima']=model_fit.predict(forecast_period)\n#             print('done3')\n            \n        \n        #df_store['arima']= model_fit.forecast(forecast_period).values  # 95% conf\n        Wsmape, Wwape,Wpe,smape_dis,wape_dis,pe_dis,clus_smape,clus_wape,clus_pe,df_store['arima'] = self.postprocess(df_store['arima'].values, df_test_org,df_test,q, wq)\n        \n        end = time.time()\n\n        self.models['arima'] = model_fit\n        # self.stores_mape['arima'] = mape\n        self.clus_smape['arima']=clus_smape\n        self.clus_wape['arima']=clus_wape\n        self.clus_pe['arima']=clus_pe\n        self.stores_smape['arima'] = Wsmape\n        self.store_wape['arima'] = Wwape\n        self.store_pe['arima']=Wpe\n        self.wape_dis['arima']=wape_dis\n        self.pe_dis['arima']=pe_dis\n        self.smape_dis['arima']=smape_dis\n        \n        self.stores_times['arima'] = end - start\n    \n\n        #print('ses')\n        # ses\n        start = time.time()\n        fit3 = SimpleExpSmoothing(self.df_train, initialization_method=\"estimated\").fit()\n        df_store['ses'] = fit3.forecast(forecast_period).values\n        Wsmape, Wwape,Wpe,smape_dis,wape_dis,pe_dis,clus_smape,clus_wape,clus_pe,df_store['ses'] = self.postprocess(df_store['ses'].values, df_test_org,df_test,q, wq)\n        \n        end = time.time()\n        self.models['ses'] = fit3\n\n        #self.stores_mape['ses'] = mape\n        self.clus_smape['ses']=clus_smape\n        self.clus_wape['ses']=clus_wape\n        self.clus_pe['ses']=clus_pe\n        self.stores_smape['ses'] = Wsmape\n        self.store_wape['ses'] = Wwape\n        self.store_pe['ses']=Wpe\n        self.wape_dis['ses']=wape_dis\n        self.pe_dis['ses']=pe_dis\n        self.smape_dis['ses']=smape_dis\n        self.stores_times['ses'] = end - start\n        #print('holts')\n        # holts\n        start = time.time()\n        try:\n            fit3 = Holt(self.df_train, damped_trend=True, exponential=True, initialization_method=\"estimated\").fit(\n                smoothing_level=0.8, smoothing_trend=0.1)\n        except (np.linalg.linalg.LinAlgError,ValueError):\n            fit3 = Holt(self.df_train, damped_trend=True, exponential=False, initialization_method=\"estimated\").fit(\n                smoothing_level=0.8, smoothing_trend=0.1)\n        df_store['holts'] = fit3.forecast(forecast_period).values\n        Wsmape, Wwape,Wpe,smape_dis,wape_dis,pe_dis,clus_smape,clus_wape,clus_pe,df_store['holts'] = self.postprocess(df_store['holts'].values, df_test_org,df_test,q, wq)\n        \n        end = time.time()\n\n        self.models['holts'] = fit3\n        #self.stores_mape['holts'] = mape\n        self.clus_smape['holts']=clus_smape\n        self.clus_wape['holts']=clus_wape\n        self.clus_pe['holts']=clus_pe\n        self.stores_smape['holts'] = Wsmape\n        self.store_wape['holts'] = Wwape\n        self.store_pe['holts']=Wpe\n        self.wape_dis['holts']=wape_dis\n        self.pe_dis['holts']=pe_dis\n        self.smape_dis['holts']=smape_dis\n        self.stores_times['holts'] = end - start\n        #print('Exp_smoothing')\n        # Exp_smoothing\n        start = time.time()\n        try:\n            fit4 = ExponentialSmoothing(self.df_train, seasonal_periods=52, trend=\"add\", seasonal=\"add\",\n                                        damped_trend=True, use_boxcox=True, initialization_method=\"estimated\", ).fit()\n        except:\n            fit4 = ExponentialSmoothing(self.df_train, seasonal_periods=52, trend=\"add\", seasonal=\"add\",\n                                        damped_trend=True, use_boxcox=False, initialization_method=\"estimated\", ).fit()\n        df_store['Es'] = fit4.forecast(forecast_period).values\n        Wsmape, Wwape,Wpe,smape_dis,wape_dis,pe_dis,clus_smape,clus_wape,clus_pe,df_store['Es'] = self.postprocess(df_store['Es'].values, df_test_org,df_test,q, wq)\n        \n        end = time.time()\n        self.models['Es'] = fit4\n        #self.stores_mape['Es'] = mape\n        self.clus_smape['Es']=clus_smape\n        self.clus_wape['Es']=clus_wape\n        self.clus_pe['Es']=clus_pe\n        self.stores_smape['Es'] = Wsmape\n        self.store_wape['Es'] = Wwape\n        self.store_pe['Es']=Wpe\n        self.wape_dis['Es']=wape_dis\n        self.pe_dis['Es']=pe_dis\n        self.smape_dis['Es']=smape_dis\n        self.stores_times['Es'] = end - start\n\n        \n        #print('Auto_ETS')\n        # Auto_ETS\n        start = time.time()\n        try: \n            forecaster = AutoETS(auto=True, sp=12, n_jobs=-1, error='add', trend='True', damped_trend='True', seasonal='add')\n            forecaster.fit(self.df_train)\n        except (ValueError):\n            forecaster = AutoETS(auto=False, sp=7, n_jobs=-1, error='add')\n            forecaster.fit(self.df_train)\n        df_store['auto_ets'] = forecaster.predict(fh).values\n        Wsmape, Wwape,Wpe,smape_dis,wape_dis,pe_dis,clus_smape,clus_wape,clus_pe,df_store['auto_ets'] = self.postprocess(df_store['auto_ets'].values, df_test_org,df_test,q, wq)\n        \n        end = time.time()\n        self.models['auto_ets'] = forecaster\n        #self.stores_mape['auto_ets'] = mape\n        self.clus_smape['auto_ets']=clus_smape\n        self.clus_wape['auto_ets']=clus_wape\n        self.clus_pe['auto_ets']=clus_pe\n        self.stores_smape['auto_ets'] = Wsmape\n        self.store_wape['auto_ets'] = Wwape\n        self.store_pe['auto_ets']=Wpe\n        self.wape_dis['auto_ets']=wape_dis\n        self.pe_dis['auto_ets']=pe_dis\n        self.smape_dis['auto_ets']=smape_dis\n        self.stores_times['auto_ets'] = end - start\n        \n        #theta\n        start = time.time()\n        forecaster4 = ThetaForecaster(sp=52, deseasonalize=False)\n        forecaster4.fit(self.df_train)\n        df_store['theta'] = forecaster4.predict(fh).values\n        Wsmape, Wwape,Wpe,smape_dis,wape_dis,pe_dis,clus_smape,clus_wape,clus_pe,df_store['theta'] = self.postprocess(df_store['theta'].values, df_test_org,df_test,q, wq)\n        \n        end = time.time()\n\n        self.models['theta'] = forecaster4\n        #self.stores_mape['theta'] = mape\n        self.clus_smape['theta']=clus_smape\n        self.clus_wape['theta']=clus_wape\n        self.clus_pe['theta']=clus_pe\n        self.stores_smape['theta'] = Wsmape\n        self.store_wape['theta'] = Wwape\n        self.store_pe['theta']=Wpe\n        self.wape_dis['theta']=wape_dis\n        self.pe_dis['theta']=pe_dis\n        self.smape_dis['theta']=smape_dis\n        self.stores_times['theta'] = end - start\n\n        #print('Croston')\n        # Croston\n        start = time.time()\n        forecaster5 = Croston(smoothing=.2)\n        forecaster5.fit(self.df_train)\n        df_store['croston'] = forecaster5.predict(fh).values\n        Wsmape, Wwape,Wpe,smape_dis,wape_dis,pe_dis,clus_smape,clus_wape,clus_pe,df_store['croston'] = self.postprocess(df_store['croston'].values, df_test_org,df_test,q, wq)\n        \n        end = time.time()\n\n        self.models['croston'] = forecaster5\n        #self.stores_mape['croston'] = mape\n        self.clus_smape['croston']=clus_smape\n        self.clus_wape['croston']=clus_wape\n        self.clus_pe['croston']=clus_pe\n        self.stores_smape['croston'] = Wsmape\n        self.store_wape['croston'] = Wwape\n        self.store_pe['croston']=Wpe\n        self.wape_dis['croston']=wape_dis\n        self.pe_dis['croston']=pe_dis\n        self.smape_dis['croston']=smape_dis\n        self.stores_times['croston'] = end - start\n\n        #prophet\n        start = time. time()\n        df_train=self.df_train.squeeze()\n        df_train.index.name='Period'\n        df_train.index=df_train.index.to_timestamp()\n        forecaster6 = Prophet(seasonality_mode='additive',n_changepoints=int(len(df_train) / 12),\n            add_country_holidays={'country_name': 'US'},yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=True)\n        forecaster6.fit(df_train)\n        df_store['prophet'] = forecaster6.predict(fh).values\n        Wsmape, Wwape,Wpe,smape_dis,wape_dis,pe_dis,clus_smape,clus_wape,clus_pe,df_store['prophet'] = self.postprocess(df_store['prophet'].values, df_test_org,df_test,q, wq)\n        \n        end = time.time()\n\n        self.models['prophet'] = forecaster6\n        # self.stores_mape['croston'] = mape\n        self.clus_smape['prophet']=clus_smape\n        self.clus_wape['prophet']=clus_wape\n        self.clus_pe['prophet']=clus_pe\n        self.stores_smape['prophet'] = Wsmape\n        self.store_wape['prophet'] = Wwape\n        self.store_pe['prophet']=Wpe\n        self.wape_dis['prophet']=wape_dis\n        self.pe_dis['prophet']=pe_dis\n        self.smape_dis['prophet']=smape_dis\n        self.stores_times['prophet'] = end - start\n\n        #print('Average')\n        # Average of stats models\n        tog_time = np.sum(list(self.stores_times.values()))\n        start = time.time()\n        df_temp = df_store.drop(['daily_supply'], axis=1)  # df_store.loc[:, df_store.columns != 'daily_supply']\n        df_store['Avg_ensemble'] = df_temp.mean(axis=1).values\n\n        Wsmape, Wwape,Wpe,smape_dis,wape_dis,pe_dis,clus_smape,clus_wape,clus_pe,df_store['Avg_ensemble'] = self.postprocess(df_store['Avg_ensemble'].values, df_test_org,df_test,q, wq,ensemble=True)\n        \n        end = time.time()\n        #self.stores_mape['Avg_ensemble'] = mape\n        self.clus_smape['Avg_ensemble']=clus_smape\n        self.clus_wape['Avg_ensemble']=clus_wape\n        self.clus_pe['Avg_ensemble']=clus_pe\n        self.stores_smape['Avg_ensemble'] = Wsmape\n        self.store_wape['Avg_ensemble'] = Wwape\n        self.store_pe['Avg_ensemble']=Wpe\n        self.wape_dis['Avg_ensemble']=wape_dis\n        self.pe_dis['Avg_ensemble']=pe_dis\n        self.smape_dis['Avg_ensemble']=smape_dis\n        self.stores_times['Avg_ensemble'] = (end - start) + tog_time-self.stores_times['Lstm']#%%%%%%%%%%%%\n        self.dfStore=df_store\n\n    def weighted_avg(self,df_train, df_test,df_val_org, df_test_org,q, wq, forecast_period=60,lstm_pred=None):\n        \n        fh = np.arange(forecast_period) + 1\n        ###\n        df_train['daily_supply'] = self.preprocessing(df_train.values)\n        self.df_trainW = df_train\n        # print(df_train['daily_supply'])\n        df_store = df_test.to_frame()\n        df_store = df_store.rename(columns={0: 'daily_supply'})\n        #############################################\n        #df_store['lstm']=lstm_pred\n        #print('naive')\n        # naive\n        start = time.time()\n        forecaster=NaiveForecaster(strategy=\"last\")\n        forecaster.fit(self.df_trainW)\n        y_pred = forecaster.predict(fh)\n        #df_store['naive'] = y_pred.values\n        # print(df_store['snaive'])\n        # print(df_store)\n        wape,df_store['naive']= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq,Weighted=True)\n        #_,_,_,_,_,_,_,_,_,df_store['naive'] = self.postprocess(y_pred.values[30:], df_test_org,df_test,q, wq)\n        self.wape_Weighted['naive'] = wape\n\n        \n\n        #print('snaive')\n        # snaive\n       # start = time.time()\n#         y = self.pysnaive(self.df_train, 52, forecast_period)[1].iloc[:forecast_period]\n#         df_store['snaive'] = y.values\n        forecaster1=NaiveForecaster(strategy=\"last\",sp=12)\n        forecaster1.fit(self.df_trainW)\n        y_pred = forecaster1.predict(fh)\n        \n        wape,df_store['snaive']= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq,Weighted=True)\n        #_,_,_,_,_,_,_,_,_,df_store['snaive'] = self.postprocess(y_pred.values[30:], df_test_org,df_test,q, wq)\n        self.wape_Weighted['snaive'] = wape\n        \n        #print('arima')\n        #start = time.time()\n        try: \n            model_fit = ARIMA(self.df_trainW, order=(1, 1, 1)).fit()\n            y_pred, _, _ = model_fit.forecast(forecast_period)  # 95% conf\n        except (np.linalg.linalg.LinAlgError,ValueError):\n            model_fit = self.arimamodel(self.df_trainW)\n            y_pred=model_fit.predict(forecast_period)\n            \n        #y_pred, _, _ = model_fit.forecast(forecast_period)  # 95% conf\n        #df_store['arima']= model_fit.forecast(forecast_period).values  # 95% conf\n        \n        wape,df_store['arima']= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq,Weighted=True)\n        #_,_,_,_,_,_,_,_,_,df_store['arima'] = self.postprocess(y_pred[30:], df_test_org,df_test,q, wq)\n        self.wape_Weighted['arima'] = wape\n    \n\n        #print('ses')\n        # ses\n       # start = time.time()\n        fit3 = SimpleExpSmoothing(self.df_trainW, initialization_method=\"estimated\").fit()\n        y_pred = fit3.forecast(forecast_period).values\n        \n        wape,df_store['ses']= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq,Weighted=True)\n        #_,_,_,_,_,_,_,_,_,df_store['ses'] = self.postprocess(y_pred[30:], df_test_org,df_test,q, wq)\n        self.wape_Weighted['ses'] = wape\n        \n        #print('holts')\n        # holts\n        start = time.time()\n        try:\n            fit3 = Holt(self.df_trainW, damped_trend=True, exponential=True, initialization_method=\"estimated\").fit(\n                smoothing_level=0.8, smoothing_trend=0.1)\n        except (np.linalg.linalg.LinAlgError,ValueError):\n            fit3 = Holt(self.df_trainW, damped_trend=True, exponential=False, initialization_method=\"estimated\").fit(\n                smoothing_level=0.8, smoothing_trend=0.1)\n        y_pred = fit3.forecast(forecast_period).values\n        \n        wape,df_store['holts']= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq,Weighted=True)\n        #_,_,_,_,_,_,_,_,_,df_store['holts'] = self.postprocess(y_pred[30:], df_test_org,df_test,q, wq)\n        self.wape_Weighted['holts'] = wape\n        # Exp_smoothing\n        #start = time.time()\n        try:\n            fit4 = ExponentialSmoothing(self.df_trainW, seasonal_periods=52, trend=\"add\", seasonal=\"add\",\n                                        damped_trend=True, use_boxcox=True, initialization_method=\"estimated\", ).fit()\n        except:\n            fit4 = ExponentialSmoothing(self.df_trainW, seasonal_periods=52, trend=\"add\", seasonal=\"add\",\n                                        damped_trend=True, use_boxcox=False, initialization_method=\"estimated\", ).fit()\n        y_pred = fit4.forecast(forecast_period).values\n        \n        wape,df_store['Es']= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq,Weighted=True)\n        #_,_,_,_,_,_,_,_,_,df_store['Es'] = self.postprocess(y_pred[30:], df_test_org,df_test,q, wq)\n        self.wape_Weighted['Es'] = wape\n\n        \n        #print('Auto_ETS')\n        # Auto_ETS\n        #start = time.time()\n        try:\n            forecaster = AutoETS(auto=True, sp=12, n_jobs=-1, error='add', trend='True', damped_trend='True', seasonal='add')\n            forecaster.fit(self.df_trainW)\n        except (ValueError):\n            forecaster = AutoETS(auto=False, sp=7, n_jobs=-1, error='add')\n            forecaster.fit(self.df_trainW)\n        y_pred = forecaster.predict(fh).values\n        \n        wape,df_store['auto_ets']= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq,Weighted=True)\n        #_,_,_,_,_,_,_,_,_,df_store['auto_ets'] = self.postprocess(y_pred[30:], df_test_org,df_test,q, wq)\n        self.wape_Weighted['auto_ets'] = wape\n        \n        #theta\n        #start = time.time()\n        forecaster4 = ThetaForecaster(sp=52, deseasonalize=False)\n        forecaster4.fit(self.df_trainW)\n        y_pred = forecaster4.predict(fh).values\n        \n        wape,df_store['theta']= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq,Weighted=True)\n        #_,_,_,_,_,_,_,_,_,df_store['theta'] = self.postprocess(y_pred[30:], df_test_org,df_test,q, wq)\n        self.wape_Weighted['theta'] = wape\n\n        #print('Croston')\n        # Croston\n        #start = time.time()\n        forecaster5 = Croston(smoothing=.2)\n        forecaster5.fit(self.df_trainW)\n        y_pred = forecaster5.predict(fh).values\n        \n        wape,df_store['croston']= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq,Weighted=True)\n        #_,_,_,_,_,_,_,_,_,df_store['croston'] = self.postprocess(y_pred[30:], df_test_org,df_test,q, wq)\n        self.wape_Weighted['croston'] = wape\n\n        #prophet\n        #start = time. time()\n        df_train=self.df_trainW.squeeze()\n        df_train.index.name='Period'\n        df_train.index=df_train.index.to_timestamp()\n        forecaster6 = Prophet(seasonality_mode='additive',n_changepoints=int(len(df_train) / 12),\n            add_country_holidays={'country_name': 'US'},yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=True)\n        forecaster6.fit(df_train)\n        y_pred = forecaster6.predict(fh).values\n        \n        wape,df_store['prophet']= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq,Weighted=True)\n        #_,_,_,_,_,_,_,_,_,df_store['prophet'] = self.postprocess(y_pred[30:], df_test_org,df_test,q, wq)\n        self.wape_Weighted['prophet'] = wape\n        \n#         store_error = self.wape_Weighted.copy()\n\n#         for k, v in self.wape_Weighted.items():\n#             store_error[k] = 1 / v\n#         norms = list(store_error.values())\n#         norm = 0\n#         for i in norms: norm += i\n#         for k, v in store_error.items():\n#             store_error[k] = v / norm\n#         self.store_error = store_error\n        def calculate_bic(x,y, num_params=1):\n            n=x.shape[0]\n            mse=mean_squared_error(x,y)\n            bic = n * np.log(mse) + num_params * np.log(n)\n            return bic\n        X = self.dfStore.drop(['daily_supply', 'Avg_ensemble'], axis=1).values  # .columns\n        Y=self.dfStore['daily_supply'].values\n        store_error=[]\n        for i in range(X.shape[1]):\n            bic=calculate_bic(X[:,i],Y)\n            store_error.append(1/bic)\n        #store_error=[(num-min(store_error))/(max(store_error)-min(store_error)) for num in store_error]\n        store_error=[bic/np.sum(store_error) for bic in store_error]\n        print(store_error)\n        del df_store['daily_supply']\n        df_store['Weighted_ensemble'] = (df_store * store_error).sum(1).values\n        Wsmape, Wwape,Wpe,smape_dis,wape_dis,pe_dis,clus_smape,clus_wape,clus_pe,df_store['Weighted_ensemble'] = self.postprocess(df_store['Weighted_ensemble'].values, df_test_org,df_test,q, wq,ensemble=True)\n        \n        end = time.time()\n        self.clus_smape['Weighted_ensemble']=clus_smape\n        self.clus_wape['Weighted_ensemble']=clus_wape\n        self.clus_pe['Weighted_ensemble']=clus_pe\n        self.stores_smape['Weighted_ensemble'] = Wsmape\n        self.store_wape['Weighted_ensemble'] = Wwape\n        self.store_pe['Weighted_ensemble']=Wpe\n        self.wape_dis['Weighted_ensemble']=wape_dis\n        self.pe_dis['Weighted_ensemble']=pe_dis\n        self.smape_dis['Weighted_ensemble']=smape_dis\n        self.stores_times['Weighted_ensemble'] = (end - start)#+self.stores_times['Lstm']\n        self.dfStore['Weighted_ensemble']=df_store['Weighted_ensemble'].values\n    \n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "Times=[]\nSMAPEs=[]\nWAPEs=[]\nPEs=[]\nAggSmape=[]\nAggWape=[]\nAggPe=[]\nSMAPEs_dist=[]\nWAPEs_dist=[]\nPEs_dist=[]"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "cluster id:  1\ngroup id /3944/1060825/447913\nTotal TS before filter:  4861\nTotal TS after filter:  751\nTotal TS before filter:  4861\nTotal TS after filter:  751\nnumber of smape:  751\nnumber of smape:  751\ndone\nnumber of smape:  751\nnumber of smape:  751\nnumber of smape:  751\nnumber of smape:  751\nnumber of smape:  751\nnumber of smape:  751\nnumber of smape:  751\nnumber of smape:  751\nnumber of smape:  751\n[0.10157214387450694, 0.09876512352933946, 0.1000050570914408, 0.10061603539401362, 0.10219076936246652, 0.0975379304159187, 0.100615928477537, 0.10089095529349018, 0.10028546219654116, 0.09752059436474558]\nnumber of smape:  751\nWape is:  {'Lstm': 0.2961820364407199, 'naive': 0.32696840503648883, 'snaive': 0.3902422503445538, 'arima': 0.3668547938780164, 'ses': 0.3512367923222345, 'holts': 0.3124132014103609, 'Es': 0.40226756341999087, 'auto_ets': 0.35123949968795737, 'theta': 0.3447269325826967, 'croston': 0.36002038984633833, 'prophet': 0.38883643888786495, 'Avg_ensemble': 0.3133454610672217, 'Weighted_ensemble': 0.4109607210448074}\nSmape is:  {'Lstm': 0.2975160837302321, 'naive': 0.3219149606626336, 'snaive': 0.36483981114279584, 'arima': 0.35079922182394213, 'ses': 0.3395967654456753, 'holts': 0.3108138278391591, 'Es': 0.36394822380743197, 'auto_ets': 0.3395986991965783, 'theta': 0.33481943054049573, 'croston': 0.3458750659866462, 'prophet': 0.47055556286890193, 'Avg_ensemble': 0.3109069223597171, 'Weighted_ensemble': 0.3813676041620934}\nTime is: {'Lstm': -191.39099884033203, 'naive': 0.3917984962463379, 'snaive': 0.36430859565734863, 'arima': 0.529914140701294, 'ses': 0.36749267578125, 'holts': 0.3806638717651367, 'Es': 0.5120139122009277, 'auto_ets': 2.2318477630615234, 'theta': 0.3768634796142578, 'croston': 0.35177159309387207, 'prophet': 2.0086655616760254, 'Avg_ensemble': 7.921887636184692, 'Weighted_ensemble': 1.648076057434082}\n===============================================\ncluster id:  2\ngroup id /976759/976787/1001392/9358664\nTotal TS before filter:  3667\nTotal TS after filter:  262\nTotal TS before filter:  3667\nTotal TS after filter:  262\nnumber of smape:  262\nnumber of smape:  262\ndone\nnumber of smape:  262\nnumber of smape:  262\nnumber of smape:  262\nnumber of smape:  262\nnumber of smape:  262\nnumber of smape:  262\nnumber of smape:  262\nnumber of smape:  262\nnumber of smape:  262\n[0.10056977993114252, 0.09860244436892233, 0.10079256166092622, 0.10056977990340928, 0.10093413683496263, 0.1001337608793266, 0.10056959381294352, 0.10053074166195303, 0.09886876089917321, 0.09842844004724066]\nnumber of smape:  262\nWape is:  {'Lstm': 0.6901325930334411, 'naive': 0.5550908723460382, 'snaive': 0.6573032645755887, 'arima': 0.5461008063786131, 'ses': 0.5550908734974966, 'holts': 0.5394949845766017, 'Es': 0.5754374000871466, 'auto_ets': 0.5550985998411693, 'theta': 0.5566999160279325, 'croston': 0.6415107337546168, 'prophet': 0.6745378909145776, 'Avg_ensemble': 0.5815694486476641, 'Weighted_ensemble': 0.6847664017611225}\nSmape is:  {'Lstm': 0.9517995760998587, 'naive': 0.6833409305930535, 'snaive': 0.8926199358107768, 'arima': 0.6668063538246948, 'ses': 0.6833409327922187, 'holts': 0.6562542020380467, 'Es': 0.7215085412202802, 'auto_ets': 0.6833556894018337, 'theta': 0.6861115172055688, 'croston': 0.8569603549825914, 'prophet': 0.9334178155670617, 'Avg_ensemble': 0.7348603843484033, 'Weighted_ensemble': 0.9545169308837301}\nTime is: {'Lstm': -186.80035281181335, 'naive': 0.14705276489257812, 'snaive': 0.1714036464691162, 'arima': 0.21784353256225586, 'ses': 0.1311476230621338, 'holts': 0.13692712783813477, 'Es': 0.3624858856201172, 'auto_ets': 0.2799873352050781, 'theta': 0.13741254806518555, 'croston': 0.12813901901245117, 'prophet': 0.32740187644958496, 'Avg_ensemble': 2.167141914367676, 'Weighted_ensemble': 0.7635712623596191}\n===============================================\ncluster id:  3\ngroup id /976759/976787/1001391/2392263\nTotal TS before filter:  5776\nTotal TS after filter:  497\nTotal TS before filter:  5776\nTotal TS after filter:  497\nnumber of smape:  497\nnumber of smape:  497\ndone\nnumber of smape:  497\nnumber of smape:  497\nnumber of smape:  497\nnumber of smape:  497\nnumber of smape:  497\nnumber of smape:  497\nnumber of smape:  497\nnumber of smape:  497\nnumber of smape:  497\n[0.1005053159391981, 0.09918780099317788, 0.10067219127462651, 0.10050531591656878, 0.10068271296043278, 0.09927240569163572, 0.10050516407307776, 0.10048859700799218, 0.09933507974657464, 0.09884541639671561]\nnumber of smape:  497\nWape is:  {'Lstm': 0.5520904100376333, 'naive': 0.4731118852921152, 'snaive': 0.5051042116833387, 'arima': 0.4699616782992698, 'ses': 0.47311188570002566, 'holts': 0.469156901685236, 'Es': 0.5061394348365813, 'auto_ets': 0.4731146228023241, 'theta': 0.4734358333553104, 'croston': 0.5000994300239648, 'prophet': 0.5181705207374729, 'Avg_ensemble': 0.4838273429432588, 'Weighted_ensemble': 0.5031904411607973}\nSmape is:  {'Lstm': 0.692831808176359, 'naive': 0.5509458950775088, 'snaive': 0.6114678396382038, 'arima': 0.5452680483923446, 'ses': 0.5509458958930336, 'holts': 0.5440172566460505, 'Es': 0.6139094264158136, 'auto_ets': 0.5509513681344543, 'theta': 0.5515252925518168, 'croston': 0.603306238714688, 'prophet': 0.6396674192509068, 'Avg_ensemble': 0.5720820339425474, 'Weighted_ensemble': 0.6098543131769195}\nTime is: {'Lstm': -186.55605792999268, 'naive': 0.24319005012512207, 'snaive': 0.24346327781677246, 'arima': 0.344851016998291, 'ses': 0.23856639862060547, 'holts': 0.24576020240783691, 'Es': 0.5635747909545898, 'auto_ets': 0.391862154006958, 'theta': 0.2752797603607178, 'croston': 0.2677018642425537, 'prophet': 0.4909214973449707, 'Avg_ensemble': 3.5445001125335693, 'Weighted_ensemble': 0.9115164279937744}\n===============================================\ncluster id:  4\ngroup id /1115193/1071966/1072133\nTotal TS before filter:  3726\nTotal TS after filter:  414\nTotal TS before filter:  3726\nTotal TS after filter:  414\nnumber of smape:  414\nnumber of smape:  414\ndone\nnumber of smape:  414\nnumber of smape:  414\nnumber of smape:  414\nnumber of smape:  414\nnumber of smape:  414\nnumber of smape:  414\nnumber of smape:  414\nnumber of smape:  414\nnumber of smape:  414\n[0.10574818408615654, 0.10132499700897298, 0.09365073186708968, 0.1036587318564742, 0.10590054619601985, 0.10242978626547758, 0.10365845329368913, 0.0977688439535447, 0.10260610919114642, 0.08325361628142883]\nnumber of smape:  414\nWape is:  {'Lstm': 0.36992699413210983, 'naive': 0.36670072823640576, 'snaive': 0.35823471062591083, 'arima': 0.3918632395947326, 'ses': 0.358347640838084, 'holts': 0.3657552226015769, 'Es': 0.35967521286787274, 'auto_ets': 0.3583468599886056, 'theta': 0.36029533334921954, 'croston': 0.355857577598105, 'prophet': 0.9293037738099961, 'Avg_ensemble': 0.3612796810141755, 'Weighted_ensemble': 0.38226408860170547}\nSmape is:  {'Lstm': 0.4124451768476899, 'naive': 0.38372154947963294, 'snaive': 0.39144532650851316, 'arima': 0.4697550757574243, 'ses': 0.38537268080616854, 'holts': 0.38294235718108033, 'Es': 0.38538775652656515, 'auto_ets': 0.3853730688608536, 'theta': 0.4091395424106076, 'croston': 0.38726121780250056, 'prophet': 1.4442128444889781, 'Avg_ensemble': 0.4144074481451193, 'Weighted_ensemble': 0.3865838635540142}\nTime is: {'Lstm': -181.9619607925415, 'naive': 0.22348499298095703, 'snaive': 0.22905349731445312, 'arima': 0.3215827941894531, 'ses': 0.219893217086792, 'holts': 0.21225976943969727, 'Es': 0.3651700019836426, 'auto_ets': 0.3376331329345703, 'theta': 0.22661828994750977, 'croston': 0.19919419288635254, 'prophet': 0.4141244888305664, 'Avg_ensemble': 2.956850051879883, 'Weighted_ensemble': 0.7612643241882324}\n===============================================\ncluster id:  5\ngroup id /2636/3475115/2762884\nTotal TS before filter:  132\nTotal TS after filter:  11\nTotal TS before filter:  132\nTotal TS after filter:  11\nnumber of smape:  11\nnumber of smape:  11\ndone\nnumber of smape:  11\nnumber of smape:  11\nnumber of smape:  11\nnumber of smape:  11\nnumber of smape:  11\nnumber of smape:  11\nnumber of smape:  11\nnumber of smape:  11\nnumber of smape:  11\n[0.09994132882652489, 0.09995731758493058, 0.10003006868521915, 0.1000192644227961, 0.09991811635755427, 0.09990631499952356, 0.10001926375490869, 0.0999998943886881, 0.09999267499310668, 0.10021575598674805]\nnumber of smape:  11\nWape is:  {'Lstm': 0.9470156252864107, 'naive': 0.948501346082314, 'snaive': 0.9595632304772688, 'arima': 0.9300546826409012, 'ses': 0.9319443383363505, 'holts': 0.9637148134229155, 'Es': 0.9744837337437193, 'auto_ets': 0.9319443900154594, 'theta': 0.935441268493351, 'croston': 0.9344255037652286, 'prophet': 0.9609215112543751, 'Avg_ensemble': 0.9345828889065712, 'Weighted_ensemble': 0.9612349990726232}\nSmape is:  {'Lstm': 1.2552534620218747, 'naive': 1.259702493236272, 'snaive': 1.231560352450597, 'arima': 1.141958837568995, 'ses': 1.1565432247727045, 'holts': 1.400379324660917, 'Es': 1.5848275834532988, 'auto_ets': 1.156543827696626, 'theta': 1.181686369824805, 'croston': 1.1778274560846873, 'prophet': 1.420004212935439, 'Avg_ensemble': 1.2046025163982972, 'Weighted_ensemble': 1.3755547552719012}\nTime is: {'Lstm': -186.2498950958252, 'naive': 0.019543170928955078, 'snaive': 0.019593477249145508, 'arima': 0.16876935958862305, 'ses': 0.017398357391357422, 'holts': 0.02397608757019043, 'Es': 0.20056843757629395, 'auto_ets': 0.17324304580688477, 'theta': 0.019910812377929688, 'croston': 0.013990402221679688, 'prophet': 0.23627519607543945, 'Avg_ensemble': 0.9029679298400879, 'Weighted_ensemble': 0.5535106658935547}\n===============================================\ncluster id:  6\ngroup id /976759/976791/6259087/1001411\nTotal TS before filter:  2838\nTotal TS after filter:  202\nTotal TS before filter:  2838\nTotal TS after filter:  202\nnumber of smape:  202\nnumber of smape:  202\ndone\nnumber of smape:  202\nnumber of smape:  202\nnumber of smape:  202\nnumber of smape:  202\nnumber of smape:  202\nnumber of smape:  202\nnumber of smape:  202\nnumber of smape:  202\nnumber of smape:  202\n[0.10036785200984207, 0.09952729198524361, 0.09969397183074444, 0.10035168815867233, 0.10044925226383851, 0.10004381764811324, 0.10035171385505895, 0.10034648045197625, 0.09961972171127723, 0.09924821008523343]\nnumber of smape:  202\nWape is:  {'Lstm': 0.7584016411756946, 'naive': 0.6913241551697417, 'snaive': 0.7508637783446072, 'arima': 0.7375288196978879, 'ses': 0.6923135229119415, 'holts': 0.6860028764544136, 'Es': 0.7151913598517338, 'auto_ets': 0.6923119484193786, 'theta': 0.6926238409734128, 'croston': 0.7440131810717647, 'prophet': 0.7750909011025758, 'Avg_ensemble': 0.7159576142360142, 'Weighted_ensemble': 0.7470142874810171}\nSmape is:  {'Lstm': 1.0845748873897518, 'naive': 0.9276923631711553, 'snaive': 1.06890440544939, 'arima': 1.0315528673487797, 'ses': 0.9298298404182714, 'holts': 0.9173168987891902, 'Es': 0.9823243536519779, 'auto_ets': 0.9298264356828078, 'theta': 0.9304496586208556, 'croston': 1.0504538727327775, 'prophet': 1.1297245414082004, 'Avg_ensemble': 0.9825475446023436, 'Weighted_ensemble': 1.0595927755551453}\nTime is: {'Lstm': -187.5533788204193, 'naive': 0.11139917373657227, 'snaive': 0.1071007251739502, 'arima': 0.2640645503997803, 'ses': 0.11434054374694824, 'holts': 0.10914850234985352, 'Es': 0.3247194290161133, 'auto_ets': 0.2246851921081543, 'theta': 0.10886502265930176, 'croston': 0.10187220573425293, 'prophet': 0.317216157913208, 'Avg_ensemble': 1.879988193511963, 'Weighted_ensemble': 0.745978593826294}\n===============================================\ncluster id:  7\ngroup id /3944/3951\nTotal TS before filter:  29525\nTotal TS after filter:  901\nTotal TS before filter:  29525\nTotal TS after filter:  901\nnumber of smape:  901\nnumber of smape:  901\ndone\nnumber of smape:  901\nnumber of smape:  901\nnumber of smape:  901\nnumber of smape:  901\nnumber of smape:  901\nnumber of smape:  901\nnumber of smape:  901\nnumber of smape:  901\nnumber of smape:  901\n[0.09789250699104285, 0.10019650960482561, 0.10377633035563393, 0.10000534089434321, 0.09777251901360016, 0.09532817078006757, 0.10000537194479377, 0.10237269942172345, 0.10106073496266857, 0.10158981603130096]\nnumber of smape:  901\nWape is:  {'Lstm': 0.6679775961029542, 'naive': 0.7514762042205784, 'snaive': 0.7154322691606536, 'arima': 0.6961039027401505, 'ses': 0.7242183452534716, 'holts': 0.7514304794129929, 'Es': 0.7604396823649517, 'auto_ets': 0.7242179675567161, 'theta': 0.7053295755753195, 'croston': 0.7112618224267925, 'prophet': 0.7041730288366357, 'Avg_ensemble': 0.7224471760185923, 'Weighted_ensemble': 0.7269930670699665}\nSmape is:  {'Lstm': 0.7190090594447431, 'naive': 0.7313303594859374, 'snaive': 0.7204588332103224, 'arima': 0.712566918100152, 'ses': 0.7220111955816303, 'holts': 0.7315192818638221, 'Es': 0.7387959904416118, 'auto_ets': 0.722011076240194, 'theta': 0.7155491417146044, 'croston': 0.7180146713675255, 'prophet': 0.7162728803454977, 'Avg_ensemble': 0.721599001487127, 'Weighted_ensemble': 0.7224124938635795}\nTime is: {'Lstm': -190.80856323242188, 'naive': 0.4624059200286865, 'snaive': 0.455000638961792, 'arima': 0.6015100479125977, 'ses': 0.4722921848297119, 'holts': 0.616591215133667, 'Es': 0.7412691116333008, 'auto_ets': 0.7042887210845947, 'theta': 0.45110249519348145, 'croston': 0.4724884033203125, 'prophet': 0.744783878326416, 'Avg_ensemble': 6.266076326370239, 'Weighted_ensemble': 1.0022530555725098}\n===============================================\ncluster id:  8\ngroup id /91083/1074765\nTotal TS before filter:  22494\nTotal TS after filter:  786\nTotal TS before filter:  22494\nTotal TS after filter:  786\nnumber of smape:  786\nnumber of smape:  786\ndone\nnumber of smape:  786\nnumber of smape:  786\nnumber of smape:  786\nnumber of smape:  786\nnumber of smape:  786\nnumber of smape:  786\nnumber of smape:  786\nnumber of smape:  786\nnumber of smape:  786\n[0.09908096421680836, 0.09694583645944671, 0.10162213883909503, 0.09989257822962137, 0.09967291715683517, 0.09833718280168785, 0.09989263187596864, 0.10133655048288527, 0.09891884522038033, 0.1043003547172713]\nnumber of smape:  786\nWape is:  {'Lstm': 0.9372335769021725, 'naive': 0.6945433566811018, 'snaive': 0.7048707813716208, 'arima': 0.6825411664817581, 'ses': 0.6896748745974826, 'holts': 0.6910196437674743, 'Es': 0.6847280591371396, 'auto_ets': 0.6896745572039822, 'theta': 0.6835701627579507, 'croston': 0.6955358170838769, 'prophet': 0.6729691550210454, 'Avg_ensemble': 0.6879763052279202, 'Weighted_ensemble': 0.7495124045367022}\nSmape is:  {'Lstm': 0.7339880118401066, 'naive': 0.6716249756061677, 'snaive': 0.6738598886696947, 'arima': 0.6723354229625892, 'ses': 0.6718148830986527, 'holts': 0.6717276171829969, 'Es': 0.676700903499333, 'auto_ets': 0.6718149062013165, 'theta': 0.6722365402439864, 'croston': 0.6716192030377994, 'prophet': 0.673367057680434, 'Avg_ensemble': 0.6718802851491367, 'Weighted_ensemble': 0.6794259539732768}\nTime is: {'Lstm': -187.4871847629547, 'naive': 0.43306851387023926, 'snaive': 0.3879687786102295, 'arima': 0.5209426879882812, 'ses': 0.4882235527038574, 'holts': 0.4190638065338135, 'Es': 0.5551121234893799, 'auto_ets': 0.568979024887085, 'theta': 0.385498046875, 'croston': 0.44417643547058105, 'prophet': 0.5944795608520508, 'Avg_ensemble': 5.209655046463013, 'Weighted_ensemble': 0.9673140048980713}\n===============================================\ncluster id:  9\ngroup id /4125/546956\nTotal TS before filter:  24472\nTotal TS after filter:  743\nTotal TS before filter:  24472\nTotal TS after filter:  743\nnumber of smape:  743\nnumber of smape:  743\ndone\nnumber of smape:  743\nnumber of smape:  743\nnumber of smape:  743\nnumber of smape:  743\nnumber of smape:  743\nnumber of smape:  743\nnumber of smape:  743\nnumber of smape:  743\nnumber of smape:  743\n[0.1001529575916547, 0.09919869345216634, 0.10310442213409787, 0.10029114845896063, 0.10046947102954629, 0.097491950888565, 0.1002916671635542, 0.10250208130876669, 0.10043488349284466, 0.09606272447984349]\nnumber of smape:  743\nWape is:  {'Lstm': 0.7176513797177058, 'naive': 0.6821125431649321, 'snaive': 0.6819222532366565, 'arima': 0.6539776705584309, 'ses': 0.6803723565997115, 'holts': 0.6782979541057044, 'Es': 0.7095759090592252, 'auto_ets': 0.6803658491424815, 'theta': 0.6605353020624568, 'croston': 0.6785696716288183, 'prophet': 0.725756501843113, 'Avg_ensemble': 0.6805239936543879, 'Weighted_ensemble': 0.6629753914777005}\nSmape is:  {'Lstm': 0.6723234979805491, 'naive': 0.6643912895979338, 'snaive': 0.6689588682455416, 'arima': 0.6594416571119146, 'ses': 0.6641035622304046, 'holts': 0.6637437359455958, 'Es': 0.6721465433650905, 'auto_ets': 0.6641025184472747, 'theta': 0.6600612953882421, 'croston': 0.6638228557149621, 'prophet': 0.6770331126982407, 'Avg_ensemble': 0.66404952683931, 'Weighted_ensemble': 0.6606500909681448}\nTime is: {'Lstm': -188.06559085845947, 'naive': 0.3686809539794922, 'snaive': 0.43140578269958496, 'arima': 0.49585795402526855, 'ses': 0.39566993713378906, 'holts': 0.39338254928588867, 'Es': 0.6290969848632812, 'auto_ets': 0.5425248146057129, 'theta': 0.3534982204437256, 'croston': 0.3477771282196045, 'prophet': 0.5765902996063232, 'Avg_ensemble': 4.909532308578491, 'Weighted_ensemble': 0.9541234970092773}\n===============================================\ncluster id:  10\ngroup id /976759/976794/5403011/9712965\nTotal TS before filter:  3715\nTotal TS after filter:  305\nTotal TS before filter:  3715\nTotal TS after filter:  305\nnumber of smape:  305\nnumber of smape:  305\ndone\nnumber of smape:  305\nnumber of smape:  305\nnumber of smape:  305\nnumber of smape:  305\nnumber of smape:  305\nnumber of smape:  305\nnumber of smape:  305\nnumber of smape:  305\nnumber of smape:  305\n[0.10016593344583807, 0.10004543653261637, 0.10002278718590887, 0.10014206033101349, 0.1001357710565234, 0.10016172547779768, 0.10014205598737423, 0.1001405050619446, 0.09999542114554832, 0.0990483037754349]\nnumber of smape:  305\nWape is:  {'Lstm': 0.6612106834628276, 'naive': 0.6264930601401002, 'snaive': 0.632264472771699, 'arima': 0.6330305067380981, 'ses': 0.6275163308440248, 'holts': 0.6278725942898765, 'Es': 0.6286484716943577, 'auto_ets': 0.6275165173671858, 'theta': 0.6275746863951014, 'croston': 0.634048663875547, 'prophet': 0.6790388745409534, 'Avg_ensemble': 0.6338772917638682, 'Weighted_ensemble': 0.6347362490686222}\nSmape is:  {'Lstm': 0.868232189906101, 'naive': 0.7889680797581724, 'snaive': 0.8015583819839661, 'arima': 0.8040029782190412, 'ses': 0.7912865965948412, 'holts': 0.7922321365118, 'Es': 0.7956885035873495, 'auto_ets': 0.7912870195197758, 'theta': 0.7914090540624736, 'croston': 0.8061866054677996, 'prophet': 0.9097943953159583, 'Avg_ensemble': 0.8056696245576251, 'Weighted_ensemble': 0.8079739835608348}\nTime is: {'Lstm': -189.3646523952484, 'naive': 0.1608574390411377, 'snaive': 0.16278505325317383, 'arima': 0.35518670082092285, 'ses': 0.16642165184020996, 'holts': 0.2785208225250244, 'Es': 0.3829183578491211, 'auto_ets': 0.29657816886901855, 'theta': 0.17138171195983887, 'croston': 0.16064095497131348, 'prophet': 0.36583495140075684, 'Avg_ensemble': 2.6631312370300293, 'Weighted_ensemble': 0.7780332565307617}\n===============================================\ncluster id:  11\ngroup id /976759/976783/8102529/4007132\nTotal TS before filter:  3898\nTotal TS after filter:  306\nTotal TS before filter:  3898\nTotal TS after filter:  306\nnumber of smape:  306\nnumber of smape:  306\ndone\nnumber of smape:  306\nnumber of smape:  306\nnumber of smape:  306\nnumber of smape:  306\nnumber of smape:  306\nnumber of smape:  306\nnumber of smape:  306\nnumber of smape:  306\nnumber of smape:  306\n[0.10062932945256735, 0.09893817147126474, 0.0993665274554453, 0.10062932943290323, 0.10093760589315785, 0.0999360173716949, 0.10062919747995706, 0.10059577400021123, 0.09923816065227604, 0.09909988679052226]\nnumber of smape:  306\nWape is:  {'Lstm': 0.6708704130930944, 'naive': 0.6120880702848215, 'snaive': 0.6863184642223097, 'arima': 0.6604147043742735, 'ses': 0.612088070897864, 'holts': 0.6026892007518537, 'Es': 0.6379518650647821, 'auto_ets': 0.6120921846441957, 'theta': 0.6131155767111974, 'croston': 0.6685175427974853, 'prophet': 0.6782449441156354, 'Avg_ensemble': 0.6349681835723936, 'Weighted_ensemble': 0.6781140198527207}\nSmape is:  {'Lstm': 0.9294815328351423, 'naive': 0.7889155550861774, 'snaive': 0.9741320419709986, 'arima': 0.9036485738396651, 'ses': 0.7889155566749159, 'holts': 0.7678707668222118, 'Es': 0.853705094025563, 'auto_ets': 0.7889262177373874, 'theta': 0.7911460847747981, 'croston': 0.9302643637527633, 'prophet': 0.9577109464933656, 'Avg_ensemble': 0.847164692042158, 'Weighted_ensemble': 0.9552595581888931}\nTime is: {'Lstm': -189.23898100852966, 'naive': 0.1665632724761963, 'snaive': 0.16094374656677246, 'arima': 0.2833704948425293, 'ses': 0.15686368942260742, 'holts': 0.15761661529541016, 'Es': 0.40739989280700684, 'auto_ets': 0.3561549186706543, 'theta': 0.16118550300598145, 'croston': 0.15661072731018066, 'prophet': 0.37111902236938477, 'Avg_ensemble': 2.5247955322265625, 'Weighted_ensemble': 0.699608325958252}\n===============================================\ncluster id:  12\ngroup id /4125/1081404/1230089/2768744\nTotal TS before filter:  1687\nTotal TS after filter:  215\nTotal TS before filter:  1687\nTotal TS after filter:  215\nnumber of smape:  215\nnumber of smape:  215\ndone\nnumber of smape:  215\nnumber of smape:  215\nnumber of smape:  215\nnumber of smape:  215\nnumber of smape:  215\nnumber of smape:  215\nnumber of smape:  215\nnumber of smape:  215\nnumber of smape:  215\n[0.10001928061605941, 0.09496689367555462, 0.09922989217033655, 0.09898033979162693, 0.1010640055410753, 0.10047037128731058, 0.09897780911202131, 0.0993171675210264, 0.09681955196487621, 0.11015468832011273]\nnumber of smape:  215\nWape is:  {'Lstm': 1.1278696402372455, 'naive': 0.3518949402062261, 'snaive': 0.5158658360405614, 'arima': 0.37359836580370115, 'ses': 0.3785470105719708, 'holts': 0.3267117739324275, 'Es': 0.3273816845810549, 'auto_ets': 0.3786179043432289, 'theta': 0.3698167947242181, 'croston': 0.44919408901043123, 'prophet': 0.21278956786484168, 'Avg_ensemble': 0.3439656297971131, 'Weighted_ensemble': 0.6495901039852816}\nSmape is:  {'Lstm': 0.7227296038078813, 'naive': 0.34600875095582856, 'snaive': 0.45318365353443585, 'arima': 0.3610168774601723, 'ses': 0.36519990008264663, 'holts': 0.3264705564586297, 'Es': 0.33305819909413287, 'auto_ets': 0.36525069679339206, 'theta': 0.35873728039723096, 'croston': 0.41404715919762963, 'prophet': 0.24398671093750637, 'Avg_ensemble': 0.33889360580315403, 'Weighted_ensemble': 0.5340353269539624}\nTime is: {'Lstm': -184.2036623954773, 'naive': 0.11742949485778809, 'snaive': 0.11677837371826172, 'arima': 0.24057888984680176, 'ses': 0.11939239501953125, 'holts': 0.12563729286193848, 'Es': 0.30243420600891113, 'auto_ets': 0.29167819023132324, 'theta': 0.11649227142333984, 'croston': 0.10736870765686035, 'prophet': 0.29901885986328125, 'Avg_ensemble': 1.9423902034759521, 'Weighted_ensemble': 0.6522059440612793}\n===============================================\ncluster id:  13\ngroup id /1085666/7876574/4034645\nTotal TS before filter:  5677\nTotal TS after filter:  517\nTotal TS before filter:  5677\nTotal TS after filter:  517\nnumber of smape:  517\nnumber of smape:  517\ndone\nnumber of smape:  517\nnumber of smape:  517\nnumber of smape:  517\nnumber of smape:  517\nnumber of smape:  517\nnumber of smape:  517\nnumber of smape:  517\nnumber of smape:  517\nnumber of smape:  517\n[0.10389488884900212, 0.0960053590342207, 0.09913758067831237, 0.10078429383218898, 0.10430497779955962, 0.09965181215423176, 0.10078424843657906, 0.10100082409297899, 0.09741777106858246, 0.09701824405434409]\nnumber of smape:  517\nWape is:  {'Lstm': 0.39098395394847685, 'naive': 0.39256202284303465, 'snaive': 0.39690072111804753, 'arima': 0.3846345713451822, 'ses': 0.3857315007726159, 'holts': 0.39312201687062215, 'Es': 0.3831413102194504, 'auto_ets': 0.38573144138699755, 'theta': 0.38588782039241015, 'croston': 0.385327239007184, 'prophet': 0.38572133227079936, 'Avg_ensemble': 0.38510514579050514, 'Weighted_ensemble': 0.3863467811975714}\nSmape is:  {'Lstm': 0.46484658054784456, 'naive': 0.42424935836683336, 'snaive': 0.462181167371014, 'arima': 0.439176585653866, 'ses': 0.43214195876545153, 'holts': 0.42344936810041417, 'Es': 0.43746432398827956, 'auto_ets': 0.43214211455088514, 'theta': 0.4314598845223879, 'croston': 0.4493379055347564, 'prophet': 0.4501080419005537, 'Avg_ensemble': 0.4353361814849313, 'Weighted_ensemble': 0.43089848041443946}\nTime is: {'Lstm': -182.4704315662384, 'naive': 0.2549896240234375, 'snaive': 0.2582225799560547, 'arima': 0.34011030197143555, 'ses': 0.24445581436157227, 'holts': 0.2555704116821289, 'Es': 0.4325706958770752, 'auto_ets': 0.41263628005981445, 'theta': 0.2550950050354004, 'croston': 0.24456262588500977, 'prophet': 0.45039796829223633, 'Avg_ensemble': 3.4069037437438965, 'Weighted_ensemble': 0.886620283126831}\n===============================================\ncluster id:  14\ngroup id /4171/4186/1105635/2927326\nTotal TS before filter:  4055\nTotal TS after filter:  462\nTotal TS before filter:  4055\nTotal TS after filter:  462\nnumber of smape:  462\nnumber of smape:  462\ndone\nnumber of smape:  462\nnumber of smape:  462\nnumber of smape:  462\nnumber of smape:  462\nnumber of smape:  462\nnumber of smape:  462\nnumber of smape:  462\nnumber of smape:  462\nnumber of smape:  462\n[0.10573746398034199, 0.09694790085168978, 0.10093437408558666, 0.10157110463867643, 0.1065378919331956, 0.09299218118430105, 0.10156570042681673, 0.10009232819628072, 0.09825354742848942, 0.09536750727462169]\nnumber of smape:  462\nWape is:  {'Lstm': 0.5747624257419327, 'naive': 0.6353202047364845, 'snaive': 0.5924181471343546, 'arima': 0.5853447122412891, 'ses': 0.5897760410831452, 'holts': 0.6488096519994253, 'Es': 0.6460994873405699, 'auto_ets': 0.5897389652816173, 'theta': 0.5799382626412989, 'croston': 0.5767398362556873, 'prophet': 0.5869559625727379, 'Avg_ensemble': 0.5766308483606681, 'Weighted_ensemble': 0.6984880129513703}\nSmape is:  {'Lstm': 0.7040126061976242, 'naive': 0.6876482875773147, 'snaive': 0.7451668872068619, 'arima': 0.6936867803364294, 'ses': 0.6921862804082255, 'holts': 0.6923531330672047, 'Es': 0.9079228109998411, 'auto_ets': 0.6922106551671017, 'theta': 0.6952418396270085, 'croston': 0.7210254997857408, 'prophet': 0.782522470464373, 'Avg_ensemble': 0.7007656718570404, 'Weighted_ensemble': 0.7029066691801825}\nTime is: {'Lstm': -187.75834894180298, 'naive': 0.22760009765625, 'snaive': 0.22720098495483398, 'arima': 0.30139708518981934, 'ses': 0.22735118865966797, 'holts': 0.23000764846801758, 'Es': 0.41399097442626953, 'auto_ets': 0.3848731517791748, 'theta': 0.23968720436096191, 'croston': 0.24424004554748535, 'prophet': 0.4761190414428711, 'Avg_ensemble': 3.212822675704956, 'Weighted_ensemble': 0.8572535514831543}\n===============================================\ncluster id:  15\ngroup id /976759/9176907/6053481\nTotal TS before filter:  418\nTotal TS after filter:  38\nTotal TS before filter:  418\nTotal TS after filter:  38\nnumber of smape:  38\nnumber of smape:  38\ndone\nnumber of smape:  38\nnumber of smape:  38\nnumber of smape:  38\nnumber of smape:  38\nnumber of smape:  38\nnumber of smape:  38\nnumber of smape:  38\nnumber of smape:  38\nnumber of smape:  38\n[0.10009828392751563, 0.09993991859859795, 0.09998468580349322, 0.10008691044872696, 0.1000943333323807, 0.09998045823645337, 0.10008691120169746, 0.10009407944095433, 0.0999401977471012, 0.09969422126307918]\nnumber of smape:  38\nWape is:  {'Lstm': 0.8104471572442892, 'naive': 0.7083091375384271, 'snaive': 0.7223842385255022, 'arima': 0.717690482701963, 'ses': 0.7092319416883897, 'holts': 0.7087738162199997, 'Es': 0.7212308266007643, 'auto_ets': 0.7092318795284829, 'theta': 0.7087201261141899, 'croston': 0.721835927806856, 'prophet': 0.7385181899886102, 'Avg_ensemble': 0.7161715950500382, 'Weighted_ensemble': 0.7319855416030713}\nSmape is:  {'Lstm': 1.198052611591336, 'naive': 0.9271652164821338, 'snaive': 0.9617458523090247, 'arima': 0.9489566290171688, 'ses': 0.9292861784927742, 'holts': 0.9284411554820619, 'Es': 0.9615648762292623, 'auto_ets': 0.9292860343875543, 'theta': 0.9281946782724435, 'croston': 0.9593746713765776, 'prophet': 0.9958365752344288, 'Avg_ensemble': 0.9452059924775339, 'Weighted_ensemble': 0.985647851694443}\nTime is: {'Lstm': -187.32640528678894, 'naive': 0.031236648559570312, 'snaive': 0.033753395080566406, 'arima': 0.15755200386047363, 'ses': 0.027657270431518555, 'holts': 0.032813310623168945, 'Es': 0.20853424072265625, 'auto_ets': 0.14977812767028809, 'theta': 0.03310894966125488, 'croston': 0.02674412727355957, 'prophet': 0.2727043628692627, 'Avg_ensemble': 0.9980447292327881, 'Weighted_ensemble': 0.6516149044036865}\n===============================================\ncluster id:  16\ngroup id /976759/976794/3029941/9677176\nTotal TS before filter:  5292\nTotal TS after filter:  441\nTotal TS before filter:  5292\nTotal TS after filter:  441\nnumber of smape:  441\nnumber of smape:  441\ndone\nnumber of smape:  441\nnumber of smape:  441\nnumber of smape:  441\nnumber of smape:  441\nnumber of smape:  441\nnumber of smape:  441\nnumber of smape:  441\nnumber of smape:  441\nnumber of smape:  441\n[0.10077710954739769, 0.09934193814719766, 0.09939997833479676, 0.10027456056957834, 0.10088802308802222, 0.10043955681163172, 0.1002743573591745, 0.10011830789206351, 0.0995514945875914, 0.09893467366254614]\nnumber of smape:  441\nWape is:  {'Lstm': 0.6892540102099447, 'naive': 0.630393683984935, 'snaive': 0.6958421681286104, 'arima': 0.690589429744248, 'ses': 0.6496085594750252, 'holts': 0.6268471404973405, 'Es': 0.6459977923599157, 'auto_ets': 0.6496170115361808, 'theta': 0.6557193194882788, 'croston': 0.6843616361240988, 'prophet': 0.716950093703206, 'Avg_ensemble': 0.6622180051249807, 'Weighted_ensemble': 0.690169944216245}\nSmape is:  {'Lstm': 0.96169418718733, 'naive': 0.8261165284383574, 'snaive': 0.9833803928586909, 'arima': 0.9657606100402949, 'ses': 0.8729646264402295, 'holts': 0.8200104266952296, 'Es': 0.8695079515960192, 'auto_ets': 0.8729851737727818, 'theta': 0.8856764976053655, 'croston': 0.9566031672034632, 'prophet': 1.0299477138957216, 'Avg_ensemble': 0.90269259221042, 'Weighted_ensemble': 0.9708618813659268}\nTime is: {'Lstm': -187.29383754730225, 'naive': 0.22324275970458984, 'snaive': 0.23947525024414062, 'arima': 0.37178754806518555, 'ses': 0.24699974060058594, 'holts': 0.24595046043395996, 'Es': 0.45411157608032227, 'auto_ets': 0.3670926094055176, 'theta': 0.23298072814941406, 'croston': 0.23476505279541016, 'prophet': 0.49968743324279785, 'Avg_ensemble': 3.3296663761138916, 'Weighted_ensemble': 0.8235747814178467}\n===============================================\ncluster id:  17\ngroup id /976759/976791/9551235/5459614\nTotal TS before filter:  2867\nTotal TS after filter:  199\nTotal TS before filter:  2867\nTotal TS after filter:  199\nnumber of smape:  199\nnumber of smape:  199\ndone\nnumber of smape:  199\nnumber of smape:  199\nnumber of smape:  199\nnumber of smape:  199\nnumber of smape:  199\nnumber of smape:  199\nnumber of smape:  199\nnumber of smape:  199\nnumber of smape:  199\n[0.10007582959234303, 0.10026217626092603, 0.1006031377926676, 0.09999415792321895, 0.09990957026342276, 0.0997699623174074, 0.09999422748478928, 0.10005629843455019, 0.10020859179995195, 0.09912604813072291]\nnumber of smape:  199\nWape is:  {'Lstm': 0.5944494551675806, 'naive': 0.634296994397171, 'snaive': 0.6205157950643531, 'arima': 0.6028764485268353, 'ses': 0.6398369556694203, 'holts': 0.6452680089558595, 'Es': 0.6599410503162665, 'auto_ets': 0.6398322045540892, 'theta': 0.636324175992691, 'croston': 0.6254867546321764, 'prophet': 0.6983279541322936, 'Avg_ensemble': 0.639579242685497, 'Weighted_ensemble': 0.566442279418178}\nSmape is:  {'Lstm': 0.742189897578649, 'naive': 0.8234864065103867, 'snaive': 0.7937910017506129, 'arima': 0.7593547756711506, 'ses': 0.8354723986321541, 'holts': 0.8469335615609065, 'Es': 0.884080432427802, 'auto_ets': 0.8354620668497669, 'theta': 0.8285373203182327, 'croston': 0.8046250902818821, 'prophet': 0.9615985792445481, 'Avg_ensemble': 0.8347719187334103, 'Weighted_ensemble': 0.6879239863079382}\nTime is: {'Lstm': -187.18598341941833, 'naive': 0.11662983894348145, 'snaive': 0.12949156761169434, 'arima': 0.27082085609436035, 'ses': 0.10171127319335938, 'holts': 0.11120748519897461, 'Es': 0.29231858253479004, 'auto_ets': 0.2754087448120117, 'theta': 0.11044955253601074, 'croston': 0.10480070114135742, 'prophet': 0.33781886100769043, 'Avg_ensemble': 1.957188367843628, 'Weighted_ensemble': 0.8611979484558105}\n===============================================\ncluster id:  18\ngroup id /2637/615760\nTotal TS before filter:  4473\nTotal TS after filter:  114\nTotal TS before filter:  4473\nTotal TS after filter:  114\nnumber of smape:  114\nnumber of smape:  114\ndone\nnumber of smape:  114\nnumber of smape:  114\nnumber of smape:  114\nnumber of smape:  114\nnumber of smape:  114\nnumber of smape:  114\nnumber of smape:  114\nnumber of smape:  114\nnumber of smape:  114\n[0.10050137783372524, 0.09917776420140154, 0.10120863547567246, 0.10050137781890696, 0.10050459204541216, 0.09557665094257696, 0.1018580036498047, 0.0995107187230984, 0.09976712222251331, 0.10139375708688822]\nnumber of smape:  114\nWape is:  {'Lstm': 0.7459941064880978, 'naive': 0.6216714334747121, 'snaive': 0.6601217133280213, 'arima': 0.6050620149045809, 'ses': 0.6216714338532539, 'holts': 0.6216700356052137, 'Es': 0.8421379036247066, 'auto_ets': 0.5924825165366409, 'theta': 0.6470513301701909, 'croston': 0.6420100794867379, 'prophet': 0.7697840445073323, 'Avg_ensemble': 0.5863518985873704, 'Weighted_ensemble': 0.7420677142632827}\nSmape is:  {'Lstm': 1.088597489630758, 'naive': 0.7952322223390638, 'snaive': 0.8841760164147109, 'arima': 0.7575053249464991, 'ses': 0.7952322232001378, 'holts': 0.7954463808257913, 'Es': 1.3875947179407435, 'auto_ets': 0.7306594169560265, 'theta': 0.8489045116181493, 'croston': 0.8415580732671055, 'prophet': 0.5499281084035762, 'Avg_ensemble': 0.7199535187458266, 'Weighted_ensemble': 1.0805251104555715}\nTime is: {'Lstm': -182.99367260932922, 'naive': 0.06580662727355957, 'snaive': 0.06538248062133789, 'arima': 0.12413454055786133, 'ses': 0.06243777275085449, 'holts': 0.06596565246582031, 'Es': 0.27640652656555176, 'auto_ets': 0.29778385162353516, 'theta': 0.0735323429107666, 'croston': 0.06696963310241699, 'prophet': 0.2790567874908447, 'Avg_ensemble': 1.4400358200073242, 'Weighted_ensemble': 0.7270658016204834}\n===============================================\ncluster id:  19\ngroup id /976759/976779/8399244/6065814\nTotal TS before filter:  606\nTotal TS after filter:  52\nTotal TS before filter:  606\nTotal TS after filter:  52\nnumber of smape:  52\nnumber of smape:  52\ndone\nnumber of smape:  52\nnumber of smape:  52\nnumber of smape:  52\nnumber of smape:  52\nnumber of smape:  52\nnumber of smape:  52\nnumber of smape:  52\nnumber of smape:  52\nnumber of smape:  52\n[0.10004662339085021, 0.09997087047266882, 0.10012556726686217, 0.1000462318131743, 0.09999978520725354, 0.10005306321492184, 0.10004623174298227, 0.10007431370483592, 0.09992792835315269, 0.09970938483329818]\nnumber of smape:  52\nWape is:  {'Lstm': 0.7662950113038015, 'naive': 0.7214400676299956, 'snaive': 0.727818558893062, 'arima': 0.7160565004903716, 'ses': 0.7214747904524572, 'holts': 0.72580896049383, 'Es': 0.7246911970740804, 'auto_ets': 0.7214747966767746, 'theta': 0.7193850126924959, 'croston': 0.732362351215422, 'prophet': 0.755384395156406, 'Avg_ensemble': 0.7262253639757287, 'Weighted_ensemble': 0.7481698354398895}\nSmape is:  {'Lstm': 1.0746741933553077, 'naive': 0.966666548954289, 'snaive': 0.9822433358729408, 'arima': 0.9557838907475863, 'ses': 0.9667501942712596, 'holts': 0.9775345632408722, 'Es': 0.9789420879942384, 'auto_ets': 0.9667502092658237, 'theta': 0.962244798019234, 'croston': 0.9937031050431291, 'prophet': 1.0563907676634001, 'Avg_ensemble': 0.9789627833932534, 'Weighted_ensemble': 1.0379827767285041}\nTime is: {'Lstm': -190.8888065814972, 'naive': 0.03704476356506348, 'snaive': 0.036562204360961914, 'arima': 0.15464234352111816, 'ses': 0.03375101089477539, 'holts': 0.03854656219482422, 'Es': 0.281174898147583, 'auto_ets': 0.20350956916809082, 'theta': 0.04356074333190918, 'croston': 0.03517484664916992, 'prophet': 0.24795222282409668, 'Avg_ensemble': 1.1441314220428467, 'Weighted_ensemble': 0.6083676815032959}\n===============================================\ncluster id:  20\ngroup id /976760/1005863/1637840\nTotal TS before filter:  364\nTotal TS after filter:  24\nTotal TS before filter:  364\nTotal TS after filter:  24\nnumber of smape:  24\nnumber of smape:  24\ndone\nnumber of smape:  24\nnumber of smape:  24\nnumber of smape:  24\nnumber of smape:  24\nnumber of smape:  24\nnumber of smape:  24\nnumber of smape:  24\nnumber of smape:  24\nnumber of smape:  24\n[0.10050858772826454, 0.09642572651222511, 0.10043340232646969, 0.10054519265344576, 0.10083118662247818, 0.09986496859454705, 0.10054522755531108, 0.10062147749340393, 0.09884824323170421, 0.1013759872821505]\nnumber of smape:  24\nWape is:  {'Lstm': 0.6931288278250997, 'naive': 0.3923617514920238, 'snaive': 0.4154629630902573, 'arima': 0.3896673041583584, 'ses': 0.3921627908897253, 'holts': 0.39066079702430534, 'Es': 0.39235815723612144, 'auto_ets': 0.3921625997676597, 'theta': 0.3913363783516413, 'croston': 0.40045195731679173, 'prophet': 0.3858936152806048, 'Avg_ensemble': 0.3932099160478199, 'Weighted_ensemble': 0.5826890221495494}\nSmape is:  {'Lstm': 1.0135122952562865, 'naive': 0.3474308849882532, 'snaive': 0.4016282133103764, 'arima': 0.33471709215332157, 'ses': 0.3468873255965807, 'holts': 0.3424008561832767, 'Es': 0.3361051571805694, 'auto_ets': 0.3468868036622724, 'theta': 0.3433723604846955, 'croston': 0.36851951881577677, 'prophet': 0.32813441423275574, 'Avg_ensemble': 0.34808365371076805, 'Weighted_ensemble': 0.7560280744041736}\nTime is: {'Lstm': -174.4357132911682, 'naive': 0.04361844062805176, 'snaive': 0.029062986373901367, 'arima': 0.1674971580505371, 'ses': 0.02336406707763672, 'holts': 0.02874612808227539, 'Es': 0.22063732147216797, 'auto_ets': 0.19590353965759277, 'theta': 0.028809309005737305, 'croston': 0.019505739212036133, 'prophet': 0.2559692859649658, 'Avg_ensemble': 1.0281250476837158, 'Weighted_ensemble': 0.599388599395752}\n===============================================\ncluster id:  21\ngroup id /4171/4187/133126\nTotal TS before filter:  1945\nTotal TS after filter:  266\nTotal TS before filter:  1945\nTotal TS after filter:  266\nnumber of smape:  266\nnumber of smape:  266\ndone\nnumber of smape:  266\nnumber of smape:  266\nnumber of smape:  266\nnumber of smape:  266\nnumber of smape:  266\nnumber of smape:  266\nnumber of smape:  266\nnumber of smape:  266\nnumber of smape:  266\n[0.10149288961323875, 0.10010828948482803, 0.10028997041123053, 0.10057868410209726, 0.10124704260324319, 0.0943715072109851, 0.10057866922692343, 0.099052192208267, 0.10047781988334263, 0.10180293525584418]\nnumber of smape:  266\nWape is:  {'Lstm': 0.5857168332326755, 'naive': 0.5879050801273985, 'snaive': 0.5921523686385819, 'arima': 0.5791654543799024, 'ses': 0.5807360245891695, 'holts': 0.5856190048347933, 'Es': 0.6233413807420557, 'auto_ets': 0.5807359338849636, 'theta': 0.5734565576089249, 'croston': 0.5801383910672138, 'prophet': 0.5891667274601436, 'Avg_ensemble': 0.5762525913564966, 'Weighted_ensemble': 0.5966349442273762}\nSmape is:  {'Lstm': 0.7052878312887855, 'naive': 0.6902300251654782, 'snaive': 0.7062671998523451, 'arima': 0.6958590244304779, 'ses': 0.6946047778193228, 'holts': 0.6912990645652828, 'Es': 0.8615815139425319, 'auto_ets': 0.6946048830671967, 'theta': 0.7032262136032025, 'croston': 0.6953375025595623, 'prophet': 0.7102133243332697, 'Avg_ensemble': 0.7008838273822181, 'Weighted_ensemble': 0.6918751299253313}\nTime is: {'Lstm': -186.6455910205841, 'naive': 0.14914536476135254, 'snaive': 0.15301132202148438, 'arima': 0.19551420211791992, 'ses': 0.13982152938842773, 'holts': 0.14647650718688965, 'Es': 0.3242037296295166, 'auto_ets': 0.25765490531921387, 'theta': 0.14075899124145508, 'croston': 0.13092470169067383, 'prophet': 0.36496472358703613, 'Avg_ensemble': 2.128462791442871, 'Weighted_ensemble': 0.7410407066345215}\n===============================================\ncluster id:  22\ngroup id /976759/976789/5428795/3333999\nTotal TS before filter:  725\nTotal TS after filter:  65\nTotal TS before filter:  725\nTotal TS after filter:  65\nnumber of smape:  65\nnumber of smape:  65\ndone\nnumber of smape:  65\nnumber of smape:  65\nnumber of smape:  65\nnumber of smape:  65\nnumber of smape:  65\nnumber of smape:  65\nnumber of smape:  65\nnumber of smape:  65\nnumber of smape:  65\n[0.10005251215252348, 0.09990443170015412, 0.10022653942743366, 0.10005251214744451, 0.10002866475254057, 0.10034927554212057, 0.10005247806800906, 0.10000201212931689, 0.09990268927058263, 0.09942888480987441]\nnumber of smape:  65\nWape is:  {'Lstm': 0.8175276507191105, 'naive': 0.7779987403515849, 'snaive': 0.791831950992747, 'arima': 0.7634306329947662, 'ses': 0.777998740841369, 'holts': 0.780683447160099, 'Es': 0.7653200169023913, 'auto_ets': 0.7780020272428796, 'theta': 0.7819966178200425, 'croston': 0.7929678996494033, 'prophet': 0.8378373143760064, 'Avg_ensemble': 0.7844282310686829, 'Weighted_ensemble': 0.8394324149815074}\nSmape is:  {'Lstm': 1.158637686448772, 'naive': 1.0672072963061607, 'snaive': 1.105694505580625, 'arima': 1.0311869817100943, 'ses': 1.067207297647448, 'holts': 1.0753551534481451, 'Es': 1.0610469650948613, 'auto_ets': 1.0672162975920438, 'theta': 1.0766271460781738, 'croston': 1.1096310601790418, 'prophet': 1.2354936280431832, 'Avg_ensemble': 1.0864011818294776, 'Weighted_ensemble': 1.2534130033546673}\nTime is: {'Lstm': -188.15374207496643, 'naive': 0.04414534568786621, 'snaive': 0.04527616500854492, 'arima': 0.10259604454040527, 'ses': 0.04303479194641113, 'holts': 0.048333168029785156, 'Es': 0.26512813568115234, 'auto_ets': 0.18983006477355957, 'theta': 0.04973554611206055, 'croston': 0.05551290512084961, 'prophet': 0.24175453186035156, 'Avg_ensemble': 1.1222236156463623, 'Weighted_ensemble': 0.6340739727020264}\n===============================================\ncluster id:  23\ngroup id /976759/976791/6259087/7801454\nTotal TS before filter:  2452\nTotal TS after filter:  151\nTotal TS before filter:  2452\nTotal TS after filter:  151\nnumber of smape:  151\nnumber of smape:  151\ndone\nnumber of smape:  151\nnumber of smape:  151\nnumber of smape:  151\nnumber of smape:  151\nnumber of smape:  151\nnumber of smape:  151\nnumber of smape:  151\nnumber of smape:  151\nnumber of smape:  151\n[0.10023851830688638, 0.099647873414363, 0.09986332717443892, 0.10017657395350921, 0.10030141045641895, 0.10007194017168416, 0.10017657039558076, 0.10021433726849108, 0.09974091323561253, 0.09956853562301508]\nnumber of smape:  151\nWape is:  {'Lstm': 0.8126569270053955, 'naive': 0.7259061189608559, 'snaive': 0.7583278599260161, 'arima': 0.745900858386161, 'ses': 0.7288777780417282, 'holts': 0.7229516522159152, 'Es': 0.7360277555809708, 'auto_ets': 0.7288779537064749, 'theta': 0.7271850665809834, 'croston': 0.7530894155341901, 'prophet': 0.7640450271163225, 'Avg_ensemble': 0.7382216311495006, 'Weighted_ensemble': 0.7557664458255625}\nSmape is:  {'Lstm': 1.249820709395745, 'naive': 1.0034175776136038, 'snaive': 1.0938741015716544, 'arima': 1.0578550994212241, 'ses': 1.0116879205895604, 'holts': 0.9968321708479979, 'Es': 1.031382233410994, 'auto_ets': 1.0116884103647703, 'theta': 1.007625048249751, 'croston': 1.078637273330832, 'prophet': 1.1092463615993604, 'Avg_ensemble': 1.037806736559543, 'Weighted_ensemble': 1.087583619132271}\nTime is: {'Lstm': -188.16310358047485, 'naive': 0.08364701271057129, 'snaive': 0.08378219604492188, 'arima': 0.21266555786132812, 'ses': 0.07845735549926758, 'holts': 0.0893259048461914, 'Es': 0.28351306915283203, 'auto_ets': 0.2306978702545166, 'theta': 0.08272409439086914, 'croston': 0.07781839370727539, 'prophet': 0.29304003715515137, 'Avg_ensemble': 1.5874340534210205, 'Weighted_ensemble': 0.6696004867553711}\n===============================================\ncluster id:  24\ngroup id /1005862/1071969/1074364\nTotal TS before filter:  5678\nTotal TS after filter:  432\nTotal TS before filter:  5678\nTotal TS after filter:  432\nnumber of smape:  432\nnumber of smape:  432\ndone\nnumber of smape:  432\nnumber of smape:  432\nnumber of smape:  432\nnumber of smape:  432\nnumber of smape:  432\nnumber of smape:  432\nnumber of smape:  432\nnumber of smape:  432\nnumber of smape:  432\n[0.10496915271272375, 0.09508392744886314, 0.09945483150091028, 0.10174864598309045, 0.10645352709808593, 0.09844749061597034, 0.10175140876534819, 0.10133412594673921, 0.09641837559838522, 0.09433851432988374]\nnumber of smape:  432\nWape is:  {'Lstm': 0.41629248725546314, 'naive': 0.44472063357132013, 'snaive': 0.42540950572537006, 'arima': 0.4074795046376712, 'ses': 0.4193708370233468, 'holts': 0.45878319130825473, 'Es': 0.4016426447380263, 'auto_ets': 0.41938845055580026, 'theta': 0.4176394922885422, 'croston': 0.40358232721276793, 'prophet': 0.4125482626361772, 'Avg_ensemble': 0.40810005754027995, 'Weighted_ensemble': 0.4164707873824932}\nSmape is:  {'Lstm': 0.5582052920075492, 'naive': 0.5291850088725542, 'snaive': 0.5573297757284066, 'arima': 0.5198404058970559, 'ses': 0.5207888405395578, 'holts': 0.534056086815697, 'Es': 0.5204729082706895, 'auto_ets': 0.5207933669089564, 'theta': 0.5206009290372346, 'croston': 0.531461863326196, 'prophet': 0.5566820278822755, 'Avg_ensemble': 0.519155098253429, 'Weighted_ensemble': 0.5199503803034482}\nTime is: {'Lstm': -160.4605360031128, 'naive': 0.2787644863128662, 'snaive': 0.24125289916992188, 'arima': 0.37183189392089844, 'ses': 0.23868370056152344, 'holts': 0.27024030685424805, 'Es': 0.3897998332977295, 'auto_ets': 0.323702335357666, 'theta': 0.21669340133666992, 'croston': 0.24386835098266602, 'prophet': 0.4215083122253418, 'Avg_ensemble': 3.199042558670044, 'Weighted_ensemble': 0.8757579326629639}\n===============================================\ncluster id:  25\ngroup id /976759/9176907/1001468/9603576\nTotal TS before filter:  609\nTotal TS after filter:  54\nTotal TS before filter:  609\nTotal TS after filter:  54\nnumber of smape:  54\nnumber of smape:  54\ndone\nnumber of smape:  54\nnumber of smape:  54\nnumber of smape:  54\nnumber of smape:  54\nnumber of smape:  54\nnumber of smape:  54\nnumber of smape:  54\nnumber of smape:  54\nnumber of smape:  54\n[0.10012810774808076, 0.09978361552628777, 0.09990688921859253, 0.10012793181158358, 0.10014906942213878, 0.09998890195969949, 0.10012790335765236, 0.1001505916093291, 0.09980729165995389, 0.09982969768668191]\nnumber of smape:  54\nWape is:  {'Lstm': 0.8721214384635751, 'naive': 0.7623529339821911, 'snaive': 0.7957920063533597, 'arima': 0.7838247953038401, 'ses': 0.7623686610169066, 'holts': 0.7611170020939925, 'Es': 0.7753565811526293, 'auto_ets': 0.7623712048248001, 'theta': 0.7606372140104554, 'croston': 0.7941042278270979, 'prophet': 0.7931131419307217, 'Avg_ensemble': 0.7746896527293379, 'Weighted_ensemble': 0.8106129223099906}\nSmape is:  {'Lstm': 1.37377118129461, 'naive': 1.0355456124255835, 'snaive': 1.1267840238785811, 'arima': 1.0922412626605906, 'ses': 1.0355844096898579, 'holts': 1.033495330922167, 'Es': 1.0687711587800808, 'auto_ets': 1.0355906858331498, 'theta': 1.0317184943960123, 'croston': 1.122108070081245, 'prophet': 1.1218308487141242, 'Avg_ensemble': 1.0675553553530315, 'Weighted_ensemble': 1.1745469564558961}\nTime is: {'Lstm': -188.5884256362915, 'naive': 0.03790926933288574, 'snaive': 0.038767337799072266, 'arima': 0.16699719429016113, 'ses': 0.036295175552368164, 'holts': 0.039322614669799805, 'Es': 0.23927831649780273, 'auto_ets': 0.21607518196105957, 'theta': 0.03975081443786621, 'croston': 0.03270459175109863, 'prophet': 0.2799661159515381, 'Avg_ensemble': 1.1563584804534912, 'Weighted_ensemble': 0.5937843322753906}\n===============================================\ncluster id:  26\ngroup id /2637/1042319\nTotal TS before filter:  11100\nTotal TS after filter:  213\nTotal TS before filter:  11100\nTotal TS after filter:  213\nnumber of smape:  213\nnumber of smape:  213\ndone\nnumber of smape:  213\nnumber of smape:  213\nnumber of smape:  213\nnumber of smape:  213\nnumber of smape:  213\nnumber of smape:  213\nnumber of smape:  213\nnumber of smape:  213\nnumber of smape:  213\n[0.09838754243007206, 0.09673436306172241, 0.10179488141662824, 0.10085387877831878, 0.09921906968831973, 0.09824711163108195, 0.10085392869786355, 0.10151458076506946, 0.10001033676730384, 0.10238430676362005]\nnumber of smape:  213\nWape is:  {'Lstm': 0.7977610185603352, 'naive': 0.749696407450964, 'snaive': 0.7551270676264283, 'arima': 0.7268591756730431, 'ses': 0.7308141408447053, 'holts': 0.7427551905970438, 'Es': 0.7231874942111226, 'auto_ets': 0.7308137741189714, 'theta': 0.7270110058016521, 'croston': 0.7370673999033346, 'prophet': 0.6839276861434084, 'Avg_ensemble': 0.7284291940316564, 'Weighted_ensemble': 0.806609679187936}\nSmape is:  {'Lstm': 0.7377010985522745, 'naive': 0.7230803554278119, 'snaive': 0.7261504726205148, 'arima': 0.7169719006251596, 'ses': 0.7180837691458586, 'holts': 0.7212346626266866, 'Es': 0.7197625104576675, 'auto_ets': 0.7180836744073746, 'theta': 0.7170697481527852, 'croston': 0.7197048102703426, 'prophet': 0.711246780148017, 'Avg_ensemble': 0.7176610940729395, 'Weighted_ensemble': 0.7403727292762013}\nTime is: {'Lstm': -186.54992961883545, 'naive': 0.11684727668762207, 'snaive': 0.11956095695495605, 'arima': 0.2063436508178711, 'ses': 0.10741233825683594, 'holts': 0.11098194122314453, 'Es': 0.27649378776550293, 'auto_ets': 0.2613997459411621, 'theta': 0.12409424781799316, 'croston': 0.1057279109954834, 'prophet': 0.36106371879577637, 'Avg_ensemble': 1.8932137489318848, 'Weighted_ensemble': 0.6997272968292236}\n===============================================\ncluster id:  27\ngroup id /976759/9176907/1001468/4800400\nTotal TS before filter:  582\nTotal TS after filter:  51\nTotal TS before filter:  582\nTotal TS after filter:  51\nnumber of smape:  51\nnumber of smape:  51\ndone\nnumber of smape:  51\nnumber of smape:  51\nnumber of smape:  51\nnumber of smape:  51\nnumber of smape:  51\nnumber of smape:  51\nnumber of smape:  51\nnumber of smape:  51\nnumber of smape:  51\n[0.10007836728786787, 0.09999622425620136, 0.10002981967378584, 0.10007201675825166, 0.10005048528700144, 0.0999267661680028, 0.10007201328941011, 0.10008405735041076, 0.0999712474410005, 0.0997190024880676]\nnumber of smape:  51\nWape is:  {'Lstm': 0.8442505763541833, 'naive': 0.7958687860209251, 'snaive': 0.803599293650148, 'arima': 0.801275156098324, 'ses': 0.7964986379881958, 'holts': 0.7988348664573038, 'Es': 0.812847664652118, 'auto_ets': 0.796498982253556, 'theta': 0.7955147455602206, 'croston': 0.8066471073248447, 'prophet': 0.831859850117134, 'Avg_ensemble': 0.8038224995900001, 'Weighted_ensemble': 0.8105107949739907}\nSmape is:  {'Lstm': 1.2546385636206083, 'naive': 1.1048028588105159, 'snaive': 1.1272452574149143, 'arima': 1.121388862401003, 'ses': 1.106618394249976, 'holts': 1.1139669511035835, 'Es': 1.1555433109467244, 'auto_ets': 1.10661938757197, 'theta': 1.104235402667277, 'croston': 1.13646255723842, 'prophet': 1.2105806468933817, 'Avg_ensemble': 1.1277274809487312, 'Weighted_ensemble': 1.1514676930701762}\nTime is: {'Lstm': -191.9509789943695, 'naive': 0.03982663154602051, 'snaive': 0.04049539566040039, 'arima': 0.16777610778808594, 'ses': 0.037198543548583984, 'holts': 0.03862452507019043, 'Es': 0.23756718635559082, 'auto_ets': 0.20286917686462402, 'theta': 0.04288172721862793, 'croston': 0.03554391860961914, 'prophet': 0.24893498420715332, 'Avg_ensemble': 1.1225190162658691, 'Weighted_ensemble': 0.6855456829071045}\n===============================================\ncluster id:  28\ngroup id /5428/91416/3013177/1225084\nTotal TS before filter:  2320\nTotal TS after filter:  234\nTotal TS before filter:  2320\nTotal TS after filter:  234\nnumber of smape:  234\nnumber of smape:  234\ndone\nnumber of smape:  234\nnumber of smape:  234\nnumber of smape:  234\nnumber of smape:  234\nnumber of smape:  234\nnumber of smape:  234\nnumber of smape:  234\nnumber of smape:  234\nnumber of smape:  234\n[0.09991438825466963, 0.09705837563705022, 0.1002934824827322, 0.10004444578079437, 0.09952008221235137, 0.09866024489289739, 0.10004525785051581, 0.09971019976184682, 0.09998090522713741, 0.10477261790000482]\nnumber of smape:  234\nWape is:  {'Lstm': 0.90501322326964, 'naive': 0.4676362787924728, 'snaive': 0.5597161891642366, 'arima': 0.48767409967420555, 'ses': 0.4692880321235161, 'holts': 0.45861376181606833, 'Es': 0.44399769623893953, 'auto_ets': 0.4693002146325784, 'theta': 0.4870829398304962, 'croston': 0.501891205675624, 'prophet': 0.39612907345650994, 'Avg_ensemble': 0.45802152831138476, 'Weighted_ensemble': 0.6858978579131617}\nSmape is:  {'Lstm': 0.6631044700431815, 'naive': 0.49370231613526167, 'snaive': 0.5345473890701931, 'arima': 0.49272215754004345, 'ses': 0.4926440043722765, 'holts': 0.49433867070479737, 'Es': 0.4990550031184275, 'auto_ets': 0.4926373599314218, 'theta': 0.5002150466979833, 'croston': 0.49831624304085204, 'prophet': 0.45616616891194484, 'Avg_ensemble': 0.4818707972669087, 'Weighted_ensemble': 0.5734546046531019}\nTime is: {'Lstm': -135.59119939804077, 'naive': 0.11971735954284668, 'snaive': 0.12095975875854492, 'arima': 0.22163057327270508, 'ses': 0.11669373512268066, 'holts': 0.12201857566833496, 'Es': 0.32427525520324707, 'auto_ets': 0.25981807708740234, 'theta': 0.13647937774658203, 'croston': 0.11512327194213867, 'prophet': 0.3597276210784912, 'Avg_ensemble': 2.022099494934082, 'Weighted_ensemble': 0.7321608066558838}\n===============================================\ncluster id:  29\ngroup id /976759/976791/5624760/1001424\nTotal TS before filter:  1947\nTotal TS after filter:  100\nTotal TS before filter:  1947\nTotal TS after filter:  100\nnumber of smape:  100\nnumber of smape:  100\ndone\nnumber of smape:  100\nnumber of smape:  100\nnumber of smape:  100\nnumber of smape:  100\nnumber of smape:  100\nnumber of smape:  100\nnumber of smape:  100\nnumber of smape:  100\nnumber of smape:  100\n[0.10009205119900591, 0.1000335507765331, 0.1000680918253691, 0.10007121530360315, 0.1000586712484604, 0.10006654740951039, 0.10007121130275758, 0.1000745217973067, 0.10001459833377016, 0.09944954080368357]\nnumber of smape:  100\nWape is:  {'Lstm': 0.8119398789132249, 'naive': 0.7867542276495009, 'snaive': 0.7918777097206973, 'arima': 0.7891933545154408, 'ses': 0.7886658038170179, 'holts': 0.7899539653837735, 'Es': 0.7937138130842167, 'auto_ets': 0.788666172127431, 'theta': 0.7884152912870617, 'croston': 0.7939601128792324, 'prophet': 0.8467048737937501, 'Avg_ensemble': 0.7954694613800954, 'Weighted_ensemble': 0.7946820025673039}\nSmape is:  {'Lstm': 1.1672777516724093, 'naive': 1.1026530957512732, 'snaive': 1.1171031776447349, 'arima': 1.1099597834538477, 'ses': 1.108097788300293, 'holts': 1.112120440038105, 'Es': 1.1291421543130613, 'auto_ets': 1.1080988408252097, 'theta': 1.1074940308432222, 'croston': 1.123380114537414, 'prophet': 1.2729583580221402, 'Avg_ensemble': 1.1272353436212281, 'Weighted_ensemble': 1.1270824298564148}\nTime is: {'Lstm': -187.15501308441162, 'naive': 0.06673312187194824, 'snaive': 0.06319355964660645, 'arima': 0.2080674171447754, 'ses': 0.05741238594055176, 'holts': 0.06290817260742188, 'Es': 0.28584980964660645, 'auto_ets': 0.22073674201965332, 'theta': 0.07000041007995605, 'croston': 0.06026482582092285, 'prophet': 0.28174638748168945, 'Avg_ensemble': 1.4279217720031738, 'Weighted_ensemble': 0.69376540184021}\n===============================================\ncluster id:  30\ngroup id /976759/1071964\nnumber of smape:  68\nnumber of smape:  68\nnumber of smape:  68\nnumber of smape:  68\nnumber of smape:  68\nnumber of smape:  68\n[0.10211966747667024, 0.09933308709315948, 0.09997530072624232, 0.10101361790561875, 0.10194213898665401, 0.09760499179863417, 0.1010136048783569, 0.10093554316514616, 0.09913249920417506, 0.09692954876534304]\nnumber of smape:  68\nWape is:  {'Lstm': 0.6645940470366817, 'naive': 0.6681290636812116, 'snaive': 0.6685811539178211, 'arima': 0.6651305195284742, 'ses': 0.6657475350466899, 'holts': 0.666814823609683, 'Es': 0.6725905578712678, 'auto_ets': 0.6657475127086009, 'theta': 0.6657175443787605, 'croston': 0.6652278640481739, 'prophet': 0.6724330082935673, 'Avg_ensemble': 0.6651224687239885, 'Weighted_ensemble': 0.6658739009351953}\nSmape is:  {'Lstm': 0.7868149070225877, 'naive': 0.7637046796086441, 'snaive': 0.8015238428856396, 'arima': 0.7876629316481294, 'ses': 0.7750362874223188, 'holts': 0.7655363788841583, 'Es': 0.8298125411286796, 'auto_ets': 0.7750364353305608, 'theta': 0.7758481723717547, 'croston': 0.8015188685631285, 'prophet': 0.846933590470199, 'Avg_ensemble': 0.7886125270163268, 'Weighted_ensemble': 0.8104320589814822}\nTime is: {'Lstm': -168.1005916595459, 'naive': 0.04846501350402832, 'snaive': 0.045539140701293945, 'arima': 0.25160741806030273, 'ses': 0.0759737491607666, 'holts': 0.053064823150634766, 'Es': 0.21428966522216797, 'auto_ets': 0.2232341766357422, 'theta': 0.05417776107788086, 'croston': 0.040673017501831055, 'prophet': 0.24500560760498047, 'Avg_ensemble': 1.2875680923461914, 'Weighted_ensemble': 0.6146159172058105}\n===============================================\ncluster id:  31\ngroup id /4171/3318550/7016108/9012469\nTotal TS before filter:  1691\nTotal TS after filter:  221\nTotal TS before filter:  1691\nTotal TS after filter:  221\nnumber of smape:  221\nnumber of smape:  221\ndone\nnumber of smape:  221\nnumber of smape:  221\nnumber of smape:  221\nnumber of smape:  221\nnumber of smape:  221\nnumber of smape:  221\nnumber of smape:  221\nnumber of smape:  221\nnumber of smape:  221\n[0.10052175735458056, 0.10052070921237051, 0.10039818151424754, 0.10047420968323022, 0.10045769723977802, 0.09649256125478622, 0.10047424734730183, 0.1000579587111977, 0.10052621042586178, 0.10007646725664568]\nnumber of smape:  221\nWape is:  {'Lstm': 0.6824614202441307, 'naive': 0.6637454502069605, 'snaive': 0.6662904540703748, 'arima': 0.6627033817141167, 'ses': 0.66342097252258, 'holts': 0.663271719676108, 'Es': 0.800470282769501, 'auto_ets': 0.6634212195095496, 'theta': 0.6603339658989287, 'croston': 0.6637780286046563, 'prophet': 0.6642356788077973, 'Avg_ensemble': 0.659521272008206, 'Weighted_ensemble': 0.6778945297700627}\nSmape is:  {'Lstm': 0.9651531191209015, 'naive': 0.9707319146080255, 'snaive': 0.9720272987702763, 'arima': 0.9724599261334097, 'ses': 0.9717759508085217, 'holts': 0.9720091715374729, 'Es': 1.29967505576939, 'auto_ets': 0.9717751113619696, 'theta': 0.9764685491760133, 'croston': 0.9706365900633298, 'prophet': 0.9797769538814887, 'Avg_ensemble': 0.97915953438334, 'Weighted_ensemble': 0.9664388905824751}\nTime is: {'Lstm': -189.1288924217224, 'naive': 0.1136012077331543, 'snaive': 0.11368680000305176, 'arima': 0.1684565544128418, 'ses': 0.11008429527282715, 'holts': 0.11451125144958496, 'Es': 0.2704660892486572, 'auto_ets': 0.23708581924438477, 'theta': 0.1599419116973877, 'croston': 0.11176156997680664, 'prophet': 0.34998083114624023, 'Avg_ensemble': 1.8641855716705322, 'Weighted_ensemble': 0.6534960269927979}\n===============================================\ncluster id:  32\ngroup id /91083/1077064/2130360\nTotal TS before filter:  6495\nTotal TS after filter:  545\nTotal TS before filter:  6495\nTotal TS after filter:  545\nnumber of smape:  545\nnumber of smape:  545\ndone\nnumber of smape:  545\nnumber of smape:  545\nnumber of smape:  545\nnumber of smape:  545\nnumber of smape:  545\nnumber of smape:  545\nnumber of smape:  545\nnumber of smape:  545\nnumber of smape:  545\n[0.10363168931014487, 0.09962095493215882, 0.09796555413571778, 0.10356627208795845, 0.10356497963381275, 0.09049544770670394, 0.10356627088203268, 0.10266467441008138, 0.10309490014677922, 0.09182925675461022]\nnumber of smape:  545\nWape is:  {'Lstm': 0.5423207232860755, 'naive': 0.5299031593381937, 'snaive': 0.5228713131210819, 'arima': 0.496014652158061, 'ses': 0.528120185854663, 'holts': 0.5312537199596408, 'Es': 0.500830201753722, 'auto_ets': 0.5281201625031734, 'theta': 0.5183557653816867, 'croston': 0.5232069155834497, 'prophet': 0.4750507655821891, 'Avg_ensemble': 0.5094133258013931, 'Weighted_ensemble': 0.5287581891830223}\nSmape is:  {'Lstm': 0.5531049673434201, 'naive': 0.5485996973052834, 'snaive': 0.5481808928236068, 'arima': 0.5367474701315398, 'ses': 0.5481043661708158, 'holts': 0.5491926638542085, 'Es': 0.5677140430725943, 'auto_ets': 0.5481043598482758, 'theta': 0.5440933079560161, 'croston': 0.5468981210008459, 'prophet': 0.5289894014502987, 'Avg_ensemble': 0.5415498063974091, 'Weighted_ensemble': 0.5470757929871874}\nTime is: {'Lstm': -188.83188724517822, 'naive': 0.3482351303100586, 'snaive': 0.27173471450805664, 'arima': 0.3599684238433838, 'ses': 0.2572500705718994, 'holts': 0.26297783851623535, 'Es': 0.4806857109069824, 'auto_ets': 0.5096900463104248, 'theta': 0.287860631942749, 'croston': 0.2899601459503174, 'prophet': 0.48615241050720215, 'Avg_ensemble': 3.8100855350494385, 'Weighted_ensemble': 0.8449845314025879}\n===============================================\ncluster id:  33\ngroup id /976759/9569500/1001443/8161643\nTotal TS before filter:  328\nTotal TS after filter:  26\nTotal TS before filter:  328\nTotal TS after filter:  26\nnumber of smape:  26\nnumber of smape:  26\ndone\nnumber of smape:  26\nnumber of smape:  26\nnumber of smape:  26\nnumber of smape:  26\nnumber of smape:  26\nnumber of smape:  26\nnumber of smape:  26\nnumber of smape:  26\nnumber of smape:  26\n[0.10010335436334461, 0.09991930004593634, 0.10001237189418094, 0.10009460364494216, 0.10010502045172535, 0.09996092335889976, 0.10009460427071987, 0.10010813827978196, 0.09992925725507591, 0.09967242643539317]\nnumber of smape:  26\nWape is:  {'Lstm': 0.7724134304473158, 'naive': 0.7271127719245742, 'snaive': 0.74376142299751, 'arima': 0.7361338166037341, 'ses': 0.7279023950460379, 'holts': 0.7273704619745602, 'Es': 0.7443185809228907, 'auto_ets': 0.727902338544018, 'theta': 0.7269116503134971, 'croston': 0.7434866281683699, 'prophet': 0.7717334117603599, 'Avg_ensemble': 0.73739583396471, 'Weighted_ensemble': 0.763904288600042}\nSmape is:  {'Lstm': 1.0796775809936423, 'naive': 0.9690858661092119, 'snaive': 1.011722981477348, 'arima': 0.9925340476105043, 'ses': 0.9710446929000025, 'holts': 0.9702673423001058, 'Es': 1.0179701316452485, 'auto_ets': 0.9710445526281308, 'theta': 0.968864816746979, 'croston': 1.0109968238214007, 'prophet': 1.0922391380263516, 'Avg_ensemble': 0.9958864908876941, 'Weighted_ensemble': 1.0691334588673653}\nTime is: {'Lstm': -188.41914343833923, 'naive': 0.027215242385864258, 'snaive': 0.02761530876159668, 'arima': 0.1315293312072754, 'ses': 0.021326541900634766, 'holts': 0.02641582489013672, 'Es': 0.22960519790649414, 'auto_ets': 0.17818260192871094, 'theta': 0.030282258987426758, 'croston': 0.022159099578857422, 'prophet': 0.2391359806060791, 'Avg_ensemble': 0.9504578113555908, 'Weighted_ensemble': 0.604910135269165}\n===============================================\ncluster id:  34\ngroup id /4171/4187\nTotal TS before filter:  11302\nTotal TS after filter:  327\nTotal TS before filter:  11302\nTotal TS after filter:  327\nnumber of smape:  327\nnumber of smape:  327\ndone\nnumber of smape:  327\nnumber of smape:  327\nnumber of smape:  327\nnumber of smape:  327\nnumber of smape:  327\nnumber of smape:  327\nnumber of smape:  327\nnumber of smape:  327\nnumber of smape:  327\n[0.10403228652429322, 0.09937941198917552, 0.1051694912032586, 0.10318976758283985, 0.10421704720744837, 0.0846068126378841, 0.1031896497608571, 0.1030664929780523, 0.10274832800010426, 0.09040071211608665]\nnumber of smape:  327\nWape is:  {'Lstm': 0.743349256318125, 'naive': 0.690328453917691, 'snaive': 0.7118916766460661, 'arima': 0.6881788798961698, 'ses': 0.701324609758676, 'holts': 0.6910052219539743, 'Es': 0.7142561316903882, 'auto_ets': 0.7013256362112, 'theta': 0.6665806532017376, 'croston': 0.7048945730428312, 'prophet': 0.7614564578379307, 'Avg_ensemble': 0.683078641024438, 'Weighted_ensemble': 0.7989159311924244}\nSmape is:  {'Lstm': 0.7186752821341749, 'naive': 0.7068502139965342, 'snaive': 0.7137940307378897, 'arima': 0.7064750890790378, 'ses': 0.7086666581554024, 'holts': 0.7068795617266139, 'Es': 0.8958185606241383, 'auto_ets': 0.7086668672654005, 'theta': 0.7067038646136757, 'croston': 0.7094063177629186, 'prophet': 0.7319503168026306, 'Avg_ensemble': 0.7060050030217264, 'Weighted_ensemble': 0.7364925667651713}\nTime is: {'Lstm': -188.82486200332642, 'naive': 0.1644599437713623, 'snaive': 0.16455769538879395, 'arima': 0.22180700302124023, 'ses': 0.18534517288208008, 'holts': 0.17533564567565918, 'Es': 0.33057284355163574, 'auto_ets': 0.3271646499633789, 'theta': 0.17318511009216309, 'croston': 0.17622756958007812, 'prophet': 0.38161635398864746, 'Avg_ensemble': 2.468841075897217, 'Weighted_ensemble': 0.7933354377746582}\n===============================================\ncluster id:  35\ngroup id /976759/976787/1001390/5517955\nTotal TS before filter:  908\nTotal TS after filter:  66\nTotal TS before filter:  908\nTotal TS after filter:  66\nnumber of smape:  66\nnumber of smape:  66\ndone\nnumber of smape:  66\nnumber of smape:  66\nnumber of smape:  66\nnumber of smape:  66\nnumber of smape:  66\nnumber of smape:  66\nnumber of smape:  66\nnumber of smape:  66\nnumber of smape:  66\n[0.10014993048789472, 0.09979287757735501, 0.10016631375093979, 0.10014993048075488, 0.10017250737260734, 0.10012437657158312, 0.10014988257324371, 0.10011524615100988, 0.09983667991471613, 0.09934225511989543]\nnumber of smape:  66\nWape is:  {'Lstm': 0.838274206330701, 'naive': 0.8020782807177674, 'snaive': 0.8360665849058657, 'arima': 0.8004771677976331, 'ses': 0.8020782813482074, 'holts': 0.8006056242856752, 'Es': 0.8030848971391246, 'auto_ets': 0.8020825115442084, 'theta': 0.8047116360145691, 'croston': 0.8320688288043261, 'prophet': 0.8892198638399711, 'Avg_ensemble': 0.8159586699736211, 'Weighted_ensemble': 0.8095084649745673}\nSmape is:  {'Lstm': 1.2108943933132577, 'naive': 1.140740020876446, 'snaive': 1.24966399180099, 'arima': 1.1352949362584108, 'ses': 1.140740022817115, 'holts': 1.1396929940882363, 'Es': 1.1384087481231202, 'auto_ets': 1.1407530446195322, 'theta': 1.1469371423869743, 'croston': 1.2353771676108565, 'prophet': 1.441055436259499, 'Avg_ensemble': 1.1837630375357293, 'Weighted_ensemble': 1.169707383160629}\nTime is: {'Lstm': -187.69266819953918, 'naive': 0.05516767501831055, 'snaive': 0.055406808853149414, 'arima': 0.16430139541625977, 'ses': 0.046044349670410156, 'holts': 0.05496811866760254, 'Es': 0.27803945541381836, 'auto_ets': 0.22034955024719238, 'theta': 0.05083036422729492, 'croston': 0.04053521156311035, 'prophet': 0.3133232593536377, 'Avg_ensemble': 1.3185005187988281, 'Weighted_ensemble': 0.7447109222412109}\n===============================================\ncluster id:  36\ngroup id /976759/976782\nTotal TS before filter:  15571\nTotal TS after filter:  273\nTotal TS before filter:  15571\nTotal TS after filter:  273\nnumber of smape:  273\nnumber of smape:  273\ndone\nnumber of smape:  273\nnumber of smape:  273\nnumber of smape:  273\nnumber of smape:  273\nnumber of smape:  273\nnumber of smape:  273\nnumber of smape:  273\nnumber of smape:  273\nnumber of smape:  273\n[0.0999600045438895, 0.10014422402318408, 0.10189472940830913, 0.10029462912388853, 0.09904101559435537, 0.0969708330800467, 0.10029632172381785, 0.09992858350730298, 0.10153411007037808, 0.09993554892482784]\nnumber of smape:  273\nWape is:  {'Lstm': 0.6675277546186354, 'naive': 0.6440307817552021, 'snaive': 0.6593615327616572, 'arima': 0.6517809361830313, 'ses': 0.6451827068974298, 'holts': 0.641055077884154, 'Es': 0.6388626284330163, 'auto_ets': 0.6451886416183699, 'theta': 0.6438802735618565, 'croston': 0.6501418297523096, 'prophet': 0.6435221508494382, 'Avg_ensemble': 0.6455303553406303, 'Weighted_ensemble': 0.6628688392398611}\nSmape is:  {'Lstm': 0.6790066940738786, 'naive': 0.6821084772574072, 'snaive': 0.6827484781240769, 'arima': 0.6790530514345205, 'ses': 0.6814805075617422, 'holts': 0.6840063495292719, 'Es': 0.6894806079285709, 'auto_ets': 0.6814774082490122, 'theta': 0.6822101941383235, 'croston': 0.6795468814803661, 'prophet': 0.6837181173612151, 'Avg_ensemble': 0.6818601386825435, 'Weighted_ensemble': 0.6776115421699191}\nTime is: {'Lstm': -189.24378561973572, 'naive': 0.15123724937438965, 'snaive': 0.14640307426452637, 'arima': 0.26268720626831055, 'ses': 0.13709783554077148, 'holts': 0.15639567375183105, 'Es': 0.34641075134277344, 'auto_ets': 0.2764158248901367, 'theta': 0.14094924926757812, 'croston': 0.15093564987182617, 'prophet': 0.3429274559020996, 'Avg_ensemble': 2.2409465312957764, 'Weighted_ensemble': 0.7224442958831787}\n===============================================\ncluster id:  37\ngroup id /2636/4646529/2002476\nTotal TS before filter:  525\nTotal TS after filter:  43\nTotal TS before filter:  525\nTotal TS after filter:  43\nnumber of smape:  43\nnumber of smape:  43\ndone\nnumber of smape:  43\nnumber of smape:  43\nnumber of smape:  43\nnumber of smape:  43\nnumber of smape:  43\nnumber of smape:  43\nnumber of smape:  43\nnumber of smape:  43\nnumber of smape:  43\n[0.11238809706128346, 0.09732936641024577, 0.09835071606741201, 0.10219201999873126, 0.11170027126726957, 0.10088716933197708, 0.10219187892364515, 0.09635111292922109, 0.0997767553833775, 0.07883261262683686]\nnumber of smape:  43\nWape is:  {'Lstm': 0.46108494342968953, 'naive': 0.148261952677067, 'snaive': 0.35049170551697517, 'arima': 0.34024169446663544, 'ses': 0.25333027892827503, 'holts': 0.15681420496958465, 'Es': 0.2537145887024953, 'auto_ets': 0.2533330246491926, 'theta': 0.40114848994810604, 'croston': 0.3055111728756806, 'prophet': 2.2657270740218913, 'Avg_ensemble': 0.21275344409114444, 'Weighted_ensemble': 0.9107915803996818}\nSmape is:  {'Lstm': 0.6746757947421075, 'naive': 0.15300389667761274, 'snaive': 0.4205237084852446, 'arima': 0.398323906293202, 'ses': 0.2791193726422244, 'holts': 0.1608087432658194, 'Es': 0.26368553044845167, 'auto_ets': 0.27912287157776744, 'theta': 0.4925643808615927, 'croston': 0.34789659441674164, 'prophet': 0.9585727768233445, 'Avg_ensemble': 0.21295932671566126, 'Weighted_ensemble': 1.5101875011561778}\nTime is: {'Lstm': -189.0604190826416, 'naive': 0.033223867416381836, 'snaive': 0.033637046813964844, 'arima': 0.09314346313476562, 'ses': 0.029239416122436523, 'holts': 0.03359270095825195, 'Es': 0.18921113014221191, 'auto_ets': 0.17469453811645508, 'theta': 0.037283897399902344, 'croston': 0.030529022216796875, 'prophet': 0.2288360595703125, 'Avg_ensemble': 0.9109969139099121, 'Weighted_ensemble': 0.5730156898498535}\n===============================================\ncluster id:  38\ngroup id /976759/9176907/1001469/2181160\nTotal TS before filter:  508\nTotal TS after filter:  37\nTotal TS before filter:  508\nTotal TS after filter:  37\nnumber of smape:  37\nnumber of smape:  37\ndone\nnumber of smape:  37\nnumber of smape:  37\nnumber of smape:  37\nnumber of smape:  37\nnumber of smape:  37\nnumber of smape:  37\nnumber of smape:  37\nnumber of smape:  37\nnumber of smape:  37\n[0.10009598979237272, 0.10000502149808442, 0.09995004391539594, 0.100095989788112, 0.1000900104915897, 0.09983788325732287, 0.10009596119996086, 0.10009853171879639, 0.09995834024896418, 0.09977222808940095]\nnumber of smape:  37\nWape is:  {'Lstm': 0.7920694474611011, 'naive': 0.7505890821607506, 'snaive': 0.7586472505728796, 'arima': 0.7646308989830595, 'ses': 0.7505890825795968, 'holts': 0.7516494384397293, 'Es': 0.7771071845307308, 'auto_ets': 0.7505918929240154, 'theta': 0.7503938912927752, 'croston': 0.7642932119327432, 'prophet': 0.7867530073221634, 'Avg_ensemble': 0.7604480356380656, 'Weighted_ensemble': 0.7738993527972219}\nSmape is:  {'Lstm': 1.1381031176777603, 'naive': 1.0210249706053518, 'snaive': 1.0422324191749734, 'arima': 1.0583351447582101, 'ses': 1.0210249717289839, 'holts': 1.0246040497240556, 'Es': 1.0943199429006132, 'auto_ets': 1.0210325110223186, 'theta': 1.0205779491663431, 'croston': 1.0583184415470188, 'prophet': 1.1282989127520997, 'Avg_ensemble': 1.0480493523529857, 'Weighted_ensemble': 1.0882675157948833}\nTime is: {'Lstm': -187.90461087226868, 'naive': 0.03132343292236328, 'snaive': 0.031406402587890625, 'arima': 0.18472981452941895, 'ses': 0.027616024017333984, 'holts': 0.03288388252258301, 'Es': 0.22954845428466797, 'auto_ets': 0.2110154628753662, 'theta': 0.05792593955993652, 'croston': 0.03527116775512695, 'prophet': 0.2758979797363281, 'Avg_ensemble': 1.1396336555480957, 'Weighted_ensemble': 0.6787376403808594}\n===============================================\ncluster id:  39\ngroup id /2636/9206773/4782003/1224995\nTotal TS before filter:  644\nTotal TS after filter:  63\nTotal TS before filter:  644\nTotal TS after filter:  63\nnumber of smape:  63\nnumber of smape:  63\ndone\nnumber of smape:  63\nnumber of smape:  63\nnumber of smape:  63\nnumber of smape:  63\nnumber of smape:  63\nnumber of smape:  63\nnumber of smape:  63\nnumber of smape:  63\nnumber of smape:  63\n[0.10342643808281639, 0.09493589357249695, 0.10324286753453543, 0.10334539009417386, 0.10308575391390211, 0.09383970435590286, 0.10334496561895956, 0.10349710276298112, 0.10345814817256817, 0.0878237358916634]\nnumber of smape:  63\nWape is:  {'Lstm': 0.70905408793073, 'naive': 0.5828489732121721, 'snaive': 1.1834540537906275, 'arima': 0.7696142196787719, 'ses': 0.748021662685435, 'holts': 0.5520778009878354, 'Es': 1.421600269068327, 'auto_ets': 0.7481134123506137, 'theta': 0.6540261129649524, 'croston': 0.7209996646710674, 'prophet': 4.26756080692123, 'Avg_ensemble': 0.9530821760683431, 'Weighted_ensemble': 0.8602661659949754}\nSmape is:  {'Lstm': 0.6124775372910485, 'naive': 0.527595056569251, 'snaive': 0.6573162037331837, 'arima': 0.6222128214773969, 'ses': 0.6122824149820126, 'holts': 0.5102486988478464, 'Es': 1.3219384198379205, 'auto_ets': 0.6123259378133472, 'theta': 0.5636148499700936, 'croston': 0.5992300842825411, 'prophet': 1.3603847950578414, 'Avg_ensemble': 0.6970687984738054, 'Weighted_ensemble': 1.2579248528715918}\nTime is: {'Lstm': -186.73775386810303, 'naive': 0.055791616439819336, 'snaive': 0.04436159133911133, 'arima': 0.17637395858764648, 'ses': 0.03919172286987305, 'holts': 0.04449319839477539, 'Es': 0.1875462532043457, 'auto_ets': 0.21491098403930664, 'theta': 0.04481625556945801, 'croston': 0.03730463981628418, 'prophet': 0.24872517585754395, 'Avg_ensemble': 1.1300899982452393, 'Weighted_ensemble': 0.5805621147155762}\n===============================================\ncluster id:  40\ngroup id /4171/1111647\nTotal TS before filter:  7776\nTotal TS after filter:  264\nTotal TS before filter:  7776\nTotal TS after filter:  264\nnumber of smape:  264\nnumber of smape:  264\ndone\nnumber of smape:  264\nnumber of smape:  264\nnumber of smape:  264\nnumber of smape:  264\nnumber of smape:  264\nnumber of smape:  264\nnumber of smape:  264\nnumber of smape:  264\nnumber of smape:  264\n[0.10394533642781909, 0.1050948812536692, 0.10272447588374112, 0.10397733393618173, 0.10360699439719216, 0.08347272185728662, 0.10258178494958291, 0.09839380374050548, 0.10526859003558695, 0.0909340775184347]\nnumber of smape:  264\nWape is:  {'Lstm': 0.6113651957874733, 'naive': 0.6053947478040209, 'snaive': 0.6125604742724186, 'arima': 0.6035427067507418, 'ses': 0.605427633043664, 'holts': 0.6049252800175288, 'Es': 0.877052509888075, 'auto_ets': 0.6036542380465364, 'theta': 0.602706169470401, 'croston': 0.6071433176249941, 'prophet': 0.6421073480814616, 'Avg_ensemble': 0.6034124551649123, 'Weighted_ensemble': 0.6244329190349294}\nSmape is:  {'Lstm': 0.6373279253414088, 'naive': 0.6465425829853539, 'snaive': 0.6433385565981362, 'arima': 0.6499995634129262, 'ses': 0.6464576153538715, 'holts': 0.6474593618885135, 'Es': 1.45248850532058, 'auto_ets': 0.6505882437219281, 'theta': 0.6692988707749493, 'croston': 0.643508148911702, 'prophet': 0.7734556461488454, 'Avg_ensemble': 0.6718982256980323, 'Weighted_ensemble': 0.6458148987399432}\nTime is: {'Lstm': -187.3940670490265, 'naive': 0.13709139823913574, 'snaive': 0.13759207725524902, 'arima': 0.23296523094177246, 'ses': 0.1587529182434082, 'holts': 0.16824626922607422, 'Es': 0.3180689811706543, 'auto_ets': 0.331082820892334, 'theta': 0.18346476554870605, 'croston': 0.1672060489654541, 'prophet': 0.4148733615875244, 'Avg_ensemble': 2.4148004055023193, 'Weighted_ensemble': 0.7689535617828369}\n===============================================\ncluster id:  41\ngroup id /1005862/1071969/2566660\nTotal TS before filter:  5106\nTotal TS after filter:  508\nTotal TS before filter:  5106\nTotal TS after filter:  508\nnumber of smape:  508\nnumber of smape:  508\ndone\nnumber of smape:  508\nnumber of smape:  508\nnumber of smape:  508\nnumber of smape:  508\nnumber of smape:  508\nnumber of smape:  508\nnumber of smape:  508\nnumber of smape:  508\nnumber of smape:  508\n[0.10326196513112398, 0.09459569197692946, 0.09659090237344505, 0.10255723374528063, 0.10425372598862016, 0.09946890894638871, 0.10255906982462837, 0.10179967680027573, 0.09676237193226864, 0.09815045328103933]\nnumber of smape:  508\nWape is:  {'Lstm': 0.3839991981115493, 'naive': 0.3914201784209999, 'snaive': 0.3855174876649556, 'arima': 0.3780243748185902, 'ses': 0.38802285183894786, 'holts': 0.3969526659922044, 'Es': 0.39939191063851026, 'auto_ets': 0.3880304765376337, 'theta': 0.38657131301428355, 'croston': 0.376777570901865, 'prophet': 0.3739067971341837, 'Avg_ensemble': 0.3813576627489665, 'Weighted_ensemble': 0.37801413958355756}\nSmape is:  {'Lstm': 0.4529338462722793, 'naive': 0.424067993937008, 'snaive': 0.44895305797167895, 'arima': 0.4358165043706333, 'ses': 0.4239734142996396, 'holts': 0.4241463966956486, 'Es': 0.42780140939239586, 'auto_ets': 0.42397320361795554, 'theta': 0.4246900789817351, 'croston': 0.4345273708135614, 'prophet': 0.42927923498010623, 'Avg_ensemble': 0.4249038582532477, 'Weighted_ensemble': 0.43942233477690246}\nTime is: {'Lstm': -184.01538014411926, 'naive': 0.2468700408935547, 'snaive': 0.24647235870361328, 'arima': 0.34641218185424805, 'ses': 0.24148845672607422, 'holts': 0.2457883358001709, 'Es': 0.4548492431640625, 'auto_ets': 0.4097473621368408, 'theta': 0.25297117233276367, 'croston': 0.2753617763519287, 'prophet': 0.4575059413909912, 'Avg_ensemble': 3.436457633972168, 'Weighted_ensemble': 0.8466198444366455}\n===============================================\ncluster id:  42\ngroup id /976759/976782/1001659/7866501\nTotal TS before filter:  849\nTotal TS after filter:  70\nTotal TS before filter:  849\nTotal TS after filter:  70\nnumber of smape:  70\nnumber of smape:  70\ndone\nnumber of smape:  70\nnumber of smape:  70\nnumber of smape:  70\nnumber of smape:  70\nnumber of smape:  70\nnumber of smape:  70\nnumber of smape:  70\nnumber of smape:  70\nnumber of smape:  70\n[0.10061026576568403, 0.09805496176067352, 0.10081181399469474, 0.10041116912665377, 0.10122266079648289, 0.10040620905072999, 0.10041115422220703, 0.10046456155251328, 0.09862160505847063, 0.09898559867189007]\nnumber of smape:  70\nWape is:  {'Lstm': 0.8425538297019812, 'naive': 0.5559251919427031, 'snaive': 0.6902164555758313, 'arima': 0.5495859741175565, 'ses': 0.5627535373579547, 'holts': 0.5369599929850746, 'Es': 0.56377525268959, 'auto_ets': 0.5627540655390793, 'theta': 0.5609526265837272, 'croston': 0.6519640849413071, 'prophet': 0.6353729460291134, 'Avg_ensemble': 0.5820853638757625, 'Weighted_ensemble': 0.8087420341159584}\nSmape is:  {'Lstm': 1.3720665632467441, 'naive': 0.667693518338527, 'snaive': 0.984284113028943, 'arima': 0.6564007092979381, 'ses': 0.6824835361062546, 'holts': 0.6299103149329999, 'Es': 0.6855759362212396, 'auto_ets': 0.6824846877608499, 'theta': 0.6789495042193298, 'croston': 0.885436960352481, 'prophet': 0.8517646387854214, 'Avg_ensemble': 0.7266240437933654, 'Weighted_ensemble': 1.2988936848056618}\nTime is: {'Lstm': -187.88607931137085, 'naive': 0.055115461349487305, 'snaive': 0.05517292022705078, 'arima': 0.10345005989074707, 'ses': 0.042324066162109375, 'holts': 0.04730820655822754, 'Es': 0.22318434715270996, 'auto_ets': 0.18506979942321777, 'theta': 0.047823190689086914, 'croston': 0.04048037528991699, 'prophet': 0.26099419593811035, 'Avg_ensemble': 1.1029834747314453, 'Weighted_ensemble': 0.670891284942627}\n===============================================\ncluster id:  43\ngroup id /976759/976794/7981173/5580286\nTotal TS before filter:  3237\nTotal TS after filter:  187\nTotal TS before filter:  3237\nTotal TS after filter:  187\nnumber of smape:  187\nnumber of smape:  187\ndone\nnumber of smape:  187\nnumber of smape:  187\nnumber of smape:  187\nnumber of smape:  187\nnumber of smape:  187\nnumber of smape:  187\nnumber of smape:  187\nnumber of smape:  187\nnumber of smape:  187\n[0.10020275528807675, 0.0999274607837931, 0.10008745218432134, 0.10006545504996857, 0.10015230268088221, 0.10021578081013585, 0.10006544488480124, 0.10009611033494036, 0.0999164678754166, 0.09927077010766408]\nnumber of smape:  187\nWape is:  {'Lstm': 0.7265018040536068, 'naive': 0.6766985183681474, 'snaive': 0.6930143038862141, 'arima': 0.6839135892464969, 'ses': 0.6848021871480061, 'holts': 0.6797536117253488, 'Es': 0.6774490086208362, 'auto_ets': 0.6848028007550071, 'theta': 0.6831536433831331, 'croston': 0.6940498404105175, 'prophet': 0.7351421230892069, 'Avg_ensemble': 0.6888508285243221, 'Weighted_ensemble': 0.6712131534814878}\nSmape is:  {'Lstm': 0.9968440174105152, 'naive': 0.8879108819718917, 'snaive': 0.9242938970535145, 'arima': 0.9045187885708098, 'ses': 0.9060083787584972, 'holts': 0.8951109251452457, 'Es': 0.8905018602729359, 'auto_ets': 0.9060097597580268, 'theta': 0.9026566820690191, 'croston': 0.9269658668764821, 'prophet': 1.0188722148841407, 'Avg_ensemble': 0.9147359726774426, 'Weighted_ensemble': 0.8764189792973105}\nTime is: {'Lstm': -185.82454347610474, 'naive': 0.09941959381103516, 'snaive': 0.1024937629699707, 'arima': 0.2051682472229004, 'ses': 0.10357928276062012, 'holts': 0.1103065013885498, 'Es': 0.31528449058532715, 'auto_ets': 0.2201697826385498, 'theta': 0.11403727531433105, 'croston': 0.10619401931762695, 'prophet': 0.32425904273986816, 'Avg_ensemble': 1.7900819778442383, 'Weighted_ensemble': 0.6840460300445557}\n===============================================\ncluster id:  44\ngroup id /2636/4646529/4234943\nTotal TS before filter:  2217\nTotal TS after filter:  277\nTotal TS before filter:  2217\nTotal TS after filter:  277\nnumber of smape:  277\nnumber of smape:  277\ndone\nnumber of smape:  277\nnumber of smape:  277\nnumber of smape:  277\nnumber of smape:  277\nnumber of smape:  277\nnumber of smape:  277\nnumber of smape:  277\nnumber of smape:  277\nnumber of smape:  277\n[0.10345269395616995, 0.1000955410983288, 0.10331131684922013, 0.104293457112031, 0.10209259435003634, 0.09264400299595427, 0.10429379393097336, 0.10023935223104936, 0.10614771651905018, 0.0834295309571867]\nnumber of smape:  277\nWape is:  {'Lstm': 0.48775125789104307, 'naive': 0.28653812846859145, 'snaive': 0.34819624142930367, 'arima': 0.2856874956068662, 'ses': 0.2861141913530618, 'holts': 0.2885726334643745, 'Es': 0.4473603351610969, 'auto_ets': 0.28611453360208083, 'theta': 0.30146869560528294, 'croston': 0.3033133993102928, 'prophet': 1.0738219598241925, 'Avg_ensemble': 0.2999484971340025, 'Weighted_ensemble': 0.3057662436063075}\nSmape is:  {'Lstm': 0.4385335817820704, 'naive': 0.30793834030764616, 'snaive': 0.3474411996996716, 'arima': 0.3076439252986375, 'ses': 0.30562992329581123, 'holts': 0.31334862185881535, 'Es': 0.5587862501988147, 'auto_ets': 0.3056294761142773, 'theta': 0.3324863891313368, 'croston': 0.3144044718553571, 'prophet': 0.6950508668385361, 'Avg_ensemble': 0.31139795322376307, 'Weighted_ensemble': 0.3150440517872051}\nTime is: {'Lstm': -187.26987552642822, 'naive': 0.14508581161499023, 'snaive': 0.1549081802368164, 'arima': 0.2882652282714844, 'ses': 0.16205835342407227, 'holts': 0.15852594375610352, 'Es': 0.3204922676086426, 'auto_ets': 0.3106508255004883, 'theta': 0.1976778507232666, 'croston': 0.13924455642700195, 'prophet': 0.34088683128356934, 'Avg_ensemble': 2.348672866821289, 'Weighted_ensemble': 0.687368631362915}\n===============================================\ncluster id:  45\ngroup id /976759/976787\nTotal TS before filter:  10040\nTotal TS after filter:  94\nTotal TS before filter:  10040\nTotal TS after filter:  94\nnumber of smape:  94\nnumber of smape:  94\ndone\nnumber of smape:  94\nnumber of smape:  94\nnumber of smape:  94\nnumber of smape:  94\nnumber of smape:  94\nnumber of smape:  94\nnumber of smape:  94\nnumber of smape:  94\nnumber of smape:  94\n[0.10164331900678383, 0.10105556008138254, 0.09890379755769454, 0.09905550394231102, 0.10055874976253332, 0.09522662627551225, 0.0990566086603267, 0.09799902770933744, 0.09972408250429447, 0.1067767244998237]\nnumber of smape:  94\nWape is:  {'Lstm': 0.701578669628021, 'naive': 0.678698842679311, 'snaive': 0.6809613667146298, 'arima': 0.6749893183667465, 'ses': 0.6744258915371338, 'holts': 0.676461639278258, 'Es': 0.6740187100001666, 'auto_ets': 0.6744273059243733, 'theta': 0.6736828712652589, 'croston': 0.6753499388514568, 'prophet': 0.6829327559379683, 'Avg_ensemble': 0.6752613625286662, 'Weighted_ensemble': 0.6763989583807822}\nSmape is:  {'Lstm': 0.72836758519549, 'naive': 0.7391915616277946, 'snaive': 0.7433637369727193, 'arima': 0.7497569508463486, 'ses': 0.749456459677294, 'holts': 0.7431008334555377, 'Es': 0.7761954670492054, 'auto_ets': 0.7494513693022242, 'theta': 0.7543367935005146, 'croston': 0.7464821725416995, 'prophet': 0.7289470392426083, 'Avg_ensemble': 0.7465412742140524, 'Weighted_ensemble': 0.745459791090439}\nTime is: {'Lstm': -179.27339816093445, 'naive': 0.05970883369445801, 'snaive': 0.05915379524230957, 'arima': 0.2132120132446289, 'ses': 0.0532991886138916, 'holts': 0.06299257278442383, 'Es': 0.27539944648742676, 'auto_ets': 0.25684523582458496, 'theta': 0.06230902671813965, 'croston': 0.05898737907409668, 'prophet': 0.29781436920166016, 'Avg_ensemble': 1.447601318359375, 'Weighted_ensemble': 0.6653957366943359}\n===============================================\ncluster id:  46\ngroup id /976760/1005863/1091528\nTotal TS before filter:  6447\nTotal TS after filter:  890\nTotal TS before filter:  6447\nTotal TS after filter:  890\nnumber of smape:  890\nnumber of smape:  890\ndone\nnumber of smape:  890\nnumber of smape:  890\nnumber of smape:  890\nnumber of smape:  890\nnumber of smape:  890\nnumber of smape:  890\nnumber of smape:  890\nnumber of smape:  890\nnumber of smape:  890\n[0.1007395402149627, 0.09731537739283744, 0.09963127386871239, 0.10073954023643039, 0.10050311873191385, 0.09391724288962963, 0.10073968427717243, 0.10049922406154986, 0.10162905124963792, 0.10428594707715336]\nnumber of smape:  890\nWape is:  {'Lstm': 0.31905342965370376, 'naive': 0.3312968304715474, 'snaive': 0.3411477641394797, 'arima': 0.33564965083789433, 'ses': 0.3312968303863671, 'holts': 0.33207074794277097, 'Es': 0.35560779515226876, 'auto_ets': 0.33129625885015357, 'theta': 0.33217601036959016, 'croston': 0.32763869692267317, 'prophet': 0.3203568321902536, 'Avg_ensemble': 0.33251844673437614, 'Weighted_ensemble': 0.32539174739102733}\nSmape is:  {'Lstm': 0.3304534266026132, 'naive': 0.3316556337667893, 'snaive': 0.33950678498958664, 'arima': 0.3337933574477334, 'ses': 0.3316556337258906, 'holts': 0.3320313905902575, 'Es': 0.3470539899343814, 'auto_ets': 0.3316553593060674, 'theta': 0.3320543853698796, 'croston': 0.3299567475640941, 'prophet': 0.32576474900579483, 'Avg_ensemble': 0.33250979703482314, 'Weighted_ensemble': 0.3291806827528573}\nTime is: {'Lstm': -180.34643077850342, 'naive': 0.47607922554016113, 'snaive': 0.43860316276550293, 'arima': 0.4730827808380127, 'ses': 0.4156160354614258, 'holts': 0.42105937004089355, 'Es': 0.6384468078613281, 'auto_ets': 0.7339973449707031, 'theta': 0.4789450168609619, 'croston': 0.483701229095459, 'prophet': 0.7519276142120361, 'Avg_ensemble': 5.757321834564209, 'Weighted_ensemble': 1.227017879486084}\n===============================================\ncluster id:  47\ngroup id /976759/1096070/1224976/3761246\nTotal TS before filter:  3321\nTotal TS after filter:  312\nTotal TS before filter:  3321\nTotal TS after filter:  312\nnumber of smape:  312\nnumber of smape:  312\ndone\nnumber of smape:  312\nnumber of smape:  312\nnumber of smape:  312\nnumber of smape:  312\nnumber of smape:  312\nnumber of smape:  312\nnumber of smape:  312\nnumber of smape:  312\nnumber of smape:  312\n[0.10088154327006243, 0.09836495757772262, 0.10057517295046842, 0.10084476831312622, 0.10135021963274436, 0.0984736362664597, 0.10084476821691125, 0.10078927104247092, 0.09869783032919678, 0.09917783240083712]\nnumber of smape:  312\nWape is:  {'Lstm': 0.6304181668476856, 'naive': 0.5650322051065025, 'snaive': 0.6353165066603949, 'arima': 0.5700311681013162, 'ses': 0.5655947172006102, 'holts': 0.5570835162872363, 'Es': 0.6339093761529627, 'auto_ets': 0.5655947186788272, 'theta': 0.5665933470935407, 'croston': 0.6209805056377423, 'prophet': 0.6078819280614173, 'Avg_ensemble': 0.5829269956189628, 'Weighted_ensemble': 0.6620391758966114}\nSmape is:  {'Lstm': 0.8758016375297817, 'naive': 0.7144146236230223, 'snaive': 0.8830682757666638, 'arima': 0.7303765803577369, 'ses': 0.7160812293786256, 'holts': 0.6964399912416941, 'Es': 0.8649468107583228, 'auto_ets': 0.716081233752497, 'theta': 0.7181900660036037, 'croston': 0.8525562907529672, 'prophet': 0.8352423173647429, 'Avg_ensemble': 0.7629572516205894, 'Weighted_ensemble': 0.9474548017769441}\nTime is: {'Lstm': -186.7046022415161, 'naive': 0.16936421394348145, 'snaive': 0.15613245964050293, 'arima': 0.2570791244506836, 'ses': 0.15397071838378906, 'holts': 0.16805601119995117, 'Es': 0.3427424430847168, 'auto_ets': 0.2704427242279053, 'theta': 0.15878009796142578, 'croston': 0.15364456176757812, 'prophet': 0.38207435607910156, 'Avg_ensemble': 2.358935594558716, 'Weighted_ensemble': 0.754462718963623}\n===============================================\ncluster id:  48\ngroup id /976759/976783/8438428/2725989\nTotal TS before filter:  2958\nTotal TS after filter:  188\nTotal TS before filter:  2958\nTotal TS after filter:  188\nnumber of smape:  188\nnumber of smape:  188\ndone\nnumber of smape:  188\nnumber of smape:  188\nnumber of smape:  188\nnumber of smape:  188\nnumber of smape:  188\nnumber of smape:  188\nnumber of smape:  188\nnumber of smape:  188\nnumber of smape:  188\n[0.1007278947564264, 0.0991198690389927, 0.09901408153292385, 0.10059812649518592, 0.10093413032446084, 0.10065068914980245, 0.10059812306938636, 0.10062879839128065, 0.09927309680337881, 0.09845519043816219]\nnumber of smape:  188\nWape is:  {'Lstm': 0.6401337443951617, 'naive': 0.4710526737512268, 'snaive': 0.5278745014158067, 'arima': 0.5318155954627055, 'ses': 0.47458340846376074, 'holts': 0.4648306736996736, 'Es': 0.4747543746625724, 'auto_ets': 0.4745835029621723, 'theta': 0.4736973781057418, 'croston': 0.5203346541771519, 'prophet': 0.5619027183575221, 'Avg_ensemble': 0.4940986334118207, 'Weighted_ensemble': 0.5397351510112416}\nSmape is:  {'Lstm': 0.8805465468857253, 'naive': 0.5589048230261394, 'snaive': 0.6577569442394618, 'arima': 0.6635245187480504, 'ses': 0.5648848403629846, 'holts': 0.5492987836358777, 'Es': 0.5689793408762902, 'auto_ets': 0.5648850005000365, 'theta': 0.5635055009684159, 'croston': 0.6438318097800683, 'prophet': 0.7222072425669824, 'Avg_ensemble': 0.5985927392486492, 'Weighted_ensemble': 0.6786988458396558}\nTime is: {'Lstm': -186.47635412216187, 'naive': 0.10206389427185059, 'snaive': 0.10230517387390137, 'arima': 0.2645738124847412, 'ses': 0.09438157081604004, 'holts': 0.10101032257080078, 'Es': 0.30800819396972656, 'auto_ets': 0.24329829216003418, 'theta': 0.09994697570800781, 'croston': 0.11191892623901367, 'prophet': 0.32239818572998047, 'Avg_ensemble': 1.8409545421600342, 'Weighted_ensemble': 0.6857137680053711}\n===============================================\ncluster id:  49\ngroup id /3944/3951/1089430/1230457\nTotal TS before filter:  1181\nTotal TS after filter:  174\nTotal TS before filter:  1181\nTotal TS after filter:  174\nnumber of smape:  174\nnumber of smape:  174\ndone\nnumber of smape:  174\nnumber of smape:  174\nnumber of smape:  174\nnumber of smape:  174\nnumber of smape:  174\nnumber of smape:  174\nnumber of smape:  174\nnumber of smape:  174\nnumber of smape:  174\n[0.09989343568662765, 0.09600738987077274, 0.1054969730183739, 0.10517476420763053, 0.09756111365138483, 0.10099785231732433, 0.10517542480556008, 0.10555812340884722, 0.09977908430152081, 0.08435583873195797]\nnumber of smape:  174\nWape is:  {'Lstm': 0.5312595806526221, 'naive': 0.42486157950231346, 'snaive': 0.46039579390031965, 'arima': 0.4248877347918421, 'ses': 0.4199287907855833, 'holts': 0.4294808425839644, 'Es': 0.4094407321840461, 'auto_ets': 0.41992968694198235, 'theta': 0.42039604674164566, 'croston': 0.4451117463166408, 'prophet': 0.6358563949017125, 'Avg_ensemble': 0.4258848950002756, 'Weighted_ensemble': 0.45609989403427725}\nSmape is:  {'Lstm': 0.6819107023073514, 'naive': 0.3756059144694766, 'snaive': 0.488999654655183, 'arima': 0.4199149491389517, 'ses': 0.3934572736453319, 'holts': 0.370239906192217, 'Es': 0.3598641509208348, 'auto_ets': 0.393462126047027, 'theta': 0.3966401105001791, 'croston': 0.4638176121458133, 'prophet': 0.9557406631923155, 'Avg_ensemble': 0.4246845325240226, 'Weighted_ensemble': 0.48590172940524234}\nTime is: {'Lstm': -173.9629349708557, 'naive': 0.09502625465393066, 'snaive': 0.09600830078125, 'arima': 0.2147047519683838, 'ses': 0.09052658081054688, 'holts': 0.0947573184967041, 'Es': 0.27725863456726074, 'auto_ets': 0.23409032821655273, 'theta': 0.09703230857849121, 'croston': 0.08726978302001953, 'prophet': 0.2947261333465576, 'Avg_ensemble': 1.6645374298095703, 'Weighted_ensemble': 0.6640727519989014}\n===============================================\ncluster id:  50\ngroup id /3944/1060825\nTotal TS before filter:  13697\nTotal TS after filter:  460\nTotal TS before filter:  13697\nTotal TS after filter:  460\nnumber of smape:  460\nnumber of smape:  460\ndone\nnumber of smape:  460\nnumber of smape:  460\nnumber of smape:  460\nnumber of smape:  460\nnumber of smape:  460\nnumber of smape:  460\nnumber of smape:  460\nnumber of smape:  460\nnumber of smape:  460\n[0.10224281099058201, 0.09845322109412642, 0.1021088288917146, 0.10156360887247673, 0.10313509512927863, 0.08972857424465186, 0.10156342611047932, 0.10460722034954746, 0.10037316187643426, 0.09622405244070875]\nnumber of smape:  460\nWape is:  {'Lstm': 0.9649103068074721, 'naive': 0.7687750688573521, 'snaive': 0.8098998528885253, 'arima': 0.7730966460272312, 'ses': 0.7768918161225039, 'holts': 0.7594743624472438, 'Es': 0.9876089364966419, 'auto_ets': 0.7768940351630426, 'theta': 0.7495464250511489, 'croston': 0.7920438917007844, 'prophet': 0.8495956435139049, 'Avg_ensemble': 0.7969429819076508, 'Weighted_ensemble': 0.8726173591727857}\nSmape is:  {'Lstm': 0.7946418323424775, 'naive': 0.7406838916929914, 'snaive': 0.7520699771818218, 'arima': 0.7411693152043942, 'ses': 0.7421105065765758, 'holts': 0.7389124271060663, 'Es': 0.7993518884480486, 'auto_ets': 0.7421109149337133, 'theta': 0.7369452435683539, 'croston': 0.7451353117778834, 'prophet': 0.7586305763796579, 'Avg_ensemble': 0.7470783422784418, 'Weighted_ensemble': 0.7666915497822161}\nTime is: {'Lstm': -186.11059951782227, 'naive': 0.23404717445373535, 'snaive': 0.23235392570495605, 'arima': 0.31646060943603516, 'ses': 0.2225022315979004, 'holts': 0.23821377754211426, 'Es': 0.4332270622253418, 'auto_ets': 0.3789784908294678, 'theta': 0.22987008094787598, 'croston': 0.23059391975402832, 'prophet': 0.49460601806640625, 'Avg_ensemble': 3.223761558532715, 'Weighted_ensemble': 0.8039159774780273}\n===============================================\n"}], "source": "training_j=[]\nfrom sklearn.metrics import mean_squared_error\n\nfor i in range(50):\n    st=time.time()\n    print('cluster id: ',i+1)\n    current_date = '20220831'\n    test_group_id = test_group_id_list[i]#'/3944/1060825/447913'\n    print('group id',test_group_id)\n    #for top cluster:\n    path = f\"gs://adtech-artifacts/ds/forecast/month_eval/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_tft_data_{current_date}.csv\"\n    #path = f\"gs://adtech-artifacts/ds/forecast/month_eval/tail_clusters/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_data_20220831.csv\"\n    # read in data\n    df = spark.read.option(\"header\", True).csv(path).toPandas() # The usage should be the same as previous\n    df=df[['ds', 'group_id', 'query_idx', 'SearchKeyValue',  'LocationKeyValue', 'daily_supply']]\n    #print('query id: ',df['query_idx'].unique())\n    #df_panda=spark.read.csv(\"gs://adtech-artifacts/ds/forecast/query_taxonomy_sample_Addedfeatures_cluster_\"+str(i)+\".csv\",header=True).toPandas()\n    Cols=['daily_supply']#, 'shifted_annual_daily_supply','Trend_Var','dayofyear_sin', 'dayofyear_cos', 'month_sin', 'month_cos']\n    for cols in Cols:df[cols]=df[cols].astype(float,errors='raise')\n    #print('time needed to preprocess the data is: ',time.time()-st)\n    df_train,df_test,train_date,test_date,q,wq=_generate_train_test_data(df,start=start_month[month])\n    df_trainW,df_valW,_,train_dateW,_,_,_=_generate_train_test_data(df,val_req=True,start=start_month[month])\n    df1_train,df1_test=df_train.sum(axis=1),df_test.sum(axis=1)\n    df1_trainW,df_valW=df_trainW.sum(axis=1),df_valW.sum(axis=1)\n    df1_train,df1_trainW,df_valW=df1_train.to_frame(),df1_trainW.to_frame(),df_valW.to_frame()\n    df1_train.index=train_date\n    df1_trainW.index=train_dateW\n    df1_train.index = df1_train.index.to_period(freq = 'D')\n    df1_trainW.index = df1_trainW.index.to_period(freq = 'D')\n    #df_test_log_temp.index = df_test_log_temp.index.to_period(freq = 'D')\n    df1_train=df1_train.rename(columns={0:'daily_supply'})\n    df1_trainW=df1_trainW.rename(columns={0:'daily_supply'})\n#     if i==0:\n#         print(q,wq)\n    start=time.time()\n    Models=statModels()\n\n    Models.clus_smape['Lstm']=AggSmapeL[i]\n    Models.clus_wape['Lstm']=AggWapeL[i]\n    Models.clus_pe['Lstm']=AggPeL[i]\n    #Models.models['Lstm'] = fit4\n    #self.stores_mape['Es'] = mape\n    Models.stores_smape['Lstm'] = SMAPEsL[i]\n    Models.store_wape['Lstm'] = WAPEsL[i]\n    Models.store_pe['Lstm'] = PEsL[i]\n    Models.smape_dis['Lstm']=[median_smape[i],Eighty_per_smape[i],Ninty_per_smape[i]]\n    Models.wape_dis['Lstm']=[median_wape[i],Eighty_per_wape[i],Ninty_per_wape[i]]\n    Models.pe_dis['Lstm']=[median_pe[i],Eighty_per_pe[i],Ninty_per_pe[i]]\n    Models.stores_times['Lstm'] = TimesL[i]\n    ##################\n    #Models.wape_Weighted['Lstm'] = wape_agg_lstm[i]\n\n    Models.training_infer(df1_train,df1_test,df_test,q.values,wq.values,df1_test.shape[0],df_lstm[:,i])#no_ts\n    Models.weighted_avg(df1_trainW, df1_test,df_valW.values, df_test,q.values,wq.values, forecast_period=60,lstm_pred=df_lstm[:,i])\n\n    end=time.time()\n    print('Wape is: ',Models.store_wape)\n    print('Smape is: ',Models.stores_smape)\n    print('Time is:' ,Models.stores_times)\n    #print('time needed to run the model: ',end-start)\n    SMAPEs.append(Models.stores_smape)\n    WAPEs.append(Models.store_wape)\n    PEs.append(Models.store_pe)\n    Times.append(Models.stores_times)\n    AggSmape.append(Models.clus_smape)\n    AggWape.append(Models.clus_wape)\n    AggPe.append(Models.clus_pe)\n    SMAPEs_dist.append(Models.smape_dis)\n    WAPEs_dist.append(Models.wape_dis)\n    PEs_dist.append(Models.pe_dis)\n    training_j.append(i)\n\n    print('===============================================')\n    "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "Smape={i:[] for i in SMAPEs[0]}\nWape={i:[] for i in WAPEs[0]}\nPe={i:[] for i in PEs[0]}\nTime={i:[] for i in Times[0]}\n\nAggSmapes={i:[] for i in AggSmape[0]}\nAggWapes={i:[] for i in AggWape[0]}\nAggPes={i:[] for i in AggPe[0]}\n\nSMAPE_dist_median={i:[] for i in SMAPEs_dist[0]}\nSMAPE_dist_80={i:[] for i in SMAPEs_dist[0]}\nSMAPE_dist_90={i:[] for i in SMAPEs_dist[0]}\n\nWAPE_dist_median={i:[] for i in WAPEs_dist[0]}\nWAPE_dist_80={i:[] for i in WAPEs_dist[0]}\nWAPE_dist_90={i:[] for i in WAPEs_dist[0]}\n\n\nPE_dist_median={i:[] for i in PEs_dist[0]}\nPE_dist_80={i:[] for i in PEs_dist[0]}\nPE_dist_90={i:[] for i in PEs_dist[0]}"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "for i in range(len(training_j)):\n    for keys,values in SMAPEs[i].items():\n        Smape[keys].append(values)\n    for keys,values in WAPEs[i].items():\n        Wape[keys].append(values)\n    for keys,values in PEs[i].items():\n        Pe[keys].append(values)\n\n    for keys,values in Times[i].items():\n        Time[keys].append(values)\n    for keys,values in AggSmape[i].items():\n        AggSmapes[keys].append(values)\n    for keys,values in AggWape[i].items():\n        AggWapes[keys].append(values)\n    for keys,values in AggPe[i].items():\n        AggPes[keys].append(values)\n    \n    for keys,values in SMAPEs_dist[i].items():\n        SMAPE_dist_median[keys].append(values[0])\n    for keys,values in SMAPEs_dist[i].items():\n        SMAPE_dist_80[keys].append(values[1])\n    for keys,values in SMAPEs_dist[i].items():\n        SMAPE_dist_90[keys].append(values[2])\n        \n    for keys,values in WAPEs_dist[i].items():\n        WAPE_dist_median[keys].append(values[0])\n    for keys,values in WAPEs_dist[i].items():\n        WAPE_dist_80[keys].append(values[1])\n    for keys,values in WAPEs_dist[i].items():\n        WAPE_dist_90[keys].append(values[2])\n        \n    for keys,values in PEs_dist[i].items():\n        PE_dist_median[keys].append(values[0])\n    for keys,values in PEs_dist[i].items():\n        PE_dist_80[keys].append(values[1])\n    for keys,values in PEs_dist[i].items():\n        PE_dist_90[keys].append(values[2])\n    "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Smape is:  {'Lstm': [0.2975160837302321, 0.9517995760998587, 0.692831808176359, 0.4124451768476899, 1.2552534620218747, 1.0845748873897518, 0.7190090594447431, 0.7339880118401066, 0.6723234979805491, 0.868232189906101, 0.9294815328351423, 0.7227296038078813, 0.46484658054784456, 0.7040126061976242, 1.198052611591336, 0.96169418718733, 0.742189897578649, 1.088597489630758, 1.0746741933553077, 1.0135122952562865, 0.7052878312887855, 1.158637686448772, 1.249820709395745, 0.5582052920075492, 1.37377118129461, 0.7377010985522745, 1.2546385636206083, 0.6631044700431815, 1.1672777516724093, 0.7868149070225877, 0.9651531191209015, 0.5531049673434201, 1.0796775809936423, 0.7186752821341749, 1.2108943933132577, 0.6790066940738786, 0.6746757947421075, 1.1381031176777603, 0.6124775372910485, 0.6373279253414088, 0.4529338462722793, 1.3720665632467441, 0.9968440174105152, 0.4385335817820704, 0.72836758519549, 0.3304534266026132, 0.8758016375297817, 0.8805465468857253, 0.6819107023073514, 0.7946418323424775], 'naive': [0.3219149606626336, 0.6833409305930535, 0.5509458950775088, 0.38372154947963294, 1.259702493236272, 0.9276923631711553, 0.7313303594859374, 0.6716249756061677, 0.6643912895979338, 0.7889680797581724, 0.7889155550861774, 0.34600875095582856, 0.42424935836683336, 0.6876482875773147, 0.9271652164821338, 0.8261165284383574, 0.8234864065103867, 0.7952322223390638, 0.966666548954289, 0.3474308849882532, 0.6902300251654782, 1.0672072963061607, 1.0034175776136038, 0.5291850088725542, 1.0355456124255835, 0.7230803554278119, 1.1048028588105159, 0.49370231613526167, 1.1026530957512732, 0.7637046796086441, 0.9707319146080255, 0.5485996973052834, 0.9690858661092119, 0.7068502139965342, 1.140740020876446, 0.6821084772574072, 0.15300389667761274, 1.0210249706053518, 0.527595056569251, 0.6465425829853539, 0.424067993937008, 0.667693518338527, 0.8879108819718917, 0.30793834030764616, 0.7391915616277946, 0.3316556337667893, 0.7144146236230223, 0.5589048230261394, 0.3756059144694766, 0.7406838916929914], 'snaive': [0.36483981114279584, 0.8926199358107768, 0.6114678396382038, 0.39144532650851316, 1.231560352450597, 1.06890440544939, 0.7204588332103224, 0.6738598886696947, 0.6689588682455416, 0.8015583819839661, 0.9741320419709986, 0.45318365353443585, 0.462181167371014, 0.7451668872068619, 0.9617458523090247, 0.9833803928586909, 0.7937910017506129, 0.8841760164147109, 0.9822433358729408, 0.4016282133103764, 0.7062671998523451, 1.105694505580625, 1.0938741015716544, 0.5573297757284066, 1.1267840238785811, 0.7261504726205148, 1.1272452574149143, 0.5345473890701931, 1.1171031776447349, 0.8015238428856396, 0.9720272987702763, 0.5481808928236068, 1.011722981477348, 0.7137940307378897, 1.24966399180099, 0.6827484781240769, 0.4205237084852446, 1.0422324191749734, 0.6573162037331837, 0.6433385565981362, 0.44895305797167895, 0.984284113028943, 0.9242938970535145, 0.3474411996996716, 0.7433637369727193, 0.33950678498958664, 0.8830682757666638, 0.6577569442394618, 0.488999654655183, 0.7520699771818218], 'arima': [0.35079922182394213, 0.6668063538246948, 0.5452680483923446, 0.4697550757574243, 1.141958837568995, 1.0315528673487797, 0.712566918100152, 0.6723354229625892, 0.6594416571119146, 0.8040029782190412, 0.9036485738396651, 0.3610168774601723, 0.439176585653866, 0.6936867803364294, 0.9489566290171688, 0.9657606100402949, 0.7593547756711506, 0.7575053249464991, 0.9557838907475863, 0.33471709215332157, 0.6958590244304779, 1.0311869817100943, 1.0578550994212241, 0.5198404058970559, 1.0922412626605906, 0.7169719006251596, 1.121388862401003, 0.49272215754004345, 1.1099597834538477, 0.7876629316481294, 0.9724599261334097, 0.5367474701315398, 0.9925340476105043, 0.7064750890790378, 1.1352949362584108, 0.6790530514345205, 0.398323906293202, 1.0583351447582101, 0.6222128214773969, 0.6499995634129262, 0.4358165043706333, 0.6564007092979381, 0.9045187885708098, 0.3076439252986375, 0.7497569508463486, 0.3337933574477334, 0.7303765803577369, 0.6635245187480504, 0.4199149491389517, 0.7411693152043942], 'ses': [0.3395967654456753, 0.6833409327922187, 0.5509458958930336, 0.38537268080616854, 1.1565432247727045, 0.9298298404182714, 0.7220111955816303, 0.6718148830986527, 0.6641035622304046, 0.7912865965948412, 0.7889155566749159, 0.36519990008264663, 0.43214195876545153, 0.6921862804082255, 0.9292861784927742, 0.8729646264402295, 0.8354723986321541, 0.7952322232001378, 0.9667501942712596, 0.3468873255965807, 0.6946047778193228, 1.067207297647448, 1.0116879205895604, 0.5207888405395578, 1.0355844096898579, 0.7180837691458586, 1.106618394249976, 0.4926440043722765, 1.108097788300293, 0.7750362874223188, 0.9717759508085217, 0.5481043661708158, 0.9710446929000025, 0.7086666581554024, 1.140740022817115, 0.6814805075617422, 0.2791193726422244, 1.0210249717289839, 0.6122824149820126, 0.6464576153538715, 0.4239734142996396, 0.6824835361062546, 0.9060083787584972, 0.30562992329581123, 0.749456459677294, 0.3316556337258906, 0.7160812293786256, 0.5648848403629846, 0.3934572736453319, 0.7421105065765758], 'holts': [0.3108138278391591, 0.6562542020380467, 0.5440172566460505, 0.38294235718108033, 1.400379324660917, 0.9173168987891902, 0.7315192818638221, 0.6717276171829969, 0.6637437359455958, 0.7922321365118, 0.7678707668222118, 0.3264705564586297, 0.42344936810041417, 0.6923531330672047, 0.9284411554820619, 0.8200104266952296, 0.8469335615609065, 0.7954463808257913, 0.9775345632408722, 0.3424008561832767, 0.6912990645652828, 1.0753551534481451, 0.9968321708479979, 0.534056086815697, 1.033495330922167, 0.7212346626266866, 1.1139669511035835, 0.49433867070479737, 1.112120440038105, 0.7655363788841583, 0.9720091715374729, 0.5491926638542085, 0.9702673423001058, 0.7068795617266139, 1.1396929940882363, 0.6840063495292719, 0.1608087432658194, 1.0246040497240556, 0.5102486988478464, 0.6474593618885135, 0.4241463966956486, 0.6299103149329999, 0.8951109251452457, 0.31334862185881535, 0.7431008334555377, 0.3320313905902575, 0.6964399912416941, 0.5492987836358777, 0.370239906192217, 0.7389124271060663], 'Es': [0.36394822380743197, 0.7215085412202802, 0.6139094264158136, 0.38538775652656515, 1.5848275834532988, 0.9823243536519779, 0.7387959904416118, 0.676700903499333, 0.6721465433650905, 0.7956885035873495, 0.853705094025563, 0.33305819909413287, 0.43746432398827956, 0.9079228109998411, 0.9615648762292623, 0.8695079515960192, 0.884080432427802, 1.3875947179407435, 0.9789420879942384, 0.3361051571805694, 0.8615815139425319, 1.0610469650948613, 1.031382233410994, 0.5204729082706895, 1.0687711587800808, 0.7197625104576675, 1.1555433109467244, 0.4990550031184275, 1.1291421543130613, 0.8298125411286796, 1.29967505576939, 0.5677140430725943, 1.0179701316452485, 0.8958185606241383, 1.1384087481231202, 0.6894806079285709, 0.26368553044845167, 1.0943199429006132, 1.3219384198379205, 1.45248850532058, 0.42780140939239586, 0.6855759362212396, 0.8905018602729359, 0.5587862501988147, 0.7761954670492054, 0.3470539899343814, 0.8649468107583228, 0.5689793408762902, 0.3598641509208348, 0.7993518884480486], 'auto_ets': [0.3395986991965783, 0.6833556894018337, 0.5509513681344543, 0.3853730688608536, 1.156543827696626, 0.9298264356828078, 0.722011076240194, 0.6718149062013165, 0.6641025184472747, 0.7912870195197758, 0.7889262177373874, 0.36525069679339206, 0.43214211455088514, 0.6922106551671017, 0.9292860343875543, 0.8729851737727818, 0.8354620668497669, 0.7306594169560265, 0.9667502092658237, 0.3468868036622724, 0.6946048830671967, 1.0672162975920438, 1.0116884103647703, 0.5207933669089564, 1.0355906858331498, 0.7180836744073746, 1.10661938757197, 0.4926373599314218, 1.1080988408252097, 0.7750364353305608, 0.9717751113619696, 0.5481043598482758, 0.9710445526281308, 0.7086668672654005, 1.1407530446195322, 0.6814774082490122, 0.27912287157776744, 1.0210325110223186, 0.6123259378133472, 0.6505882437219281, 0.42397320361795554, 0.6824846877608499, 0.9060097597580268, 0.3056294761142773, 0.7494513693022242, 0.3316553593060674, 0.716081233752497, 0.5648850005000365, 0.393462126047027, 0.7421109149337133], 'theta': [0.33481943054049573, 0.6861115172055688, 0.5515252925518168, 0.4091395424106076, 1.181686369824805, 0.9304496586208556, 0.7155491417146044, 0.6722365402439864, 0.6600612953882421, 0.7914090540624736, 0.7911460847747981, 0.35873728039723096, 0.4314598845223879, 0.6952418396270085, 0.9281946782724435, 0.8856764976053655, 0.8285373203182327, 0.8489045116181493, 0.962244798019234, 0.3433723604846955, 0.7032262136032025, 1.0766271460781738, 1.007625048249751, 0.5206009290372346, 1.0317184943960123, 0.7170697481527852, 1.104235402667277, 0.5002150466979833, 1.1074940308432222, 0.7758481723717547, 0.9764685491760133, 0.5440933079560161, 0.968864816746979, 0.7067038646136757, 1.1469371423869743, 0.6822101941383235, 0.4925643808615927, 1.0205779491663431, 0.5636148499700936, 0.6692988707749493, 0.4246900789817351, 0.6789495042193298, 0.9026566820690191, 0.3324863891313368, 0.7543367935005146, 0.3320543853698796, 0.7181900660036037, 0.5635055009684159, 0.3966401105001791, 0.7369452435683539], 'croston': [0.3458750659866462, 0.8569603549825914, 0.603306238714688, 0.38726121780250056, 1.1778274560846873, 1.0504538727327775, 0.7180146713675255, 0.6716192030377994, 0.6638228557149621, 0.8061866054677996, 0.9302643637527633, 0.41404715919762963, 0.4493379055347564, 0.7210254997857408, 0.9593746713765776, 0.9566031672034632, 0.8046250902818821, 0.8415580732671055, 0.9937031050431291, 0.36851951881577677, 0.6953375025595623, 1.1096310601790418, 1.078637273330832, 0.531461863326196, 1.122108070081245, 0.7197048102703426, 1.13646255723842, 0.49831624304085204, 1.123380114537414, 0.8015188685631285, 0.9706365900633298, 0.5468981210008459, 1.0109968238214007, 0.7094063177629186, 1.2353771676108565, 0.6795468814803661, 0.34789659441674164, 1.0583184415470188, 0.5992300842825411, 0.643508148911702, 0.4345273708135614, 0.885436960352481, 0.9269658668764821, 0.3144044718553571, 0.7464821725416995, 0.3299567475640941, 0.8525562907529672, 0.6438318097800683, 0.4638176121458133, 0.7451353117778834], 'prophet': [0.47055556286890193, 0.9334178155670617, 0.6396674192509068, 1.4442128444889781, 1.420004212935439, 1.1297245414082004, 0.7162728803454977, 0.673367057680434, 0.6770331126982407, 0.9097943953159583, 0.9577109464933656, 0.24398671093750637, 0.4501080419005537, 0.782522470464373, 0.9958365752344288, 1.0299477138957216, 0.9615985792445481, 0.5499281084035762, 1.0563907676634001, 0.32813441423275574, 0.7102133243332697, 1.2354936280431832, 1.1092463615993604, 0.5566820278822755, 1.1218308487141242, 0.711246780148017, 1.2105806468933817, 0.45616616891194484, 1.2729583580221402, 0.846933590470199, 0.9797769538814887, 0.5289894014502987, 1.0922391380263516, 0.7319503168026306, 1.441055436259499, 0.6837181173612151, 0.9585727768233445, 1.1282989127520997, 1.3603847950578414, 0.7734556461488454, 0.42927923498010623, 0.8517646387854214, 1.0188722148841407, 0.6950508668385361, 0.7289470392426083, 0.32576474900579483, 0.8352423173647429, 0.7222072425669824, 0.9557406631923155, 0.7586305763796579], 'Avg_ensemble': [0.3109069223597171, 0.7348603843484033, 0.5720820339425474, 0.4144074481451193, 1.2046025163982972, 0.9825475446023436, 0.721599001487127, 0.6718802851491367, 0.66404952683931, 0.8056696245576251, 0.847164692042158, 0.33889360580315403, 0.4353361814849313, 0.7007656718570404, 0.9452059924775339, 0.90269259221042, 0.8347719187334103, 0.7199535187458266, 0.9789627833932534, 0.34808365371076805, 0.7008838273822181, 1.0864011818294776, 1.037806736559543, 0.519155098253429, 1.0675553553530315, 0.7176610940729395, 1.1277274809487312, 0.4818707972669087, 1.1272353436212281, 0.7886125270163268, 0.97915953438334, 0.5415498063974091, 0.9958864908876941, 0.7060050030217264, 1.1837630375357293, 0.6818601386825435, 0.21295932671566126, 1.0480493523529857, 0.6970687984738054, 0.6718982256980323, 0.4249038582532477, 0.7266240437933654, 0.9147359726774426, 0.31139795322376307, 0.7465412742140524, 0.33250979703482314, 0.7629572516205894, 0.5985927392486492, 0.4246845325240226, 0.7470783422784418], 'Weighted_ensemble': [0.3813676041620934, 0.9545169308837301, 0.6098543131769195, 0.3865838635540142, 1.3755547552719012, 1.0595927755551453, 0.7224124938635795, 0.6794259539732768, 0.6606500909681448, 0.8079739835608348, 0.9552595581888931, 0.5340353269539624, 0.43089848041443946, 0.7029066691801825, 0.985647851694443, 0.9708618813659268, 0.6879239863079382, 1.0805251104555715, 1.0379827767285041, 0.7560280744041736, 0.6918751299253313, 1.2534130033546673, 1.087583619132271, 0.5199503803034482, 1.1745469564558961, 0.7403727292762013, 1.1514676930701762, 0.5734546046531019, 1.1270824298564148, 0.8104320589814822, 0.9664388905824751, 0.5470757929871874, 1.0691334588673653, 0.7364925667651713, 1.169707383160629, 0.6776115421699191, 1.5101875011561778, 1.0882675157948833, 1.2579248528715918, 0.6458148987399432, 0.43942233477690246, 1.2988936848056618, 0.8764189792973105, 0.3150440517872051, 0.745459791090439, 0.3291806827528573, 0.9474548017769441, 0.6786988458396558, 0.48590172940524234, 0.7666915497822161]}\n====================\nWape is:  {'Lstm': [0.2961820364407199, 0.6901325930334411, 0.5520904100376333, 0.36992699413210983, 0.9470156252864107, 0.7584016411756946, 0.6679775961029542, 0.9372335769021725, 0.7176513797177058, 0.6612106834628276, 0.6708704130930944, 1.1278696402372455, 0.39098395394847685, 0.5747624257419327, 0.8104471572442892, 0.6892540102099447, 0.5944494551675806, 0.7459941064880978, 0.7662950113038015, 0.6931288278250997, 0.5857168332326755, 0.8175276507191105, 0.8126569270053955, 0.41629248725546314, 0.8721214384635751, 0.7977610185603352, 0.8442505763541833, 0.90501322326964, 0.8119398789132249, 0.6645940470366817, 0.6824614202441307, 0.5423207232860755, 0.7724134304473158, 0.743349256318125, 0.838274206330701, 0.6675277546186354, 0.46108494342968953, 0.7920694474611011, 0.70905408793073, 0.6113651957874733, 0.3839991981115493, 0.8425538297019812, 0.7265018040536068, 0.48775125789104307, 0.701578669628021, 0.31905342965370376, 0.6304181668476856, 0.6401337443951617, 0.5312595806526221, 0.9649103068074721], 'naive': [0.32696840503648883, 0.5550908723460382, 0.4731118852921152, 0.36670072823640576, 0.948501346082314, 0.6913241551697417, 0.7514762042205784, 0.6945433566811018, 0.6821125431649321, 0.6264930601401002, 0.6120880702848215, 0.3518949402062261, 0.39256202284303465, 0.6353202047364845, 0.7083091375384271, 0.630393683984935, 0.634296994397171, 0.6216714334747121, 0.7214400676299956, 0.3923617514920238, 0.5879050801273985, 0.7779987403515849, 0.7259061189608559, 0.44472063357132013, 0.7623529339821911, 0.749696407450964, 0.7958687860209251, 0.4676362787924728, 0.7867542276495009, 0.6681290636812116, 0.6637454502069605, 0.5299031593381937, 0.7271127719245742, 0.690328453917691, 0.8020782807177674, 0.6440307817552021, 0.148261952677067, 0.7505890821607506, 0.5828489732121721, 0.6053947478040209, 0.3914201784209999, 0.5559251919427031, 0.6766985183681474, 0.28653812846859145, 0.678698842679311, 0.3312968304715474, 0.5650322051065025, 0.4710526737512268, 0.42486157950231346, 0.7687750688573521], 'snaive': [0.3902422503445538, 0.6573032645755887, 0.5051042116833387, 0.35823471062591083, 0.9595632304772688, 0.7508637783446072, 0.7154322691606536, 0.7048707813716208, 0.6819222532366565, 0.632264472771699, 0.6863184642223097, 0.5158658360405614, 0.39690072111804753, 0.5924181471343546, 0.7223842385255022, 0.6958421681286104, 0.6205157950643531, 0.6601217133280213, 0.727818558893062, 0.4154629630902573, 0.5921523686385819, 0.791831950992747, 0.7583278599260161, 0.42540950572537006, 0.7957920063533597, 0.7551270676264283, 0.803599293650148, 0.5597161891642366, 0.7918777097206973, 0.6685811539178211, 0.6662904540703748, 0.5228713131210819, 0.74376142299751, 0.7118916766460661, 0.8360665849058657, 0.6593615327616572, 0.35049170551697517, 0.7586472505728796, 1.1834540537906275, 0.6125604742724186, 0.3855174876649556, 0.6902164555758313, 0.6930143038862141, 0.34819624142930367, 0.6809613667146298, 0.3411477641394797, 0.6353165066603949, 0.5278745014158067, 0.46039579390031965, 0.8098998528885253], 'arima': [0.3668547938780164, 0.5461008063786131, 0.4699616782992698, 0.3918632395947326, 0.9300546826409012, 0.7375288196978879, 0.6961039027401505, 0.6825411664817581, 0.6539776705584309, 0.6330305067380981, 0.6604147043742735, 0.37359836580370115, 0.3846345713451822, 0.5853447122412891, 0.717690482701963, 0.690589429744248, 0.6028764485268353, 0.6050620149045809, 0.7160565004903716, 0.3896673041583584, 0.5791654543799024, 0.7634306329947662, 0.745900858386161, 0.4074795046376712, 0.7838247953038401, 0.7268591756730431, 0.801275156098324, 0.48767409967420555, 0.7891933545154408, 0.6651305195284742, 0.6627033817141167, 0.496014652158061, 0.7361338166037341, 0.6881788798961698, 0.8004771677976331, 0.6517809361830313, 0.34024169446663544, 0.7646308989830595, 0.7696142196787719, 0.6035427067507418, 0.3780243748185902, 0.5495859741175565, 0.6839135892464969, 0.2856874956068662, 0.6749893183667465, 0.33564965083789433, 0.5700311681013162, 0.5318155954627055, 0.4248877347918421, 0.7730966460272312], 'ses': [0.3512367923222345, 0.5550908734974966, 0.47311188570002566, 0.358347640838084, 0.9319443383363505, 0.6923135229119415, 0.7242183452534716, 0.6896748745974826, 0.6803723565997115, 0.6275163308440248, 0.612088070897864, 0.3785470105719708, 0.3857315007726159, 0.5897760410831452, 0.7092319416883897, 0.6496085594750252, 0.6398369556694203, 0.6216714338532539, 0.7214747904524572, 0.3921627908897253, 0.5807360245891695, 0.777998740841369, 0.7288777780417282, 0.4193708370233468, 0.7623686610169066, 0.7308141408447053, 0.7964986379881958, 0.4692880321235161, 0.7886658038170179, 0.6657475350466899, 0.66342097252258, 0.528120185854663, 0.7279023950460379, 0.701324609758676, 0.8020782813482074, 0.6451827068974298, 0.25333027892827503, 0.7505890825795968, 0.748021662685435, 0.605427633043664, 0.38802285183894786, 0.5627535373579547, 0.6848021871480061, 0.2861141913530618, 0.6744258915371338, 0.3312968303863671, 0.5655947172006102, 0.47458340846376074, 0.4199287907855833, 0.7768918161225039], 'holts': [0.3124132014103609, 0.5394949845766017, 0.469156901685236, 0.3657552226015769, 0.9637148134229155, 0.6860028764544136, 0.7514304794129929, 0.6910196437674743, 0.6782979541057044, 0.6278725942898765, 0.6026892007518537, 0.3267117739324275, 0.39312201687062215, 0.6488096519994253, 0.7087738162199997, 0.6268471404973405, 0.6452680089558595, 0.6216700356052137, 0.72580896049383, 0.39066079702430534, 0.5856190048347933, 0.780683447160099, 0.7229516522159152, 0.45878319130825473, 0.7611170020939925, 0.7427551905970438, 0.7988348664573038, 0.45861376181606833, 0.7899539653837735, 0.666814823609683, 0.663271719676108, 0.5312537199596408, 0.7273704619745602, 0.6910052219539743, 0.8006056242856752, 0.641055077884154, 0.15681420496958465, 0.7516494384397293, 0.5520778009878354, 0.6049252800175288, 0.3969526659922044, 0.5369599929850746, 0.6797536117253488, 0.2885726334643745, 0.676461639278258, 0.33207074794277097, 0.5570835162872363, 0.4648306736996736, 0.4294808425839644, 0.7594743624472438], 'Es': [0.40226756341999087, 0.5754374000871466, 0.5061394348365813, 0.35967521286787274, 0.9744837337437193, 0.7151913598517338, 0.7604396823649517, 0.6847280591371396, 0.7095759090592252, 0.6286484716943577, 0.6379518650647821, 0.3273816845810549, 0.3831413102194504, 0.6460994873405699, 0.7212308266007643, 0.6459977923599157, 0.6599410503162665, 0.8421379036247066, 0.7246911970740804, 0.39235815723612144, 0.6233413807420557, 0.7653200169023913, 0.7360277555809708, 0.4016426447380263, 0.7753565811526293, 0.7231874942111226, 0.812847664652118, 0.44399769623893953, 0.7937138130842167, 0.6725905578712678, 0.800470282769501, 0.500830201753722, 0.7443185809228907, 0.7142561316903882, 0.8030848971391246, 0.6388626284330163, 0.2537145887024953, 0.7771071845307308, 1.421600269068327, 0.877052509888075, 0.39939191063851026, 0.56377525268959, 0.6774490086208362, 0.4473603351610969, 0.6740187100001666, 0.35560779515226876, 0.6339093761529627, 0.4747543746625724, 0.4094407321840461, 0.9876089364966419], 'auto_ets': [0.35123949968795737, 0.5550985998411693, 0.4731146228023241, 0.3583468599886056, 0.9319443900154594, 0.6923119484193786, 0.7242179675567161, 0.6896745572039822, 0.6803658491424815, 0.6275165173671858, 0.6120921846441957, 0.3786179043432289, 0.38573144138699755, 0.5897389652816173, 0.7092318795284829, 0.6496170115361808, 0.6398322045540892, 0.5924825165366409, 0.7214747966767746, 0.3921625997676597, 0.5807359338849636, 0.7780020272428796, 0.7288779537064749, 0.41938845055580026, 0.7623712048248001, 0.7308137741189714, 0.796498982253556, 0.4693002146325784, 0.788666172127431, 0.6657475127086009, 0.6634212195095496, 0.5281201625031734, 0.727902338544018, 0.7013256362112, 0.8020825115442084, 0.6451886416183699, 0.2533330246491926, 0.7505918929240154, 0.7481134123506137, 0.6036542380465364, 0.3880304765376337, 0.5627540655390793, 0.6848028007550071, 0.28611453360208083, 0.6744273059243733, 0.33129625885015357, 0.5655947186788272, 0.4745835029621723, 0.41992968694198235, 0.7768940351630426], 'theta': [0.3447269325826967, 0.5566999160279325, 0.4734358333553104, 0.36029533334921954, 0.935441268493351, 0.6926238409734128, 0.7053295755753195, 0.6835701627579507, 0.6605353020624568, 0.6275746863951014, 0.6131155767111974, 0.3698167947242181, 0.38588782039241015, 0.5799382626412989, 0.7087201261141899, 0.6557193194882788, 0.636324175992691, 0.6470513301701909, 0.7193850126924959, 0.3913363783516413, 0.5734565576089249, 0.7819966178200425, 0.7271850665809834, 0.4176394922885422, 0.7606372140104554, 0.7270110058016521, 0.7955147455602206, 0.4870829398304962, 0.7884152912870617, 0.6657175443787605, 0.6603339658989287, 0.5183557653816867, 0.7269116503134971, 0.6665806532017376, 0.8047116360145691, 0.6438802735618565, 0.40114848994810604, 0.7503938912927752, 0.6540261129649524, 0.602706169470401, 0.38657131301428355, 0.5609526265837272, 0.6831536433831331, 0.30146869560528294, 0.6736828712652589, 0.33217601036959016, 0.5665933470935407, 0.4736973781057418, 0.42039604674164566, 0.7495464250511489], 'croston': [0.36002038984633833, 0.6415107337546168, 0.5000994300239648, 0.355857577598105, 0.9344255037652286, 0.7440131810717647, 0.7112618224267925, 0.6955358170838769, 0.6785696716288183, 0.634048663875547, 0.6685175427974853, 0.44919408901043123, 0.385327239007184, 0.5767398362556873, 0.721835927806856, 0.6843616361240988, 0.6254867546321764, 0.6420100794867379, 0.732362351215422, 0.40045195731679173, 0.5801383910672138, 0.7929678996494033, 0.7530894155341901, 0.40358232721276793, 0.7941042278270979, 0.7370673999033346, 0.8066471073248447, 0.501891205675624, 0.7939601128792324, 0.6652278640481739, 0.6637780286046563, 0.5232069155834497, 0.7434866281683699, 0.7048945730428312, 0.8320688288043261, 0.6501418297523096, 0.3055111728756806, 0.7642932119327432, 0.7209996646710674, 0.6071433176249941, 0.376777570901865, 0.6519640849413071, 0.6940498404105175, 0.3033133993102928, 0.6753499388514568, 0.32763869692267317, 0.6209805056377423, 0.5203346541771519, 0.4451117463166408, 0.7920438917007844], 'prophet': [0.38883643888786495, 0.6745378909145776, 0.5181705207374729, 0.9293037738099961, 0.9609215112543751, 0.7750909011025758, 0.7041730288366357, 0.6729691550210454, 0.725756501843113, 0.6790388745409534, 0.6782449441156354, 0.21278956786484168, 0.38572133227079936, 0.5869559625727379, 0.7385181899886102, 0.716950093703206, 0.6983279541322936, 0.7697840445073323, 0.755384395156406, 0.3858936152806048, 0.5891667274601436, 0.8378373143760064, 0.7640450271163225, 0.4125482626361772, 0.7931131419307217, 0.6839276861434084, 0.831859850117134, 0.39612907345650994, 0.8467048737937501, 0.6724330082935673, 0.6642356788077973, 0.4750507655821891, 0.7717334117603599, 0.7614564578379307, 0.8892198638399711, 0.6435221508494382, 2.2657270740218913, 0.7867530073221634, 4.26756080692123, 0.6421073480814616, 0.3739067971341837, 0.6353729460291134, 0.7351421230892069, 1.0738219598241925, 0.6829327559379683, 0.3203568321902536, 0.6078819280614173, 0.5619027183575221, 0.6358563949017125, 0.8495956435139049], 'Avg_ensemble': [0.3133454610672217, 0.5815694486476641, 0.4838273429432588, 0.3612796810141755, 0.9345828889065712, 0.7159576142360142, 0.7224471760185923, 0.6879763052279202, 0.6805239936543879, 0.6338772917638682, 0.6349681835723936, 0.3439656297971131, 0.38510514579050514, 0.5766308483606681, 0.7161715950500382, 0.6622180051249807, 0.639579242685497, 0.5863518985873704, 0.7262253639757287, 0.3932099160478199, 0.5762525913564966, 0.7844282310686829, 0.7382216311495006, 0.40810005754027995, 0.7746896527293379, 0.7284291940316564, 0.8038224995900001, 0.45802152831138476, 0.7954694613800954, 0.6651224687239885, 0.659521272008206, 0.5094133258013931, 0.73739583396471, 0.683078641024438, 0.8159586699736211, 0.6455303553406303, 0.21275344409114444, 0.7604480356380656, 0.9530821760683431, 0.6034124551649123, 0.3813576627489665, 0.5820853638757625, 0.6888508285243221, 0.2999484971340025, 0.6752613625286662, 0.33251844673437614, 0.5829269956189628, 0.4940986334118207, 0.4258848950002756, 0.7969429819076508], 'Weighted_ensemble': [0.4109607210448074, 0.6847664017611225, 0.5031904411607973, 0.38226408860170547, 0.9612349990726232, 0.7470142874810171, 0.7269930670699665, 0.7495124045367022, 0.6629753914777005, 0.6347362490686222, 0.6781140198527207, 0.6495901039852816, 0.3863467811975714, 0.6984880129513703, 0.7319855416030713, 0.690169944216245, 0.566442279418178, 0.7420677142632827, 0.7481698354398895, 0.5826890221495494, 0.5966349442273762, 0.8394324149815074, 0.7557664458255625, 0.4164707873824932, 0.8106129223099906, 0.806609679187936, 0.8105107949739907, 0.6858978579131617, 0.7946820025673039, 0.6658739009351953, 0.6778945297700627, 0.5287581891830223, 0.763904288600042, 0.7989159311924244, 0.8095084649745673, 0.6628688392398611, 0.9107915803996818, 0.7738993527972219, 0.8602661659949754, 0.6244329190349294, 0.37801413958355756, 0.8087420341159584, 0.6712131534814878, 0.3057662436063075, 0.6763989583807822, 0.32539174739102733, 0.6620391758966114, 0.5397351510112416, 0.45609989403427725, 0.8726173591727857]}\n====================\nPe is:  {'Lstm': [0.19537138530423412, 0.6740341813008532, 0.5049056099429929, 0.29194599354149026, 0.9211623537586227, 0.7505240952684394, 0.3309157223012169, 0.6907015912458557, 0.42751602879606054, 0.6404077337542503, 0.6458693252551657, 1.0527837147624628, 0.3256690642778302, 0.4671239216440135, 0.8094195927284132, 0.6591772721143437, 0.5717351465995281, 0.721778841666879, 0.7642501309050308, 0.688612016846456, 0.48244070515267345, 0.8117135414406031, 0.7986975007648563, 0.3421595649098705, 0.8715946117728668, 0.43099862432080865, 0.8429937558409638, 0.7556075036792333, 0.8069283955739864, 0.5733764986505909, 0.616836077338456, 0.41185991263898303, 0.7709700117591054, 0.39828581800395374, 0.8254129436782095, 0.3881161067485341, 0.450370511240148, 0.7909104960001491, 0.21311886815067124, 0.33055473604722635, 0.2892762773899094, 0.8397178697062297, 0.7125525541449343, 0.35130351573492663, 0.45771285587375216, 0.17980293069790854, 0.5918877769021875, 0.6274576870005036, 0.4664330781579982, 0.6110163981104872], 'naive': [0.21700014828571415, 0.5033141395787074, 0.42286989421115284, 0.2922352072700231, 0.9435800333467266, 0.6671820079320772, 0.4270306876935172, 0.3898121694517689, 0.38405540439072533, 0.6006025352681684, 0.5765309217088102, 0.17434741477853777, 0.3302583817074276, 0.5341637937614135, 0.7028432313490489, 0.586913402736646, 0.6180074916132084, 0.5616504968631577, 0.717243978686138, 0.3529163438782688, 0.49154252467942894, 0.7727505903940618, 0.7069268594853982, 0.38281535724071986, 0.7568537353758811, 0.377564234654223, 0.7920078456170376, 0.22543913301591265, 0.7801521208348852, 0.5699155887994078, 0.5963379510111511, 0.3931969551004029, 0.7242777884063518, 0.3403428442561604, 0.7938579688310539, 0.3779786330646292, 0.051715048491915494, 0.7481797526316596, 0.20106975770592722, 0.3324532415805258, 0.3057083668587813, 0.5379592389658294, 0.6543014712337472, 0.1502668326075059, 0.4495038603799983, 0.19423059627117542, 0.4962696202331846, 0.4118155286128508, 0.3621292172867077, 0.41456657498003574], 'snaive': [0.2949906106647314, 0.6419556307726939, 0.46515277454220333, 0.2893834256139696, 0.8663008340840279, 0.742842979721567, 0.3758081262347277, 0.3988032222922346, 0.37396885777975386, 0.6052431446678137, 0.6656541841431238, 0.41604619230351686, 0.3225225098584263, 0.47156933570307324, 0.7174787772249166, 0.667990646642703, 0.5989938222957402, 0.6096709741946095, 0.7230823805899005, 0.3796623010837978, 0.4842323674035093, 0.7881062711843494, 0.7456990067569901, 0.3229235423615615, 0.7935851397155097, 0.3773996785819981, 0.7995808350356782, 0.3233532860164246, 0.7850225418821033, 0.5745073973595793, 0.5965319864312587, 0.37533897137047956, 0.7419686244499714, 0.35387793099041326, 0.8323506578267211, 0.3814511152651442, 0.33047165824435254, 0.7563148144502148, 0.5810146502823611, 0.3304871114069176, 0.2855457301689246, 0.6805600844302377, 0.6739895526865151, 0.2048286818397118, 0.44954821177557086, 0.19727323614028447, 0.5936443990175019, 0.4943269376838305, 0.39588479984895536, 0.4494544279624866], 'arima': [0.276763735312723, 0.4911086581174261, 0.41917802373445345, 0.3190321407896541, 0.8754337103198444, 0.724171492673051, 0.35991910986867803, 0.3778523940565689, 0.35478420130277366, 0.6086487428907135, 0.632341282322121, 0.2446450224069028, 0.32130859123603095, 0.47200742561206394, 0.7132758944258172, 0.6607768580978967, 0.5828526442585457, 0.5396270790126134, 0.711861790879728, 0.3432640067977141, 0.4828172868209468, 0.7557286057639687, 0.7310212633359464, 0.33198983094906914, 0.7806164923513774, 0.35276847888984103, 0.798005821169808, 0.27669486225987305, 0.7829896020026408, 0.5732910623072563, 0.5953922459947755, 0.34490760149263844, 0.7340612392111919, 0.3395974221708692, 0.7918328886888031, 0.3800711297927127, 0.3322355260114074, 0.762639875062061, 0.3725627988120387, 0.3347919200190971, 0.28215629269162734, 0.5331332039359256, 0.6638135725318349, 0.15425367648334468, 0.45209709971865636, 0.2011090550190498, 0.5029605285462623, 0.5003027506004307, 0.37889037871715087, 0.421113885607615], 'ses': [0.25645781882812324, 0.5033141412992866, 0.42286989488839044, 0.28917158087332184, 0.8812776021784172, 0.6685714518222088, 0.3905457608787028, 0.38432801946313694, 0.381725545047839, 0.6018885626602524, 0.57653092254929, 0.23312848363531474, 0.3229027624549246, 0.4766493744996639, 0.7039550855213828, 0.6122986324319195, 0.6243865098018486, 0.5616504973479974, 0.7172835743213993, 0.35259586713842395, 0.48424914903592564, 0.7727505909452216, 0.7109358773632766, 0.34967430574104563, 0.7568725718558494, 0.35567220433248836, 0.7927165770546684, 0.23095753502783123, 0.7823784431436938, 0.5712828880405182, 0.5959133903330239, 0.3900753857598188, 0.7251482008384356, 0.34911675014731053, 0.7938579695951069, 0.3781633116646343, 0.23786385988427278, 0.7481797530671805, 0.34586866853485443, 0.3324157683429443, 0.3005666912253424, 0.5467283102876781, 0.6648247661889822, 0.13651312705203977, 0.45235067009571445, 0.19423059613738003, 0.49695064247915355, 0.41786688288846346, 0.3700741225870775, 0.42294951287352084], 'holts': [0.19657998720307626, 0.4869707487326244, 0.41934364478608166, 0.29201934993555684, 0.9617566219521599, 0.6630061056760667, 0.4257196560267572, 0.38583906703378107, 0.37913450142351135, 0.6025517878158799, 0.5664011775420077, 0.1297738998151042, 0.3310452919451526, 0.545523502952548, 0.703824686882313, 0.5841433788949119, 0.6305377306313804, 0.5619740688058276, 0.7223494435825527, 0.3498307888939593, 0.48912607594146607, 0.7758805077225507, 0.7042545989907337, 0.4006717896843786, 0.756069134033046, 0.36909193473494484, 0.7955189205539752, 0.22491438824939508, 0.7839701608640455, 0.5702693213750338, 0.5957857230961154, 0.3948054935974153, 0.7247657447428226, 0.34106241381082003, 0.7928618406363259, 0.37782504078332657, 0.05748995818344457, 0.7493466410907423, 0.3076733232768868, 0.3329880198021803, 0.3143445005676002, 0.5169054610821048, 0.6589226501773064, 0.17529321441128304, 0.4503051247962456, 0.19532396620477638, 0.49091222788719696, 0.40583146768959283, 0.3590762737285983, 0.4059561855824885], 'Es': [0.2704390237407729, 0.5285195969326937, 0.46040576723956816, 0.289233955277882, 0.9015189345941917, 0.6948451193009513, 0.4144287448599987, 0.3717553256364223, 0.40843446432961805, 0.6027918070112044, 0.6079576208094637, 0.11072683335968608, 0.32128398793023366, 0.5849822663701384, 0.7166598025526124, 0.6090833797771953, 0.647178384629718, 0.7686499959011752, 0.7204552747621481, 0.3410757301295346, 0.5570997326449557, 0.7584991683265165, 0.7183597638261906, 0.3239695809575839, 0.7696178335180676, 0.33358779493193785, 0.809593173382962, 0.27814163647017315, 0.7878431450183714, 0.5787720956621863, 0.7822422843632546, 0.33278970431914545, 0.7425799689989513, 0.4708203319155422, 0.7935657945925217, 0.37966851213575714, 0.0537667562565842, 0.7752187618606229, 0.22318471750736216, 0.8173890936446666, 0.30945256713272234, 0.548312126986064, 0.6533907986353245, 0.3939282808807193, 0.4641909602696563, 0.2133015494084307, 0.5744429572120527, 0.4242142616746552, 0.36315200258495467, 0.615357241357458], 'auto_ets': [0.2564621184449386, 0.5033256864732074, 0.42287443919412804, 0.2891714484036961, 0.881278106747851, 0.6685692415232224, 0.3905452708168314, 0.3843276638446515, 0.38171675855296655, 0.6018887968558193, 0.5765365624719365, 0.23326430535972717, 0.32290268532521144, 0.4766028843754015, 0.7039550118801299, 0.6123095132152263, 0.6243810586182947, 0.5241247417282654, 0.7172835814192035, 0.35259555928764463, 0.4842490384472416, 0.7727542891727447, 0.710936108842723, 0.34969765163193983, 0.7568756182734783, 0.3556718007474488, 0.792716964275952, 0.23099876235077127, 0.782378874318284, 0.5712829137542017, 0.5959137109971048, 0.3900753445610471, 0.7251481385731484, 0.34911762474283126, 0.7938630963197177, 0.3781643740577202, 0.23786690772826063, 0.748182675294975, 0.34598231178509176, 0.3352440897771507, 0.30057893877404207, 0.5467289711758326, 0.6648255594905396, 0.13650737213864741, 0.45234882053946235, 0.19422969840727958, 0.4969506442627141, 0.41786704332829555, 0.3700757293886664, 0.42295177530175304], 'theta': [0.248771924961159, 0.5050801417479504, 0.4232375069726569, 0.29608330064776045, 0.8928848697841616, 0.6689114875929021, 0.36979046211838174, 0.37870578920803216, 0.3619434517000259, 0.6019515875574499, 0.5776079550348717, 0.22186868192401984, 0.3232001251847362, 0.4685196884378058, 0.7034243824305062, 0.6188851015513139, 0.620627752391224, 0.5904320562798463, 0.7151139187837227, 0.35000276612956066, 0.48058124886412357, 0.7768970678016661, 0.7090699980015172, 0.3469852401154104, 0.7550486150369279, 0.3520600181506806, 0.7917050003992461, 0.24915361393468077, 0.7821078055371116, 0.5713977261499206, 0.5952662493126338, 0.3774416776791626, 0.7241265456034782, 0.32647038523728344, 0.7966441930402322, 0.37797626595293965, 0.3940273213493704, 0.7479824713911848, 0.23504577787288106, 0.35158814666567084, 0.2972565369825402, 0.544950160898007, 0.6630127825847155, 0.20873488006314866, 0.4538128427360194, 0.19557141241709372, 0.4976899375177869, 0.41682977766286855, 0.37100324033759263, 0.3995018907284226], 'croston': [0.2697079640081016, 0.6234800619512796, 0.461767150327582, 0.2891365152810095, 0.9011524161048886, 0.7348443212108456, 0.3740292686083504, 0.3909522399199304, 0.3793282135436786, 0.6099704700391939, 0.6466511127826712, 0.3495929894486723, 0.3222698808169651, 0.4690665466690707, 0.7183852172907969, 0.6560693447922924, 0.6078389982150472, 0.587600974167503, 0.7293041604715892, 0.36481836941376666, 0.4835472753368412, 0.7891163792129363, 0.7397803320158871, 0.3188071262293312, 0.7917450664331964, 0.3627876177801317, 0.8040079850604358, 0.2983798260448071, 0.7884935634825151, 0.5754220513921554, 0.5963805948628028, 0.38118808770304646, 0.7417051539556073, 0.35231056678623746, 0.8280873647126249, 0.37940050328055697, 0.2952408617102473, 0.7623791494020413, 0.3121081483563158, 0.3310950029944462, 0.28216532135115335, 0.6430466516044677, 0.676633158780988, 0.18119393187447355, 0.4512730048065722, 0.18910627716810477, 0.5784831599558905, 0.4894125727668362, 0.39148076758947736, 0.43894054350198514], 'prophet': [0.35747078985433334, 0.6632787322587745, 0.4877301155576989, 0.9225074370575723, 0.8727691866065462, 0.7691532367849998, 0.3637238807166996, 0.3700115876963458, 0.4179071523383621, 0.6598215494745611, 0.6584827313416421, 0.14310568868158347, 0.3222355340733764, 0.5073227276042168, 0.7355887469909894, 0.6941041423589229, 0.6886297943588416, 0.6607869938625492, 0.753647362848583, 0.34118341375998434, 0.4833000783912505, 0.8347969862079928, 0.7516124726789764, 0.3482988904201717, 0.7909524142084066, 0.31498531101601124, 0.8301627113945895, 0.23967100583332354, 0.8441694843719035, 0.5834358278385359, 0.5948786078268367, 0.3289344487615659, 0.7706045385271123, 0.37795233952518387, 0.8878470146813162, 0.3780113687738533, 2.2467599528525075, 0.7856080399047588, 4.176099499787085, 0.4466415342088969, 0.28221008691642346, 0.6263025423019312, 0.7225760160846258, 1.028230666672723, 0.4491285223245803, 0.182510822966538, 0.5675149505735763, 0.5429926269725412, 0.6011872004237389, 0.49802468824634766], 'Avg_ensemble': [0.20815431998000677, 0.5430273162287184, 0.43945267318666065, 0.2993099896704843, 0.8954884418730277, 0.6998912808203622, 0.38739128283425156, 0.3825748994223293, 0.3811336381947797, 0.6094323114077264, 0.6065379948875123, 0.19536635371486047, 0.32167083675047997, 0.46732789379985135, 0.7119390836548517, 0.6276213731192116, 0.6239042766183409, 0.5170780086126264, 0.7227625466344967, 0.3524943366662983, 0.4806671549277232, 0.7797054912123198, 0.7228337759605165, 0.3338892323633863, 0.7708205785785389, 0.3518009087919683, 0.8006015833944355, 0.23519813228959993, 0.7899370697403824, 0.5734490254620739, 0.5962654513331896, 0.3629106323168328, 0.7354385943307071, 0.3351103127145528, 0.8101954552520938, 0.37819235539192514, 0.08876496691871444, 0.7583733229995041, 0.5703865667127896, 0.354883884668304, 0.2897656269905131, 0.5701597233614768, 0.6694647540934509, 0.17777864220861792, 0.45134367406405435, 0.19488334417564757, 0.5211278141653949, 0.45097984263331325, 0.38028604706855684, 0.44156583180057535], 'Weighted_ensemble': [0.3316608323159685, 0.6745327670454173, 0.4672036344576759, 0.3020378739318031, 0.9462750879556255, 0.7391429851253847, 0.39778182330013356, 0.45252737489890715, 0.36348134942125737, 0.6113769882699459, 0.658136946290073, 0.6044763422349198, 0.32354265471802957, 0.5987801093222541, 0.7293447291691607, 0.6635847666445897, 0.536047600309637, 0.7172342697646884, 0.746417981248319, 0.5653335550560991, 0.49546765981437496, 0.8377537341662734, 0.7431522682880141, 0.345676160538242, 0.8091693774176122, 0.44716412150570967, 0.8084967691569573, 0.558924540773765, 0.7897022214205289, 0.5768913239372397, 0.6105776451322762, 0.39556775496066293, 0.7626939132528537, 0.456922537694422, 0.8039581088497428, 0.38570853933206706, 0.836441778414507, 0.7723895793281701, 0.6805830267294116, 0.33111898146648244, 0.284027657247239, 0.8061228353382518, 0.6485338908321302, 0.16863879866791232, 0.4507120891296047, 0.1856415282891654, 0.6346272282275273, 0.5156890429363115, 0.3970201828707525, 0.536894043557249]}\n====================\nTime is:  {'Lstm': [-191.39099884033203, -186.80035281181335, -186.55605792999268, -181.9619607925415, -186.2498950958252, -187.5533788204193, -190.80856323242188, -187.4871847629547, -188.06559085845947, -189.3646523952484, -189.23898100852966, -184.2036623954773, -182.4704315662384, -187.75834894180298, -187.32640528678894, -187.29383754730225, -187.18598341941833, -182.99367260932922, -190.8888065814972, -174.4357132911682, -186.6455910205841, -188.15374207496643, -188.16310358047485, -160.4605360031128, -188.5884256362915, -186.54992961883545, -191.9509789943695, -135.59119939804077, -187.15501308441162, -168.1005916595459, -189.1288924217224, -188.83188724517822, -188.41914343833923, -188.82486200332642, -187.69266819953918, -189.24378561973572, -189.0604190826416, -187.90461087226868, -186.73775386810303, -187.3940670490265, -184.01538014411926, -187.88607931137085, -185.82454347610474, -187.26987552642822, -179.27339816093445, -180.34643077850342, -186.7046022415161, -186.47635412216187, -173.9629349708557, -186.11059951782227], 'naive': [0.3917984962463379, 0.14705276489257812, 0.24319005012512207, 0.22348499298095703, 0.019543170928955078, 0.11139917373657227, 0.4624059200286865, 0.43306851387023926, 0.3686809539794922, 0.1608574390411377, 0.1665632724761963, 0.11742949485778809, 0.2549896240234375, 0.22760009765625, 0.031236648559570312, 0.22324275970458984, 0.11662983894348145, 0.06580662727355957, 0.03704476356506348, 0.04361844062805176, 0.14914536476135254, 0.04414534568786621, 0.08364701271057129, 0.2787644863128662, 0.03790926933288574, 0.11684727668762207, 0.03982663154602051, 0.11971735954284668, 0.06673312187194824, 0.04846501350402832, 0.1136012077331543, 0.3482351303100586, 0.027215242385864258, 0.1644599437713623, 0.05516767501831055, 0.15123724937438965, 0.033223867416381836, 0.03132343292236328, 0.055791616439819336, 0.13709139823913574, 0.2468700408935547, 0.055115461349487305, 0.09941959381103516, 0.14508581161499023, 0.05970883369445801, 0.47607922554016113, 0.16936421394348145, 0.10206389427185059, 0.09502625465393066, 0.23404717445373535], 'snaive': [0.36430859565734863, 0.1714036464691162, 0.24346327781677246, 0.22905349731445312, 0.019593477249145508, 0.1071007251739502, 0.455000638961792, 0.3879687786102295, 0.43140578269958496, 0.16278505325317383, 0.16094374656677246, 0.11677837371826172, 0.2582225799560547, 0.22720098495483398, 0.033753395080566406, 0.23947525024414062, 0.12949156761169434, 0.06538248062133789, 0.036562204360961914, 0.029062986373901367, 0.15301132202148438, 0.04527616500854492, 0.08378219604492188, 0.24125289916992188, 0.038767337799072266, 0.11956095695495605, 0.04049539566040039, 0.12095975875854492, 0.06319355964660645, 0.045539140701293945, 0.11368680000305176, 0.27173471450805664, 0.02761530876159668, 0.16455769538879395, 0.055406808853149414, 0.14640307426452637, 0.033637046813964844, 0.031406402587890625, 0.04436159133911133, 0.13759207725524902, 0.24647235870361328, 0.05517292022705078, 0.1024937629699707, 0.1549081802368164, 0.05915379524230957, 0.43860316276550293, 0.15613245964050293, 0.10230517387390137, 0.09600830078125, 0.23235392570495605], 'arima': [0.529914140701294, 0.21784353256225586, 0.344851016998291, 0.3215827941894531, 0.16876935958862305, 0.2640645503997803, 0.6015100479125977, 0.5209426879882812, 0.49585795402526855, 0.35518670082092285, 0.2833704948425293, 0.24057888984680176, 0.34011030197143555, 0.30139708518981934, 0.15755200386047363, 0.37178754806518555, 0.27082085609436035, 0.12413454055786133, 0.15464234352111816, 0.1674971580505371, 0.19551420211791992, 0.10259604454040527, 0.21266555786132812, 0.37183189392089844, 0.16699719429016113, 0.2063436508178711, 0.16777610778808594, 0.22163057327270508, 0.2080674171447754, 0.25160741806030273, 0.1684565544128418, 0.3599684238433838, 0.1315293312072754, 0.22180700302124023, 0.16430139541625977, 0.26268720626831055, 0.09314346313476562, 0.18472981452941895, 0.17637395858764648, 0.23296523094177246, 0.34641218185424805, 0.10345005989074707, 0.2051682472229004, 0.2882652282714844, 0.2132120132446289, 0.4730827808380127, 0.2570791244506836, 0.2645738124847412, 0.2147047519683838, 0.31646060943603516], 'ses': [0.36749267578125, 0.1311476230621338, 0.23856639862060547, 0.219893217086792, 0.017398357391357422, 0.11434054374694824, 0.4722921848297119, 0.4882235527038574, 0.39566993713378906, 0.16642165184020996, 0.15686368942260742, 0.11939239501953125, 0.24445581436157227, 0.22735118865966797, 0.027657270431518555, 0.24699974060058594, 0.10171127319335938, 0.06243777275085449, 0.03375101089477539, 0.02336406707763672, 0.13982152938842773, 0.04303479194641113, 0.07845735549926758, 0.23868370056152344, 0.036295175552368164, 0.10741233825683594, 0.037198543548583984, 0.11669373512268066, 0.05741238594055176, 0.0759737491607666, 0.11008429527282715, 0.2572500705718994, 0.021326541900634766, 0.18534517288208008, 0.046044349670410156, 0.13709783554077148, 0.029239416122436523, 0.027616024017333984, 0.03919172286987305, 0.1587529182434082, 0.24148845672607422, 0.042324066162109375, 0.10357928276062012, 0.16205835342407227, 0.0532991886138916, 0.4156160354614258, 0.15397071838378906, 0.09438157081604004, 0.09052658081054688, 0.2225022315979004], 'holts': [0.3806638717651367, 0.13692712783813477, 0.24576020240783691, 0.21225976943969727, 0.02397608757019043, 0.10914850234985352, 0.616591215133667, 0.4190638065338135, 0.39338254928588867, 0.2785208225250244, 0.15761661529541016, 0.12563729286193848, 0.2555704116821289, 0.23000764846801758, 0.032813310623168945, 0.24595046043395996, 0.11120748519897461, 0.06596565246582031, 0.03854656219482422, 0.02874612808227539, 0.14647650718688965, 0.048333168029785156, 0.0893259048461914, 0.27024030685424805, 0.039322614669799805, 0.11098194122314453, 0.03862452507019043, 0.12201857566833496, 0.06290817260742188, 0.053064823150634766, 0.11451125144958496, 0.26297783851623535, 0.02641582489013672, 0.17533564567565918, 0.05496811866760254, 0.15639567375183105, 0.03359270095825195, 0.03288388252258301, 0.04449319839477539, 0.16824626922607422, 0.2457883358001709, 0.04730820655822754, 0.1103065013885498, 0.15852594375610352, 0.06299257278442383, 0.42105937004089355, 0.16805601119995117, 0.10101032257080078, 0.0947573184967041, 0.23821377754211426], 'Es': [0.5120139122009277, 0.3624858856201172, 0.5635747909545898, 0.3651700019836426, 0.20056843757629395, 0.3247194290161133, 0.7412691116333008, 0.5551121234893799, 0.6290969848632812, 0.3829183578491211, 0.40739989280700684, 0.30243420600891113, 0.4325706958770752, 0.41399097442626953, 0.20853424072265625, 0.45411157608032227, 0.29231858253479004, 0.27640652656555176, 0.281174898147583, 0.22063732147216797, 0.3242037296295166, 0.26512813568115234, 0.28351306915283203, 0.3897998332977295, 0.23927831649780273, 0.27649378776550293, 0.23756718635559082, 0.32427525520324707, 0.28584980964660645, 0.21428966522216797, 0.2704660892486572, 0.4806857109069824, 0.22960519790649414, 0.33057284355163574, 0.27803945541381836, 0.34641075134277344, 0.18921113014221191, 0.22954845428466797, 0.1875462532043457, 0.3180689811706543, 0.4548492431640625, 0.22318434715270996, 0.31528449058532715, 0.3204922676086426, 0.27539944648742676, 0.6384468078613281, 0.3427424430847168, 0.30800819396972656, 0.27725863456726074, 0.4332270622253418], 'auto_ets': [2.2318477630615234, 0.2799873352050781, 0.391862154006958, 0.3376331329345703, 0.17324304580688477, 0.2246851921081543, 0.7042887210845947, 0.568979024887085, 0.5425248146057129, 0.29657816886901855, 0.3561549186706543, 0.29167819023132324, 0.41263628005981445, 0.3848731517791748, 0.14977812767028809, 0.3670926094055176, 0.2754087448120117, 0.29778385162353516, 0.20350956916809082, 0.19590353965759277, 0.25765490531921387, 0.18983006477355957, 0.2306978702545166, 0.323702335357666, 0.21607518196105957, 0.2613997459411621, 0.20286917686462402, 0.25981807708740234, 0.22073674201965332, 0.2232341766357422, 0.23708581924438477, 0.5096900463104248, 0.17818260192871094, 0.3271646499633789, 0.22034955024719238, 0.2764158248901367, 0.17469453811645508, 0.2110154628753662, 0.21491098403930664, 0.331082820892334, 0.4097473621368408, 0.18506979942321777, 0.2201697826385498, 0.3106508255004883, 0.25684523582458496, 0.7339973449707031, 0.2704427242279053, 0.24329829216003418, 0.23409032821655273, 0.3789784908294678], 'theta': [0.3768634796142578, 0.13741254806518555, 0.2752797603607178, 0.22661828994750977, 0.019910812377929688, 0.10886502265930176, 0.45110249519348145, 0.385498046875, 0.3534982204437256, 0.17138171195983887, 0.16118550300598145, 0.11649227142333984, 0.2550950050354004, 0.23968720436096191, 0.03310894966125488, 0.23298072814941406, 0.11044955253601074, 0.0735323429107666, 0.04356074333190918, 0.028809309005737305, 0.14075899124145508, 0.04973554611206055, 0.08272409439086914, 0.21669340133666992, 0.03975081443786621, 0.12409424781799316, 0.04288172721862793, 0.13647937774658203, 0.07000041007995605, 0.05417776107788086, 0.1599419116973877, 0.287860631942749, 0.030282258987426758, 0.17318511009216309, 0.05083036422729492, 0.14094924926757812, 0.037283897399902344, 0.05792593955993652, 0.04481625556945801, 0.18346476554870605, 0.25297117233276367, 0.047823190689086914, 0.11403727531433105, 0.1976778507232666, 0.06230902671813965, 0.4789450168609619, 0.15878009796142578, 0.09994697570800781, 0.09703230857849121, 0.22987008094787598], 'croston': [0.35177159309387207, 0.12813901901245117, 0.2677018642425537, 0.19919419288635254, 0.013990402221679688, 0.10187220573425293, 0.4724884033203125, 0.44417643547058105, 0.3477771282196045, 0.16064095497131348, 0.15661072731018066, 0.10736870765686035, 0.24456262588500977, 0.24424004554748535, 0.02674412727355957, 0.23476505279541016, 0.10480070114135742, 0.06696963310241699, 0.03517484664916992, 0.019505739212036133, 0.13092470169067383, 0.05551290512084961, 0.07781839370727539, 0.24386835098266602, 0.03270459175109863, 0.1057279109954834, 0.03554391860961914, 0.11512327194213867, 0.06026482582092285, 0.040673017501831055, 0.11176156997680664, 0.2899601459503174, 0.022159099578857422, 0.17622756958007812, 0.04053521156311035, 0.15093564987182617, 0.030529022216796875, 0.03527116775512695, 0.03730463981628418, 0.1672060489654541, 0.2753617763519287, 0.04048037528991699, 0.10619401931762695, 0.13924455642700195, 0.05898737907409668, 0.483701229095459, 0.15364456176757812, 0.11191892623901367, 0.08726978302001953, 0.23059391975402832], 'prophet': [2.0086655616760254, 0.32740187644958496, 0.4909214973449707, 0.4141244888305664, 0.23627519607543945, 0.317216157913208, 0.744783878326416, 0.5944795608520508, 0.5765902996063232, 0.36583495140075684, 0.37111902236938477, 0.29901885986328125, 0.45039796829223633, 0.4761190414428711, 0.2727043628692627, 0.49968743324279785, 0.33781886100769043, 0.2790567874908447, 0.24795222282409668, 0.2559692859649658, 0.36496472358703613, 0.24175453186035156, 0.29304003715515137, 0.4215083122253418, 0.2799661159515381, 0.36106371879577637, 0.24893498420715332, 0.3597276210784912, 0.28174638748168945, 0.24500560760498047, 0.34998083114624023, 0.48615241050720215, 0.2391359806060791, 0.38161635398864746, 0.3133232593536377, 0.3429274559020996, 0.2288360595703125, 0.2758979797363281, 0.24872517585754395, 0.4148733615875244, 0.4575059413909912, 0.26099419593811035, 0.32425904273986816, 0.34088683128356934, 0.29781436920166016, 0.7519276142120361, 0.38207435607910156, 0.32239818572998047, 0.2947261333465576, 0.49460601806640625], 'Avg_ensemble': [7.921887636184692, 2.167141914367676, 3.5445001125335693, 2.956850051879883, 0.9029679298400879, 1.879988193511963, 6.266076326370239, 5.209655046463013, 4.909532308578491, 2.6631312370300293, 2.5247955322265625, 1.9423902034759521, 3.4069037437438965, 3.212822675704956, 0.9980447292327881, 3.3296663761138916, 1.957188367843628, 1.4400358200073242, 1.1441314220428467, 1.0281250476837158, 2.128462791442871, 1.1222236156463623, 1.5874340534210205, 3.199042558670044, 1.1563584804534912, 1.8932137489318848, 1.1225190162658691, 2.022099494934082, 1.4279217720031738, 1.2875680923461914, 1.8641855716705322, 3.8100855350494385, 0.9504578113555908, 2.468841075897217, 1.3185005187988281, 2.2409465312957764, 0.9109969139099121, 1.1396336555480957, 1.1300899982452393, 2.4148004055023193, 3.436457633972168, 1.1029834747314453, 1.7900819778442383, 2.348672866821289, 1.447601318359375, 5.757321834564209, 2.358935594558716, 1.8409545421600342, 1.6645374298095703, 3.223761558532715], 'Weighted_ensemble': [1.648076057434082, 0.7635712623596191, 0.9115164279937744, 0.7612643241882324, 0.5535106658935547, 0.745978593826294, 1.0022530555725098, 0.9673140048980713, 0.9541234970092773, 0.7780332565307617, 0.699608325958252, 0.6522059440612793, 0.886620283126831, 0.8572535514831543, 0.6516149044036865, 0.8235747814178467, 0.8611979484558105, 0.7270658016204834, 0.6083676815032959, 0.599388599395752, 0.7410407066345215, 0.6340739727020264, 0.6696004867553711, 0.8757579326629639, 0.5937843322753906, 0.6997272968292236, 0.6855456829071045, 0.7321608066558838, 0.69376540184021, 0.6146159172058105, 0.6534960269927979, 0.8449845314025879, 0.604910135269165, 0.7933354377746582, 0.7447109222412109, 0.7224442958831787, 0.5730156898498535, 0.6787376403808594, 0.5805621147155762, 0.7689535617828369, 0.8466198444366455, 0.670891284942627, 0.6840460300445557, 0.687368631362915, 0.6653957366943359, 1.227017879486084, 0.754462718963623, 0.6857137680053711, 0.6640727519989014, 0.8039159774780273]}\n====================\nAggSmapes is:  {'Lstm': [0.18129113, 0.90916866, 0.5821601, 0.18745206, 1.1922519, 1.0586249, 0.19425587, 0.46773827, 0.2714877, 0.7763762, 0.83045286, 0.6887028, 0.24269068, 0.31713033, 1.1946248, 0.871863, 0.6837027, 1.0746644, 1.0662749, 0.9502467, 0.3115867, 1.1404376, 1.1833847, 0.33553633, 1.3753049, 0.26930195, 1.2465256, 0.57135475, 1.1440444, 0.38509274, 0.50263625, 0.0629457, 1.0728611, 0.16967304, 1.1615742, 0.08780744, 0.66274774, 1.1299752, 0.40319565, 0.10216217, 0.2189459, 1.3576949, 0.94983983, 0.29899696, 0.17806068, 0.08933321, 0.74092144, 0.8524585, 0.48359603, 0.4784595], 'naive': [0.21252172019962912, 0.5565721207116313, 0.39050119095183244, 0.10186137858677434, 1.2186000683772062, 0.8463743832400091, 0.23198100897386656, 0.15441695175957976, 0.2256453596783286, 0.6729894471340678, 0.6295520001797923, 0.27477428228200806, 0.10152043013258623, 0.17182392580495365, 0.9163020675027507, 0.6735517766475763, 0.7814420669801286, 0.6646607672121732, 0.9535486802804213, 0.13935846968321552, 0.2646961802309878, 1.0471990338657393, 0.8508540038237267, 0.11316261255532513, 1.01743572385274, 0.194342058226993, 1.0858279187816826, 0.21851664245024313, 1.073158672870544, 0.33183069782528096, 0.5998768455047623, 0.05922404712959622, 0.9588647539581002, 0.11019016736637702, 1.0783205028401275, 0.12502443060338497, 0.10464376839415822, 1.0092682337418413, 0.19729709000758772, 0.15565215783375494, 0.08342453453026853, 0.5748352219650986, 0.81050151321718, 0.1394010710396559, 0.19697002943978179, 0.06719748181290894, 0.45541129543459724, 0.4477131181627351, 0.1158359861628058, 0.24758216307909425], 'snaive': [0.265844509373314, 0.8480998102260305, 0.4909431510619539, 0.144744057508412, 1.1572915213730997, 1.042231563209731, 0.19971616711296378, 0.16618826344790932, 0.2259992751654281, 0.6932419450899265, 0.8967360759227689, 0.3929343229855592, 0.2266286706531131, 0.43407512689731814, 0.9522250765072829, 0.9016748356941938, 0.7476061583426569, 0.8009248590773276, 0.972015476580785, 0.19079175546333282, 0.3253278403614479, 1.0891310587486498, 0.9838528353414794, 0.29533566579683523, 1.119613910638508, 0.21698866249716373, 1.1065985823299114, 0.32450096529037237, 1.0872319164949282, 0.42310420821024225, 0.5984141306600568, 0.08090657963382954, 1.003562647980043, 0.14579245974313612, 1.2091725765553047, 0.13080834843390773, 0.40217197157934836, 1.0297622552068528, 0.4290530714649161, 0.13742773777679068, 0.20336880410831149, 0.9300100570416545, 0.8607794714051615, 0.18651364264442347, 0.21295553113333912, 0.09442447709875099, 0.7460823961751044, 0.5855751111718586, 0.18526645078743958, 0.2932728778292932], 'arima': [0.24948459944882329, 0.5362095268380317, 0.38196763126791766, 0.315123016256758, 1.0278828981897714, 0.9869896709372961, 0.1603403965294705, 0.1316055562410161, 0.17728343949559772, 0.6923637471127183, 0.790100598900663, 0.29026686389073925, 0.1634377922086165, 0.24773756420482262, 0.9385081974873556, 0.8770502292087833, 0.7086847471118378, 0.6017188224447813, 0.9423593207290202, 0.1435494849252576, 0.30413067070160554, 1.0086570660617262, 0.930779764918116, 0.1786795266249937, 1.0805543358035412, 0.16091317518739534, 1.1034101693731158, 0.2320647140527307, 1.081950497112075, 0.39001216263428845, 0.6122793579982847, 0.10919328092495209, 0.9834175373822165, 0.10149153279939582, 1.072495340193851, 0.09472086415390606, 0.3802983349202505, 1.0469430818361505, 0.4806017431287891, 0.17406981408314656, 0.16510661385295428, 0.56047850356807, 0.8320386901024337, 0.14149729140145564, 0.23976958473791316, 0.07569478000768745, 0.4882272450638996, 0.5893241032035061, 0.07989669375407618, 0.2494368257800647], 'ses': [0.23539730445887952, 0.5565721234990914, 0.39050119221935814, 0.11342654790348067, 1.0434969892426698, 0.8497641211478432, 0.20455982197770212, 0.1474943175666806, 0.22355597413032033, 0.675990482730429, 0.6295520024395638, 0.29579808716778405, 0.13427414505736873, 0.23285481928496843, 0.9183315402558294, 0.7476441041162742, 0.7958605596641153, 0.6646607686258393, 0.9536298055966795, 0.13912188098241496, 0.2942881396248578, 1.0471990352878648, 0.8638600956868676, 0.13753272632296326, 1.0174828519767736, 0.17099379360406167, 1.08774466497616, 0.21654055409497167, 1.079776138584684, 0.3589187102179334, 0.6053955682105653, 0.060190035297996394, 0.9608878424904832, 0.11322604887735141, 1.0783205053010532, 0.11943240936217513, 0.252508415670237, 1.0092682348106667, 0.45656743334976146, 0.15521186973217044, 0.08842082937279393, 0.5896306731441303, 0.8340144788288802, 0.12910681919662356, 0.2410182589138468, 0.06719748170957195, 0.4584451816235865, 0.4559074602186465, 0.07497622101717077, 0.25773537578245365], 'holts': [0.19772218196607272, 0.5239522443855535, 0.3805694223419267, 0.10087471200474557, 1.3757602207751543, 0.8383207712765405, 0.23473615402620382, 0.1492654248720615, 0.2207370897061902, 0.677128900119516, 0.5984816893290569, 0.24767160658577594, 0.09939068656345006, 0.15611578649267407, 0.9171871591717274, 0.6665676963450874, 0.8092060001546316, 0.6658198620476141, 0.9639899414689833, 0.1375613279966537, 0.2712163389227654, 1.0555840434720494, 0.8434977171286978, 0.10330157785136068, 1.0160598772586458, 0.18518002989535132, 1.0956860351593776, 0.2172007444786307, 1.0848959784230443, 0.33370361456064224, 0.6071181689502398, 0.05927239332946563, 0.9601457948627328, 0.10818972824704053, 1.0777841723172006, 0.14166821464335944, 0.1101555277451131, 1.0123289136430103, 0.1948160874116323, 0.16055541824364847, 0.0789174649954656, 0.5299485444659013, 0.8203730980821602, 0.16020410098093277, 0.21417957616875555, 0.06869334499356827, 0.4221965240149714, 0.43517879229575424, 0.14369094845096597, 0.2340437701522161], 'Es': [0.2644804521918725, 0.6055466206510249, 0.48688869337422613, 0.13298268326231538, 1.5624745660620467, 0.9148144287049672, 0.2841645349248917, 0.16700782942590586, 0.2712189506937098, 0.6813779829196471, 0.7219280433461027, 0.25547127570646677, 0.16096838093491095, 0.8090942252500191, 0.9516229188080897, 0.7432587665492753, 0.8518551418797985, 1.3484403067823447, 0.9673693526605404, 0.14762119362750922, 0.7563295288400584, 1.038890435022785, 0.8895188940561511, 0.2070401428476162, 1.0509725932989356, 0.21127437143800434, 1.1388297363394113, 0.2617410648471296, 1.1031241692591187, 0.48540422245781667, 1.2391249891060847, 0.23076063589973614, 1.0100057475688493, 0.6761242328563093, 1.0803524920797887, 0.1801571935430494, 0.22473764317052655, 1.0836931309455176, 1.2836343602128275, 1.447213506558907, 0.10519990411764522, 0.591092054493413, 0.8145691380372214, 0.49848582584540274, 0.3384282448862793, 0.10621635140499029, 0.6960336532582906, 0.469898939259863, 0.1128758863351308, 0.46280753075990494], 'auto_ets': [0.23539972435072049, 0.5565908276646399, 0.3905096974325686, 0.11342879286981686, 1.0434974744439218, 0.8497587216100692, 0.20455945368550388, 0.14749390569988027, 0.2235480472410708, 0.6759910299592845, 0.6295671664290745, 0.2958516563770737, 0.13427480464427957, 0.2329812860434263, 0.9183314057229333, 0.7476755817125234, 0.7958481682092343, 0.558202293690026, 0.9536298201394764, 0.1391216538036199, 0.2942887396144469, 1.0472085776482054, 0.8638608539031967, 0.13749744228946986, 1.0174904741726354, 0.17099338046336485, 1.0877457131041965, 0.21653337474571807, 1.0797774138813474, 0.35891906007343544, 0.6053911788340477, 0.06019004811995552, 0.9608876976442071, 0.11322664451625628, 1.0783370180095122, 0.11940408053544478, 0.252512190027337, 1.0092754063585656, 0.4566742386867865, 0.17593034080477307, 0.08840422414684526, 0.5896317964951248, 0.8340163037195117, 0.12910253228267807, 0.24099698536204597, 0.06719678834613027, 0.45844518958054203, 0.45590767877831445, 0.07497099271174272, 0.2577380753539293], 'theta': [0.22931894854050106, 0.5598465357620674, 0.3913674209171042, 0.19109049043443274, 1.0799703458044216, 0.8505142823706251, 0.17616232083252348, 0.13426895978190953, 0.18898868864619353, 0.6761483517083106, 0.6326784613504678, 0.2884069117190292, 0.13166905027023965, 0.27252861746087986, 0.9172406054629685, 0.7654391170417463, 0.7879928044996312, 0.7432442965272672, 0.9490440957920934, 0.13923005556766327, 0.3544740864898144, 1.056946348933562, 0.8583555475250579, 0.14255025134593463, 1.0133043221507214, 0.1647239006130476, 1.0853015769880945, 0.23362242754855633, 1.0791071303914543, 0.3610285319531702, 0.6445621685533086, 0.06938628540285285, 0.9586606862484717, 0.1260023414269448, 1.0857351091704552, 0.12551330774480765, 0.4789907554543068, 1.0088201318041823, 0.32785466993342094, 0.24993455121118874, 0.09498507882307593, 0.5855728223335478, 0.8299888629149919, 0.20227498674070346, 0.25893899577106916, 0.06890093368677247, 0.4620391269330087, 0.4541569011263905, 0.07220237578299636, 0.21305229779682033], 'croston': [0.24289143578204803, 0.8024861463476122, 0.4783558451442646, 0.12418670890780799, 1.0838616532342051, 1.0190972990886065, 0.19313502706620125, 0.1557891852714174, 0.2213330186396507, 0.6947182804387467, 0.8378868909337469, 0.3523615809338818, 0.20002401717175586, 0.36897415221840285, 0.9495842128207675, 0.8686122685262828, 0.759169982528058, 0.7388087730490802, 0.9814485910229439, 0.14969380032316704, 0.2983766507528491, 1.092425750386075, 0.962073325714402, 0.2600423964090095, 1.114160550929407, 0.17787094949462148, 1.1211662008300354, 0.25026595702582066, 1.0980041805974805, 0.41825050824454296, 0.599362283376992, 0.06424692038512807, 1.0028660264751599, 0.11564793979244334, 1.1932135824321133, 0.09936446629156633, 0.32597583028218385, 1.0471528630465372, 0.4237622166300841, 0.13915164115218057, 0.1672160666259106, 0.8177742273985344, 0.8635949929565239, 0.11435974561234886, 0.22840612617168632, 0.06375465669211192, 0.7037588388199265, 0.5664821963829683, 0.12404125203573006, 0.2754131011550436], 'prophet': [0.4323548245964108, 0.8998609160213495, 0.5369646396344389, 1.4363329278859898, 1.352567015622278, 1.1121538855725168, 0.18584706992613545, 0.11223672404396738, 0.2862789953758573, 0.8313831776957417, 0.8777632989307486, 0.14359686040447506, 0.2049102355808559, 0.5782631165695153, 0.9868580502280598, 0.9633469306481123, 0.9387602640423478, 0.40250461058662745, 1.048625205367869, 0.13768716069094794, 0.30846147025918075, 1.2226960593336738, 1.004236010874466, 0.3501134501368457, 1.114789074856161, 0.16380485089704455, 1.2001703197501201, 0.23234348195043927, 1.2609117192716548, 0.5283067615291891, 0.6380348022125394, 0.17903423423801998, 1.0863317713362777, 0.2802050826608888, 1.4229259248536743, 0.12766938452524743, 0.9420756638641405, 1.119982957294295, 1.3766751832010626, 0.5349747874764457, 0.1587462041577464, 0.7783577140439274, 0.9769257294480765, 0.6416628271437599, 0.16177209317602687, 0.054895929126552785, 0.6855061155685798, 0.6716434979853161, 0.9390215776641271, 0.3487183766489712], 'Avg_ensemble': [0.1992796716462833, 0.6261542479574318, 0.42725947237424045, 0.21005695403883395, 1.107831578143087, 0.9273057933876124, 0.20548011424002455, 0.14362388095192047, 0.22480758552887012, 0.6944091112554848, 0.7141842547404706, 0.26482367200343315, 0.14914234399075368, 0.3005889198923497, 0.9346075190655064, 0.79159825813785, 0.7946206451400395, 0.5423828709772234, 0.9662237738798564, 0.140722342379794, 0.3310005257369015, 1.0665637140342692, 0.903675058044002, 0.17415259066485353, 1.0527414313366223, 0.17170130253233365, 1.1100223413985228, 0.19617696521111058, 1.1016637055604979, 0.39236628623206976, 0.6613238998465121, 0.08372921600260863, 0.987166197722607, 0.11114389787490349, 1.1318835531608402, 0.12080741982253651, 0.17537463758579183, 1.0363306061790198, 0.6084682346408792, 0.2582341070028556, 0.11384346517774921, 0.6280293640274605, 0.8449453695900008, 0.12150220474033772, 0.22993836294815054, 0.06890572279702616, 0.5496870085287572, 0.5062825471137286, 0.08905722166243164, 0.2871503141832605], 'Weighted_ensemble': [0.28827886413306447, 0.9245924825859158, 0.4916720081907057, 0.10491871856019085, 1.3628101346908543, 1.0327833419877457, 0.19831401297122514, 0.24372517882588304, 0.19591991988027685, 0.6965159981878803, 0.8739523349349307, 0.48688205856001787, 0.13046015991284515, 0.16312952893943997, 0.9767827022882405, 0.888477342359337, 0.6260147253399441, 1.0649951326500606, 1.0299764548237549, 0.6492944741543167, 0.2575635223431601, 1.2456273410666845, 0.9757618767125341, 0.14632382636663527, 1.1707907938174364, 0.28057731966273075, 1.1389255101608706, 0.443229292760697, 1.103629070325213, 0.4375784697117883, 0.5223783771951563, 0.057270289206168674, 1.0627920297569737, 0.25043957011100615, 1.115850143120594, 0.07941253677958224, 1.5111316949173428, 1.078589640019862, 1.2380069960932016, 0.14818696281160898, 0.18429974936778418, 1.2807929943691498, 0.7970611684501532, 0.14298407815470016, 0.22270282078253562, 0.06405568084177563, 0.8456873646929635, 0.6154832742564305, 0.1614670508020839, 0.36853398360035666]}\n====================\nAggWapes is:  {'Lstm': [0.18456796, 0.670097, 0.47697294, 0.17538524, 0.9371351, 0.7469659, 0.19470479, 0.59905845, 0.28974026, 0.60849524, 0.63056076, 1.078634, 0.22097763, 0.28792307, 0.80941963, 0.64835507, 0.5641842, 0.71890193, 0.76425016, 0.6501283, 0.2863996, 0.81314826, 0.7910785, 0.29200906, 0.8715908, 0.29657945, 0.84299374, 0.7692821, 0.806579, 0.3443419, 0.43840066, 0.062052134, 0.77097, 0.17710644, 0.8277543, 0.09381598, 0.44848564, 0.7909105, 0.5197639, 0.10075865, 0.20104514, 0.8395829, 0.7068073, 0.33554754, 0.17462328, 0.08691503, 0.5711519, 0.62494296, 0.3820075, 0.5917092], 'naive': [0.22006850221447702, 0.4861052266774568, 0.3554335273504585, 0.10149507756940754, 0.9437676989074497, 0.6594527229845628, 0.24302899912123868, 0.15737355476707635, 0.2333314724873928, 0.5552146513443807, 0.5262358571144418, 0.281190769040848, 0.0999689566228106, 0.16940052626663105, 0.7041635918479947, 0.5494137758153077, 0.6139235504590407, 0.524846356453029, 0.7180495303412139, 0.13892106441798327, 0.25298990465824417, 0.7733426634070162, 0.6617747750215139, 0.11168383674419974, 0.7575162070695205, 0.2012717773274083, 0.7923221246779112, 0.22778536281269327, 0.7796016668515202, 0.30468482845092903, 0.4977547335771493, 0.0582081344617678, 0.7246948536445379, 0.1090566780003934, 0.7904082396471429, 0.12748132423717004, 0.10415261017524066, 0.7484280486369395, 0.30786811442962, 0.14970846195305992, 0.0828003439273808, 0.4915795646340342, 0.6435786503644485, 0.13787740949896157, 0.1898346601347214, 0.06561922365988909, 0.41044351538780904, 0.40457155020754165, 0.11887884706996833, 0.25752725104662166], 'snaive': [0.28987203520951804, 0.6358971520953977, 0.4213279308553409, 0.1403572874750082, 0.923534303901118, 0.7390356989708405, 0.20515741947213725, 0.1717280736073823, 0.23637860771506725, 0.5653076945322996, 0.656661635485041, 0.4495628858715862, 0.20697166183139423, 0.36679399270183494, 0.7188454774302563, 0.6587562140971664, 0.596720087420734, 0.5927480245829675, 0.7252182010331755, 0.18174798066379405, 0.297950990232319, 0.7881062711843494, 0.7169199101095091, 0.26002881710956255, 0.7933876426253865, 0.23070486956845096, 0.7995808350356783, 0.3570576269645556, 0.7845275757591762, 0.36799270489000346, 0.49644306590744247, 0.07878858715125223, 0.7419686244499714, 0.14848118695029086, 0.8311042889561631, 0.13375249145209123, 0.3337831410589654, 0.7563128407212266, 0.9759894436347016, 0.13252976627958254, 0.18765494464810586, 0.6674796647481559, 0.6658876087467671, 0.19396144801661547, 0.20136762604613187, 0.09512389152764454, 0.5734211969567795, 0.4886942512328326, 0.17452972483050164, 0.32199655230515983], 'arima': [0.26415798150156905, 0.47358889586425407, 0.3493543252017374, 0.2742547677751517, 0.8967534698674939, 0.7196472116588183, 0.16123588938085037, 0.13233805845733732, 0.17995844840898229, 0.565465812275208, 0.6110788922972894, 0.300233832280447, 0.15546735822258623, 0.23498329545537353, 0.7138214233248266, 0.6502462885165973, 0.5768168169008482, 0.48966646944397296, 0.7126098707808329, 0.14318006874166625, 0.2834687885916121, 0.7579719481789365, 0.6961015910802566, 0.16976669950465226, 0.7803867910214627, 0.1623047851299219, 0.7980058211698079, 0.2413061692309003, 0.7824787809685259, 0.34788942015372376, 0.5048969550219495, 0.10502867640513247, 0.7340612392111918, 0.1001603892595655, 0.7884859478223707, 0.10004608547309549, 0.3247513293256458, 0.7626398750620613, 0.6044137810530527, 0.1656782504606935, 0.15652204231959968, 0.4824716297242136, 0.6533426390934822, 0.13974100531616898, 0.225028187100823, 0.0746682466990733, 0.4308564548346523, 0.49246694287158777, 0.07917413356672787, 0.2598567498856161], 'ses': [0.24698637837135476, 0.4861052283548781, 0.35543352824159824, 0.11242727051319086, 0.9019244012235742, 0.6609478251458988, 0.21013390960659017, 0.14962211050320692, 0.23089835565984243, 0.5568336020977812, 0.5262358583841622, 0.3071247887093456, 0.12993847461772073, 0.2229363549330617, 0.7050768493155662, 0.5873114883040818, 0.620761555898814, 0.5248463572222901, 0.7180838465584403, 0.13869451821308398, 0.27597240951108937, 0.7733426639225439, 0.6674734356947026, 0.13409667577032747, 0.7575338923391328, 0.1737399363570297, 0.7929836073530333, 0.22583343755839638, 0.781862855677709, 0.3252317791645038, 0.5008739376636792, 0.05916609643743238, 0.7255072385811494, 0.11229085676300259, 0.7904082404632311, 0.12250600899807938, 0.23087297718109895, 0.7484280490466972, 0.573408870983588, 0.1493204007997094, 0.08757968667102378, 0.500774474776102, 0.6544652907874741, 0.12842146659140552, 0.22596618569953855, 0.06561922355049865, 0.4124002311973376, 0.41006884262431637, 0.07459402176179897, 0.2700613687628525], 'holts': [0.2032848962656321, 0.46591210898932867, 0.3484171546053818, 0.10054067644711592, 0.9617566219521599, 0.654934617741356, 0.24635314349120274, 0.1515948538293994, 0.2276417772404783, 0.5574115230458989, 0.5082196382720646, 0.2502882501666825, 0.09792579975287469, 0.15449088129850994, 0.7045347138286081, 0.5449857609641147, 0.6272053876468684, 0.5253597457588361, 0.7223494435825525, 0.13719152875485632, 0.2581685519703894, 0.7761433587247669, 0.6579182946100764, 0.10206285100444615, 0.7564364200087235, 0.19033863324684158, 0.7955189205539752, 0.2272067171040696, 0.7834669872147931, 0.30625097852203453, 0.5018594755894888, 0.05825850861281338, 0.7249958471524414, 0.10702446287828739, 0.7893442708090816, 0.14208386710132737, 0.1096335358762107, 0.7493924701767889, 0.30826017875673006, 0.15401325144460384, 0.07839505565376523, 0.4630356117880853, 0.6480216420011469, 0.15637637273947078, 0.20416403150884174, 0.06720219044735148, 0.3883563456782292, 0.39606035201658923, 0.15064942783098048, 0.24122712866938262], 'Es': [0.30469323503206225, 0.5144829632828607, 0.4185525371335497, 0.12705616810847595, 0.9494819824733596, 0.6889135658330788, 0.3058076289839537, 0.17022025318565417, 0.2887438499440041, 0.5586898546791157, 0.5751133327617951, 0.25660075028395146, 0.15282395819811853, 0.5761323360264008, 0.7176109096256863, 0.5826912832311066, 0.6445394014252613, 0.7912057467779844, 0.7217256059668246, 0.1475422155831797, 0.5559025811440874, 0.7599242439002023, 0.6780548691565341, 0.19246308993079217, 0.7705770590597901, 0.21604751157265117, 0.809725469991556, 0.2592697997828147, 0.7873701767974345, 0.41121306397401214, 0.7827446032083512, 0.20649222909838738, 0.7425799689989513, 0.4821633990813575, 0.7915708755330622, 0.17403871628156486, 0.218368746352139, 0.775218761860623, 1.3666453717452092, 0.8173890936446659, 0.10477292314939325, 0.5014463700734736, 0.6446720561452827, 0.38938824009631573, 0.2984174904140252, 0.11069216591235576, 0.5499247896370453, 0.4175173861105967, 0.11727468383404745, 0.6057019882462739], 'auto_ets': [0.24698929234277697, 0.48611648393747964, 0.35543950784418327, 0.11242934795211082, 0.9019246514110423, 0.6609454468221437, 0.21013348047863617, 0.14962165259167282, 0.2308891510987348, 0.5568338969212754, 0.5262443786705888, 0.30719275534920215, 0.12993906258695664, 0.2230397673399644, 0.705076788827936, 0.5873271236073148, 0.6207557124881686, 0.46354236700902895, 0.7180838527098709, 0.13869430059221777, 0.2759728641871394, 0.7733461230621012, 0.6674737656275986, 0.13406518977514711, 0.7575367525718948, 0.17373946077456243, 0.7929839687595647, 0.2258259672341273, 0.781863290215033, 0.32523203955350866, 0.5008714622704622, 0.05916610912678826, 0.7255071804668817, 0.11229148982757389, 0.7904137163399871, 0.12248071459914196, 0.23087595910215544, 0.7484307984103123, 0.5735435102847865, 0.16733445588572243, 0.08756390436920558, 0.5007751677579438, 0.6544661211307756, 0.12841749096696967, 0.2259491523682247, 0.06561848957084565, 0.41240023632187167, 0.41006898858262064, 0.07458860396535186, 0.2700647368426993], 'theta': [0.23971408578920017, 0.48813379867262513, 0.3560476332487528, 0.1795764829966774, 0.9121030176241771, 0.6613137101035103, 0.17818249724605498, 0.13519589636214005, 0.19233774011765115, 0.556922523856741, 0.5280601641638872, 0.2978838136241959, 0.12760650141227992, 0.2543889174652922, 0.7045585698238623, 0.5967102565724755, 0.6167323459388224, 0.568045096465767, 0.7159755682474215, 0.13880683026925503, 0.3204356178718692, 0.7774309444535712, 0.6648105057952753, 0.13855977456670923, 0.7557287435858645, 0.1666000278861393, 0.7919884455625973, 0.24288876104362395, 0.7815901073666941, 0.3268017751119755, 0.5234225139168627, 0.06810983050046764, 0.7245021897317999, 0.12256186031784853, 0.7933842177894215, 0.1279099068043333, 0.38890020321721885, 0.7482323936424377, 0.4298936880082968, 0.22710382901156492, 0.09377990294382069, 0.49826586423088465, 0.652467623033591, 0.19073571605284895, 0.24024645921405924, 0.06742116936600083, 0.41478161262833524, 0.4088820919402498, 0.07172873513040513, 0.2167693616986866], 'croston': [0.25607655188237094, 0.6159096438209147, 0.4138292966675786, 0.12226293994610239, 0.9146143839890484, 0.7307422966089661, 0.19700453118390707, 0.1589258777794682, 0.22832487343455393, 0.5668414858413965, 0.6316528927271718, 0.38253184469479606, 0.18647876987476292, 0.3253769104333008, 0.7182516131390695, 0.6446780866457085, 0.6031502549806519, 0.5640631316763001, 0.7293041604715892, 0.14861931718187954, 0.27906397378076514, 0.7891163792129364, 0.7085845925089004, 0.23605564088571304, 0.791543000002914, 0.181727667935322, 0.8040079850604357, 0.2607879913692019, 0.7880256678793605, 0.36776802220975363, 0.4974631939517648, 0.06315505104716525, 0.7417051539556074, 0.11487526045758858, 0.8266417465836334, 0.10434150054394704, 0.286965477016915, 0.7623791494020414, 0.5331489030011314, 0.1350123075927588, 0.15848852080308468, 0.6209009012462582, 0.6675087034014536, 0.11444335364650651, 0.21580279919683837, 0.06199075960103703, 0.5536329360249301, 0.47900759927038916, 0.12036284966074827, 0.2925160838291648], 'prophet': [0.3553127667352121, 0.6586988656809314, 0.44889571582713644, 0.922507437057572, 0.9247406617726414, 0.7661992034100513, 0.18993276483354485, 0.11192394236493576, 0.3091769712149061, 0.6352334178609202, 0.6479318957788376, 0.12189172861389147, 0.18921377999482974, 0.4543783366935593, 0.7352656227916784, 0.6877680292853741, 0.6873331214067471, 0.5113756108555092, 0.7536473628485832, 0.1372876682200406, 0.2789373610854592, 0.8347969862079929, 0.7252377723736351, 0.302399506692553, 0.7907483794338851, 0.1616911556829247, 0.8301627113945894, 0.2041154295090477, 0.8440172117878726, 0.43832281937202433, 0.517892578523913, 0.16033208535888369, 0.7706045385271123, 0.29727841732527027, 0.8878470146813162, 0.12884526709256205, 2.234851412928114, 0.7856080399047586, 4.176035977438925, 0.41955041406880333, 0.1490916731642208, 0.5996038984585136, 0.7176087669323019, 0.992907703599222, 0.15682068096357915, 0.05311733457523316, 0.5391726174787264, 0.5361783754576556, 0.6005807109282872, 0.39527226319470826], 'Avg_ensemble': [0.2050472806257177, 0.5261341488196645, 0.3804317383375465, 0.19510846324061004, 0.9112191738240752, 0.6942132299279227, 0.21119474528293755, 0.14539293925010993, 0.23239576084546992, 0.5666888913426296, 0.5715833529228825, 0.2692297241342489, 0.14303789949129833, 0.27549155529291225, 0.7122441961480696, 0.6089888307039247, 0.6202446247607386, 0.45231242238481667, 0.7229807210187564, 0.14019911597450035, 0.3029553845466341, 0.7798714443388826, 0.6844317799489191, 0.1658647539526824, 0.770566551661902, 0.17455443635577647, 0.8006015833944353, 0.20579909267358876, 0.7894804320518121, 0.34941832277522555, 0.5323685751177819, 0.08168481267997961, 0.735438594330707, 0.10967919534836208, 0.8077893145126351, 0.12371233759401229, 0.1772670117071157, 0.7583733229995041, 0.8259245269211088, 0.2343019812270005, 0.11122859818816323, 0.5235546554664287, 0.6595680319401422, 0.12177139244626509, 0.21685750185158167, 0.06751440026291583, 0.4681171946309166, 0.44220409568507957, 0.08793241673193308, 0.307935128122516], 'Weighted_ensemble': [0.313683676042181, 0.6706249171120799, 0.4218310675140601, 0.10461872759043284, 0.9580402481913364, 0.7352010676083606, 0.2034847678914447, 0.26616490260704173, 0.20021371800376472, 0.5677559128194269, 0.647462055069697, 0.5931335273163543, 0.12648724918374646, 0.16277602196370622, 0.7288547147277809, 0.6535636990288108, 0.531340165733581, 0.7141477211412626, 0.7464179812483187, 0.4988760654789852, 0.2472473497674977, 0.8377537341662735, 0.7133376759194153, 0.14182503105013722, 0.8090105797009731, 0.3110066885432195, 0.8084967691569571, 0.519444100184081, 0.7892437521484023, 0.38074369738612723, 0.45124410366146156, 0.056224990524852886, 0.7626939132528536, 0.2787469399350132, 0.8011664116809729, 0.08557267058436821, 0.909070781598841, 0.7723895793281701, 0.820199504168961, 0.14505205268048604, 0.17301445180206612, 0.8057118431753509, 0.6369811614475108, 0.14708032205632926, 0.21112535441382219, 0.06226853150863727, 0.6226387504662106, 0.506906285667658, 0.15291848098789726, 0.4228715549067745]}\n====================\nAggPes is:  {'Lstm': [0.10312165, 0.670097, 0.47115707, 0.14382638, 0.9202667, 0.74696594, 0.08357436, 0.5990582, 0.22333132, 0.60849524, 0.6305607, 1.0301983, 0.21711935, 0.2879231, 0.80941963, 0.64835495, 0.5619536, 0.71890193, 0.7642501, 0.6501283, 0.28375876, 0.8117135, 0.7910785, 0.29200912, 0.8715908, 0.2799835, 0.84299374, 0.709856, 0.806579, 0.3293821, 0.43840066, 0.02445737, 0.77097005, 0.16508377, 0.82384235, 0.019783624, 0.44681606, 0.7909105, 0.013681516, 0.062245592, 0.19688718, 0.8395828, 0.7068073, 0.24995025, 0.065663815, 0.042339843, 0.571152, 0.6249429, 0.3820075, 0.5127261], 'naive': [0.12851250531398772, 0.47911950929417946, 0.3484184681360789, 0.03969895983600334, 0.9435800333467266, 0.6594527229845626, 0.19741826005272567, 0.11139592299286322, 0.13660470049684587, 0.5520834154921246, 0.5220962476745411, 0.13461489244147096, 0.06077798209797537, 0.07122313511911856, 0.7016448843063554, 0.5494137758153077, 0.6139235504590406, 0.524846356453029, 0.7172439786861379, 0.0575916230366491, 0.23858412878951937, 0.7727292756913391, 0.6617430625327035, 0.053202220629065995, 0.7565606090429416, 0.17695182492609188, 0.7920078456170377, 0.08414085545124103, 0.7796016668515203, 0.26684144534366805, 0.4977547335771492, 0.003607950136328015, 0.7242777884063516, 0.008233343636721275, 0.790408239647143, 0.12300886389784642, 0.018571159950889846, 0.7481188411042222, 0.17437839049927123, 0.1262678230192562, 0.03901015346980861, 0.47335027994874807, 0.6424406301434281, 0.1014263890557682, 0.17330067360685855, 0.05018250310021292, 0.4096370927095631, 0.39052257915067584, 0.11174810913960222, 0.1836221533835366], 'snaive': [0.2166145454165211, 0.6358971520953977, 0.41882467977247384, 0.1108511755624731, 0.8446858967727138, 0.7390356989708404, 0.10718463076876458, 0.13697502799400996, 0.1133859952734513, 0.5587928310210563, 0.656661635485041, 0.3869382138765482, 0.1860704963124387, 0.3523168292596517, 0.7166716093658402, 0.6587562140971664, 0.5932973803755712, 0.5927480245829675, 0.7230823805899003, 0.144743548162906, 0.27282656387408033, 0.7881062711843494, 0.716919910109509, 0.24963557720502097, 0.7933876426253865, 0.17659855344625625, 0.7995808350356782, 0.12101711259637098, 0.7845275757591762, 0.34542345833808275, 0.4964430659074426, 0.031933137000553745, 0.7419686244499712, 0.05996571781830444, 0.831104288956163, 0.0679295425450892, 0.32290664277523995, 0.7563128407212266, 0.5213585283184848, 0.08270860894703318, 0.17661743614939734, 0.667479664748156, 0.6645802962614625, 0.040968777356288295, 0.17462747499489692, 0.057447530358688784, 0.5734211969567795, 0.4835897082730743, 0.13781133847390742, 0.262841367153578], 'arima': [0.19635742397407827, 0.4639545374411935, 0.341785796087804, 0.27425476777515173, 0.8647052417852635, 0.7196472116588181, 0.07309477232343818, 0.07209623134043258, 0.061355852452172496, 0.5636244708635199, 0.6110788922972895, 0.21122198572585532, 0.1456035020307027, 0.22662131429023757, 0.7123563918252738, 0.6502462885165973, 0.5749944256762812, 0.48920242512249573, 0.7118617908797282, 0.022639659045073222, 0.28098547230995813, 0.7556163664490851, 0.6961015910802565, 0.16264904113399178, 0.7803867910214627, 0.11925255141733027, 0.7980058211698079, 0.03407382522337485, 0.7824787809685259, 0.32817019347103465, 0.5048969550219495, 0.08665854923293118, 0.7340612392111919, 0.004871966077454605, 0.7882452427785321, 0.08205633563974137, 0.32475132932564604, 0.7626398750620613, 0.24862010239963683, 0.14639129242730453, 0.14951530343296374, 0.46574289335795266, 0.6532631082891788, 0.10724155501411987, 0.21441613550790914, 0.06567428043626838, 0.4308564548346523, 0.4901176232992772, 0.034617589281076847, 0.1999537433571937], 'ses': [0.17347899734098793, 0.4791195113909562, 0.3484184693512696, 0.08413615391059326, 0.872743570171428, 0.6609478251458987, 0.13617525557193577, 0.09444570837760174, 0.13139087872352398, 0.5539514355922022, 0.5220962491396031, 0.19898835761695852, 0.11538016179951062, 0.20891607489482408, 0.7027864561408196, 0.5873114883040819, 0.620761555898814, 0.52484635722229, 0.717283574321399, 0.05645889201215301, 0.2727403185407525, 0.7727292762436901, 0.6674734356947024, 0.11703576998664807, 0.7565795575460974, 0.1267204054290969, 0.7927165770546686, 0.0679143696566793, 0.7818628556777091, 0.299234879507043, 0.5008739376636788, 0.008397760014651028, 0.7251482008384353, 0.04435534319062787, 0.7904082404632311, 0.11622434311726809, 0.22595405712570435, 0.7481188415432486, 0.2091866378933417, 0.1257827465775682, 0.05311448303544815, 0.4871426451618501, 0.6543971184338266, 0.08116365425386247, 0.2171106372749636, 0.05018250277204151, 0.4118948417205579, 0.3980359615797938, 0.03230809908217437, 0.20451234957725534], 'holts': [0.10455375972779779, 0.4587020551481119, 0.34208369278522366, 0.04117024122916366, 0.9617566219521599, 0.6549346177413561, 0.19530506388066105, 0.09930062610078627, 0.12542664872377943, 0.5549148038137621, 0.5043895760411513, 0.08264808318493004, 0.056244231679805086, 0.04903472210504254, 0.7026525722041322, 0.5449857609641148, 0.6272053876468686, 0.525359745758836, 0.7223494435825527, 0.04654064179989432, 0.2487977666577173, 0.7758659580926752, 0.6579182946100766, 0.02259725169306447, 0.7557713415659016, 0.15840613892666927, 0.7955189205539752, 0.13882570803302102, 0.7834669872147932, 0.2763662410623701, 0.5018594755894888, 0.001148194975781864, 0.7247657447428225, 0.01146303890220986, 0.7893442708090815, 0.1407969417238252, 0.005708718701309191, 0.7492951210197164, 0.29087478147216483, 0.13181753046299752, 0.01671627208649974, 0.43987747146269296, 0.6477180634568241, 0.1359983034401705, 0.19227839076198486, 0.05286200581351054, 0.38799562883715466, 0.3830639257539274, 0.14366825225550556, 0.16109056942363217], 'Es': [0.1892428702488564, 0.5092391066058405, 0.41179263373973424, 0.08179365663709397, 0.8978349023890126, 0.6889135658330788, 0.1768857662292957, 0.04798912880282618, 0.18691895046139773, 0.5552634438847524, 0.573846406629228, 0.03043041551035347, 0.14395321134527567, 0.5761323360264008, 0.7158307451209548, 0.5826912832311065, 0.6445394014252613, 0.7675249279563875, 0.7204552747621481, 0.014675033122912011, 0.5510804319563333, 0.7584138789332984, 0.6780548691565342, 0.18945020558097786, 0.769360819899021, 0.06411702470658569, 0.8095931733829624, 0.24186744542788394, 0.7873701767974346, 0.39418634221017884, 0.7822078347551062, 0.19893667111670216, 0.7425799689989512, 0.4293954995824332, 0.7900961670778411, 0.17195417577244526, 0.021541554358299127, 0.7752187618606229, 0.0041578664079790755, 0.8173890936446659, 0.029262976441568948, 0.4896228555057784, 0.6414005675502074, 0.3893882400963159, 0.29663591967699626, 0.09054189715783587, 0.5483205513606948, 0.40590863562418444, 0.10129607112932006, 0.5183549208665356], 'auto_ets': [0.17348385396002491, 0.47913358086920815, 0.34842662335479474, 0.08414134750789301, 0.8727441956400989, 0.6609454468221436, 0.13617433601203444, 0.09444456359876745, 0.13137115466400753, 0.5539517757731568, 0.5221060802393259, 0.1991340004166506, 0.11538096357574205, 0.20908842890632853, 0.702786380531282, 0.5873271236073148, 0.6207557124881686, 0.46354236700902895, 0.7172835814192035, 0.05645780390782188, 0.27274088688581494, 0.7727329824646444, 0.6674737656275986, 0.11698329332801465, 0.7565826220811994, 0.12671938632381016, 0.7927169642759518, 0.0678023147926428, 0.7818632902150331, 0.29923527009055045, 0.5008714622704622, 0.008397823461430478, 0.7251481385731484, 0.044358508513483996, 0.7904137163399871, 0.11618985075508112, 0.22595725204112216, 0.7481217872899789, 0.20935493701983962, 0.14900704724542266, 0.05308066381726592, 0.4871436846346128, 0.6543980080873638, 0.08115513505864275, 0.21709098343113953, 0.05018030083308271, 0.4118948476334814, 0.39803616061384506, 0.032294554591055985, 0.20451796304366665], 'theta': [0.16479112877701085, 0.4812697090513901, 0.3490750179011633, 0.17384278876891265, 0.8871320403398824, 0.6613137101035101, 0.0947145129525728, 0.07525315040723696, 0.0829266091128818, 0.5540429824562073, 0.5239736532166342, 0.18691118912759722, 0.11231482504216526, 0.24872781904278643, 0.7022415684063642, 0.5967102565724756, 0.6167323459388224, 0.568045096465767, 0.7151139187837227, 0.047166583833920465, 0.31991131197296013, 0.7768847153873294, 0.6648105057952752, 0.12315379291413696, 0.7547447534002132, 0.11740267118292956, 0.7917050003992457, 0.02198382851715571, 0.7815901073666942, 0.3009792375779768, 0.5234225139168627, 0.02849561354115742, 0.7241265456034782, 0.07913747973237381, 0.7933842177894215, 0.12316312931538367, 0.38890020321721896, 0.7479199720871643, 0.025177902770378367, 0.22093342912967828, 0.06240272794382866, 0.48434589680781487, 0.6523650568453766, 0.17901624521121903, 0.2326480466867054, 0.05346809029186303, 0.414345778752625, 0.39674937799244153, 0.02453183421466424, 0.1431611407312816], 'croston': [0.18841999621071093, 0.6159096438209145, 0.41382929666757856, 0.10347440699222805, 0.8973805628727155, 0.7307422966089661, 0.10356169815461234, 0.11472232944798867, 0.12587627395504816, 0.5654989937579121, 0.631652892727172, 0.3192278485343575, 0.18078669598550215, 0.32537691043330086, 0.7176022767812142, 0.6446780866457086, 0.6029033662918103, 0.5640631316763002, 0.7293041604715891, 0.09965955465920028, 0.27660477387784693, 0.7891163792129362, 0.7085845925089004, 0.23398250611709806, 0.7915430000029137, 0.143836973096866, 0.8040079850604358, 0.0762225441196336, 0.7880256678793607, 0.35839748901788043, 0.497463193951765, 0.022432380186876953, 0.7417051539556075, 0.055237786330422756, 0.8266417465836333, 0.09009783523360959, 0.28605316409265, 0.7623791494020415, 0.15886167791527092, 0.1064146957666144, 0.15042050962698453, 0.6209009012462581, 0.6675087034014535, 0.0017641343047276755, 0.20538365284877066, 0.035406433454583136, 0.5536329360249301, 0.4782086677044141, 0.11346723088769467, 0.24131582859392828], 'prophet': [0.35365584002717204, 0.6586988656809313, 0.4488957158271366, 0.9225074370575721, 0.86103144483894, 0.7661992034100513, 0.08165194747818187, 0.040602748737345096, 0.20523127271363323, 0.6352334178609204, 0.6479318957788376, 0.1075644040474517, 0.17999509354263113, 0.4543783366935592, 0.7352656227916783, 0.6877680292853741, 0.6873331214067471, 0.5113756108555092, 0.753647362848583, 0.015066967034626273, 0.27802107138209436, 0.8347969862079929, 0.7252377723736351, 0.3023995066925531, 0.7907483794338851, 0.04274448938110244, 0.8301627113945893, 0.17364816041597406, 0.8440172117878727, 0.4352738699878128, 0.5178925785239131, 0.14254973163311166, 0.7706045385271123, 0.12165116960847928, 0.8878470146813163, 0.12139288304531268, 2.226711191836998, 0.7856080399047587, 4.176035977438924, 0.41955041406880333, 0.13972616372375546, 0.5996038984585135, 0.7176087669323017, 0.9929077035992216, 0.13871369635225575, 0.01188210088415712, 0.5391726174787265, 0.5361783754576555, 0.6005807109282871, 0.3486066969940732], 'Avg_ensemble': [0.11817992409428033, 0.5261043671398123, 0.37715503936232564, 0.19158709352770842, 0.8903594510108941, 0.6942132299279226, 0.13021662434242248, 0.08872254377998567, 0.13004883365767414, 0.5647357570515615, 0.5715833529228824, 0.15816897513665648, 0.1296507163411749, 0.2721815906771251, 0.7109838507473916, 0.6089888307039247, 0.6202446247607386, 0.4508802821391592, 0.7227625466344965, 0.05610003066151563, 0.3012292726247077, 0.779699208986734, 0.6844317799489191, 0.1571089165280572, 0.7705665516619022, 0.11672610400745323, 0.8006015833944352, 0.0564869200355218, 0.789480432051812, 0.33041084266065995, 0.5323685751177818, 0.05325578112995253, 0.7354385943307071, 0.015839610523710246, 0.807789314512635, 0.1152813901045603, 0.04177845476515368, 0.7583733229995041, 0.5087500458192419, 0.23262626812893464, 0.08698666897275194, 0.5215210191332377, 0.6595680319401425, 0.003974890686986089, 0.20622056111424786, 0.05178276451022467, 0.4681171946309166, 0.4360411015449289, 0.044062994915864365, 0.24679767331246802], 'Weighted_ensemble': [0.25715832370735253, 0.67062491711208, 0.42183106751405997, 0.01592999181443896, 0.9462750879556256, 0.7352010676083606, 0.14870704490717732, 0.26121654976042263, 0.08715049158501452, 0.5674937705376392, 0.647462055069697, 0.5779967845690308, 0.10884100476101893, 0.04371810539274311, 0.7288547147277807, 0.6535636990288108, 0.5185695998526554, 0.7141477211412627, 0.7464179812483187, 0.49887606547898533, 0.22411044895290264, 0.8377537341662735, 0.7133376759194152, 0.126386306301839, 0.8090105797009731, 0.30605224966535016, 0.8084967691569571, 0.4552433761144958, 0.7892437521484024, 0.3762671042200221, 0.4512441036614617, 1.3592075782783548e-05, 0.7626939132528536, 0.27032114529714385, 0.8011664116809729, 0.03542705037017458, 0.8364417784145068, 0.7723895793281702, 0.680515143770611, 0.04776022668340846, 0.16664657203300176, 0.8057118431753507, 0.6358458413384861, 0.019693223311149047, 0.19852217515663126, 0.023679534519796576, 0.6226387504662106, 0.506906285667658, 0.143102532185044, 0.4084025905812215]}\n====================\nSMAPE_dist_median is:  {'Lstm': [0.5609963028325597, 0.7443376070999033, 0.6611682592321555, 0.5239184318310555, 0.8450375598356804, 0.9268463071871067, 0.818242524054085, 0.8638721587331211, 0.8053591505305961, 0.7391909793013006, 0.6700093630952193, 0.8967028198577659, 0.5102648917893631, 0.6399481381017194, 1.0215864709891254, 0.7964634903353782, 0.6544059371817383, 0.9164293035444917, 0.9678498605204879, 0.8332140287221519, 0.6040737747032603, 1.1354941948366437, 1.0597877056622158, 0.5277394112961478, 1.2891260754251928, 0.7541981809040518, 1.2269213562190922, 0.9185028114139084, 1.1114671202245208, 0.6962885002244106, 0.5875282175944833, 0.6862289106108098, 0.981195980588972, 0.7844536501894523, 0.8681696827947185, 0.6933225200672036, 0.7847144182938397, 1.0045381277783634, 0.6820313767357861, 0.7924206888307058, 0.5593588142036068, 1.0354065582306418, 0.8015866306280423, 0.6546358555796439, 0.7495176594286284, 0.5214693274462701, 0.6575955164295922, 0.7943955032966543, 0.6256993957670753, 0.8872844345977977], 'naive': [0.5709374686108181, 0.5808057094899024, 0.5600193594542868, 0.5031146517721965, 0.6520775431086759, 0.7793741591445067, 0.8414135816654439, 0.7919508714133352, 0.7917093272335078, 0.679047025622705, 0.590506001374016, 0.6918115218644905, 0.5306653750956034, 0.730713856486765, 0.7977787296854091, 0.705944486055809, 0.7287075672857334, 0.7008252424818853, 0.8670233461676864, 0.518873516157962, 0.6071856121901633, 1.037933046645986, 0.797889826834215, 0.5341470360053907, 0.9909262128515531, 0.7364071808159589, 1.0959206436788882, 0.7190624588229629, 1.038083081773859, 0.6881368448443381, 0.5705555209544528, 0.6799545901634574, 0.8717194534749778, 0.7645406181490308, 0.7226413009539439, 0.7029122102576258, 0.7029033077259864, 0.9032504107416028, 0.6327408082433816, 0.7793227101678357, 0.5553997023634372, 0.5459826472346827, 0.7041825050765513, 0.59273095127025, 0.7483733625892148, 0.5219063127776142, 0.5942643435859019, 0.5670758937977425, 0.7211830935159929, 0.8393113744266858], 'snaive': [0.5861783240884946, 0.6835642191760842, 0.5864244481505191, 0.504568463096714, 0.8182596586145827, 0.907674068520238, 0.8311327852265096, 0.7918607302553313, 0.7915618284211569, 0.6880166608065581, 0.6882996553606956, 0.7309207602322261, 0.5154102647256924, 0.6406259365874551, 0.8174395515578092, 0.8098557653074693, 0.702967439346243, 0.7619613236616596, 0.8794726667140185, 0.5118821839389286, 0.6052554775049663, 1.0817800241940412, 0.896658871065654, 0.5296985767653171, 1.0611678867411851, 0.7388194419331762, 1.1110407993998128, 0.7602867400263292, 1.0536275060789797, 0.7012444649504461, 0.5786956914886302, 0.6716497001778695, 0.90788428146042, 0.7733594383062888, 0.8276259298011237, 0.7014509223556746, 0.6832809409479075, 0.9334659402724533, 0.7044508673355319, 0.792569776686378, 0.5580988755566578, 0.6070737097866334, 0.7349899605828749, 0.6281295585514487, 0.7497214278156719, 0.5261600683691277, 0.6690813105911396, 0.637337650063639, 0.6400578149091971, 0.8478706600304531], 'arima': [0.5835700689036484, 0.576780923982678, 0.5548437487880844, 0.5614850253594486, 1.003675453352213, 0.8819011064038869, 0.8234500090845668, 0.7877578557559777, 0.7807583075413341, 0.6854822425468443, 0.6611978622002721, 0.693122805142905, 0.5152682873606828, 0.6607320561914044, 0.8156593073395353, 0.7922603548765149, 0.6675853982546258, 0.6703499525275005, 0.8544695756799067, 0.5162822956677144, 0.6022426798176159, 1.0017306389781377, 0.8639659545266124, 0.5091811144795242, 1.0378725599382965, 0.7314158604274968, 1.1084063803353816, 0.7272067770804809, 1.0452401371298072, 0.6946073805857031, 0.5732659893840396, 0.6552875798325003, 0.8925174634418517, 0.7619952546630842, 0.7177732696127109, 0.6972622504103744, 0.669670207740555, 0.9421698866679408, 0.6188843417453608, 0.7760567774447971, 0.558259978318651, 0.5371264891275034, 0.7146836647227501, 0.5957830901129525, 0.763124116270012, 0.524220980599531, 0.5908359909660801, 0.6461506433728748, 0.6692635225059056, 0.8402924342676361], 'ses': [0.5801820682105057, 0.5808057095329024, 0.5600193599612061, 0.503335824493248, 0.9716009086652472, 0.781728030631804, 0.8301578358251882, 0.7883279180913036, 0.7919131225684375, 0.679109270859328, 0.5905060019177046, 0.6945270726673043, 0.5251101841181819, 0.6719916652894115, 0.7993226441920451, 0.7283820560774814, 0.7356701224998652, 0.700825243274894, 0.8670902827903024, 0.5189111015383151, 0.6045048004145654, 1.0379330479382998, 0.8086371796313558, 0.5151836929121929, 0.9909561810033446, 0.7351103873301797, 1.0972581699005821, 0.7179910985140394, 1.0432888267474756, 0.6940941247209207, 0.5694053125515296, 0.6781811556284398, 0.8734238612792007, 0.7648425512632587, 0.7226413030451144, 0.7006869486428966, 0.6795616631427691, 0.90325041186637, 0.6048507259381914, 0.7794697748208635, 0.5491104830753015, 0.5537214167438815, 0.7174384748576746, 0.5932857126002432, 0.7638501272363071, 0.5219063126977268, 0.5929349896739651, 0.5721271697300228, 0.6841967684622807, 0.8397834147648383], 'holts': [0.5630674241098154, 0.572506900995702, 0.554858232662674, 0.5014724213491639, 0.7789222468265916, 0.7656981602480906, 0.8417533959019906, 0.789452815696374, 0.790340134587525, 0.6790802208277228, 0.5776168861644018, 0.6936700590936747, 0.5312185676109965, 0.7408070151861863, 0.797133984555268, 0.6956744350685984, 0.7462914231001094, 0.7002070767997295, 0.8751028364624425, 0.518942365654705, 0.6081921550983619, 1.0461026116596914, 0.7907867752962806, 0.5494982465157994, 0.9848761727392652, 0.735612666863585, 1.1017649235583669, 0.7027101565230871, 1.0469826137491158, 0.6907157275064686, 0.570180394002114, 0.6809936420557003, 0.8714032072236397, 0.765338462289828, 0.7148922805511284, 0.7035825169840967, 0.7077504814165148, 0.9058651768651297, 0.6950514059675154, 0.7777395865036347, 0.5634046593050646, 0.535923719436896, 0.7098574678282714, 0.5976114370736324, 0.7591544542986006, 0.5229299034083182, 0.5975275692847046, 0.5576659413089109, 0.7188256288238666, 0.8414976722662945], 'Es': [0.5851914401608667, 0.5963193260391719, 0.5887220673217199, 0.5035945541134471, 1.452678405134794, 0.8400158429972007, 0.8470273658840517, 0.7880117589440303, 0.8031500498106274, 0.6798833980081395, 0.622509665310627, 0.6884439426056711, 0.5146321142856103, 0.7012861396865817, 0.8262191810576747, 0.7243352226840295, 0.7705903080369866, 1.318802966822707, 0.8764026691619698, 0.5164271095807141, 0.6686167169790042, 1.031593533952594, 0.830381221815158, 0.5062203009635823, 1.0271802239782661, 0.7200643678999125, 1.1453033817297251, 0.6975871225927393, 1.0649196716607867, 0.7143326748875889, 0.8715559377976989, 0.6432072228732988, 0.914362788722676, 0.9226652515978593, 0.7318917852033323, 0.7085480923637493, 0.7030049737683899, 0.9842180251541727, 1.3431095604626595, 1.435810983364095, 0.5617311584564177, 0.5514039101163342, 0.7105799538380858, 0.7048546544518018, 0.782915790152887, 0.53004035100026, 0.6747339076935805, 0.5719567277631987, 0.7113937123851055, 0.8847981728264195], 'auto_ets': [0.5801840868472046, 0.5808059979875381, 0.5600227614241997, 0.50333562671521, 0.9715981157015289, 0.7817242826147502, 0.830158028320412, 0.7883280948149535, 0.7919057901673634, 0.6791092818639956, 0.590509650257351, 0.6945324102475392, 0.5251096571227791, 0.6719248775725108, 0.7993225418819554, 0.7283938864235222, 0.7356593361310722, 0.6488518156899785, 0.8670902947894856, 0.5189111374756292, 0.6045046047465685, 1.0379417192642952, 0.8086376731533355, 0.5152231192875245, 0.9909610277948231, 0.7351105033440852, 1.0972589009693683, 0.7179835307600331, 1.0432898292232655, 0.6940942423864027, 0.5694062317497259, 0.6781811215392811, 0.8734237392685, 0.7648414107294096, 0.7226553346952685, 0.7006688813940273, 0.6795621319259323, 0.9032579587692048, 0.6048726598339894, 0.7762042169945935, 0.5491238108973024, 0.5537220017188045, 0.7174391948679335, 0.5932843345224112, 0.7638498342156257, 0.5219057766729824, 0.5929349831157048, 0.5721273038858654, 0.6841957685771909, 0.8397829212591439], 'theta': [0.5779748498175342, 0.5824018087863171, 0.5605584740235926, 0.525877743584128, 0.9043699828744032, 0.7824757524187886, 0.8266941552952982, 0.78877178495605, 0.7828376404633242, 0.6791238643412174, 0.5917162184780449, 0.6922708513169604, 0.5273731545612238, 0.6507281159217015, 0.7981912502238304, 0.737162478618734, 0.7306262888423647, 0.7441435138694631, 0.8624270292556164, 0.5182947597273931, 0.5977959533186693, 1.0475067668916431, 0.8035565522648, 0.5110137666455413, 0.9870367705306884, 0.7322633241306434, 1.095001658029697, 0.7166159650036995, 1.0426226804533423, 0.6948323356757367, 0.5704137706069704, 0.6698370268937093, 0.871083142324493, 0.7616506504823028, 0.7325168695912736, 0.7039903392284638, 0.6840451568169001, 0.9027263534868295, 0.5976175228372634, 0.7665222747648756, 0.5456454462645453, 0.551369058352343, 0.7144123940697888, 0.6035727623362158, 0.7636840262131153, 0.5228442885900285, 0.5932541077968871, 0.5707582990384245, 0.6828790949837844, 0.8374795608496368], 'croston': [0.5805957265214194, 0.6552218248134418, 0.5829767256188574, 0.5025598371073021, 0.8527578665237534, 0.889018798094751, 0.8274261369600078, 0.7924281733927028, 0.7906358007146178, 0.686444243935711, 0.6659480016515139, 0.7093687575733065, 0.5073587915037047, 0.6355732480372493, 0.8195005731473939, 0.7818483781424758, 0.711288642877046, 0.738899879190384, 0.8885367896659935, 0.5182286871283397, 0.6031678762541695, 1.085039948483336, 0.8855941913473752, 0.515827547097101, 1.0598750138948438, 0.7357850967699542, 1.1211277639415231, 0.737978588355272, 1.0586739193594603, 0.6928581831070181, 0.5706622068156759, 0.6744337736084379, 0.9081423648678952, 0.7652562552896984, 0.8173499271392923, 0.6977910019708371, 0.6879248978521458, 0.9403204140045642, 0.5979387211928867, 0.7872274789187572, 0.5564604213132459, 0.5783374861892484, 0.7319898779485826, 0.5997637643085789, 0.761775409968492, 0.5234885123339826, 0.6463716161582784, 0.6289460975734547, 0.6529489768591683, 0.8403426973604193], 'prophet': [0.6202209239335602, 0.7039602328383856, 0.5988022198242078, 1.430300391517298, 1.2070259846456375, 0.9707434821663943, 0.8283616613156453, 0.7852334365835563, 0.8113437822444084, 0.764822149833053, 0.6751705333594549, 0.6496383745574694, 0.5142875291643405, 0.6405343057061941, 0.8505846938968926, 0.8481946665284502, 0.8367717741620291, 0.7906028586872968, 0.943276752232669, 0.516509903887449, 0.6061977245133774, 1.2164627810412263, 0.9115612949152633, 0.5200848647448514, 1.0579433482789038, 0.7376559417898979, 1.1916625947019501, 0.677117140825457, 1.2254481879088468, 0.6966721509715301, 0.5680211460891831, 0.6364547545360693, 0.9739447800712401, 0.7894955151698008, 1.0707995865536948, 0.6994031370689052, 1.1630371320348396, 0.993722790397848, 1.2714184145818355, 0.7835141384654356, 0.5570096202514144, 0.5581351248829823, 0.8181547687440801, 0.8477067183028012, 0.7504504098914337, 0.5234814279510016, 0.6254047260865541, 0.6832655267088746, 0.8724598681662071, 0.8608960450692038], 'Avg_ensemble': [0.5635080176837993, 0.5976944627224945, 0.5733361302907634, 0.524970503375713, 0.8710695252777172, 0.8359429225379534, 0.8299293195372704, 0.7886680714629152, 0.792595844803869, 0.6869931974779158, 0.6165087533425083, 0.6909443018623613, 0.5224218355041803, 0.6433083602533906, 0.8113992050480066, 0.7463374847242216, 0.7376603709947784, 0.6485391562525699, 0.8766248673436131, 0.5172039987222099, 0.5960544989490268, 1.057876709007329, 0.8341243837582583, 0.510799630277435, 1.0147777029305063, 0.7338532783118781, 1.1154495361248682, 0.7174337877413022, 1.063534228713051, 0.6958861457575848, 0.5675454349648763, 0.6663688034606022, 0.8949876362070006, 0.763334009335425, 0.767992516339238, 0.6998712753128461, 0.7271361406335974, 0.9319061819153293, 0.665009809497687, 0.7613711619280145, 0.5509909788902625, 0.5598346627267754, 0.722736844951956, 0.6066595015669081, 0.7624614246395114, 0.5232456624129407, 0.6072931535182133, 0.5937635583309647, 0.665110458202723, 0.8424730112946663], 'Weighted_ensemble': [0.5982697817862752, 0.7232985755447319, 0.5830398100313361, 0.5010412362153847, 0.8316283735659328, 0.8957697753855687, 0.8344448331793718, 0.8055982428660047, 0.7858002791327948, 0.6840648107521866, 0.6749048207867512, 0.7531324372475284, 0.5273345034534227, 0.7905644762814703, 0.8316399002757044, 0.7952074876167753, 0.6168887287378187, 0.9005211334902822, 0.919818133016616, 0.6387493776932298, 0.6112734149814393, 1.2141371248216746, 0.8855921762112514, 0.5122874656225553, 1.0957207502043596, 0.7622643976129239, 1.1274632635676651, 0.8465396397313839, 1.062859968799317, 0.6943522008865359, 0.57650082457072, 0.6786474391197658, 0.9499628025080453, 0.8021620137376725, 0.7431555729046648, 0.6984863076147145, 1.4733518923469313, 0.955513470578362, 1.3724296258122284, 0.8006359338525322, 0.5595051099019708, 0.8726898296782271, 0.6958887603442391, 0.6069242615311692, 0.7611641250549105, 0.5236172321144947, 0.708238678441586, 0.6480201660467133, 0.6387316801463568, 0.864788350279263]}\n====================\nSMAPE_dist_80 is:  {'Lstm': [0.7998104372051873, 0.9253911318018133, 0.8401092869726376, 0.7440541750414166, 0.9751814469575949, 1.118049691719625, 1.0017042964681677, 1.1749334422956546, 1.0449494814517268, 0.99564534878048, 0.8744917865648797, 1.098717953591028, 0.766290743765839, 0.8463415580071236, 1.2188407374497494, 1.069544005194346, 0.8990248530007395, 1.1704168296764355, 1.0944925292024978, 0.9976148722040172, 0.8342879169344808, 1.2539219692318089, 1.2255407440552077, 0.6983241136950588, 1.4111715461819856, 0.9549572753928957, 1.3323894840717054, 1.1203818712777307, 1.288439695343414, 0.8926745119294498, 0.8275311070612865, 0.8769342826648803, 1.1572349484325692, 0.9716115194507725, 1.0308316891302511, 0.8663841430572876, 1.065971665695065, 1.2359188420346034, 0.8469657980590652, 0.9580880191297139, 0.7488813749440566, 1.2950994096746085, 1.032276990064368, 0.8613218530795514, 0.9813991714173502, 0.7051296806916151, 0.8789599032232565, 1.0294671824063044, 0.8486812939049553, 1.0995171344814711], 'naive': [0.8038338454637981, 0.7484590998826121, 0.7347334451762528, 0.7339259534693898, 0.9135830462377671, 0.9767787460279195, 1.0324112909741896, 1.0880368321730267, 1.0286081096121722, 0.9243310099664754, 0.7915946909555385, 0.9632084939965458, 0.7698630307917619, 0.9367797925141691, 0.9808563928789498, 0.9472817033333389, 0.9591055222287148, 0.8846096218872717, 1.002287839757934, 0.743237919062486, 0.8436051204213674, 1.1719528970166722, 1.0164351313231508, 0.7234978896389612, 1.1131628355521332, 0.93852876938482, 1.2060878090861014, 0.9310156219211365, 1.2339176763088315, 0.9071870898409231, 0.8128589029320373, 0.8742060215950128, 1.070400619382712, 0.9585963605485673, 0.9137076774248096, 0.8510109611958432, 1.0588093689352016, 1.1395614028869807, 0.8663318360241236, 0.9538748489471758, 0.7497938155890996, 0.7960787728594388, 0.9246843850406625, 0.8200842622691544, 0.9635852325513321, 0.7119853914102175, 0.7780055310952789, 0.7683927849644487, 0.8976003807173044, 1.0568882934845123], 'snaive': [0.8299375705996199, 0.860361003251409, 0.7727151643203305, 0.7281558089494526, 0.9149822435932758, 1.1034808767539876, 1.0217659272808493, 1.0975595340555087, 1.0321999056861837, 0.9321759179079621, 0.9017110392423452, 1.0031622412708903, 0.7710214550436023, 0.8356801617795113, 1.0041087234113963, 1.0851612314052184, 0.9271456376148078, 0.9565946553684193, 1.0153131179698922, 0.7159157281728041, 0.8368585568329914, 1.1954859252069967, 1.098176915227905, 0.6996346191490199, 1.1829187912288988, 0.943484118951659, 1.22126313465494, 0.9688267813152214, 1.2456095245663346, 0.8964112077090232, 0.8235176091632296, 0.8739621066983664, 1.099636513118984, 0.9718477981141173, 1.0347878955030678, 0.8561508917658167, 0.996845647945821, 1.1549894629744901, 0.858708486646444, 0.9620201265101533, 0.7431590348172021, 0.8525728004219151, 0.9616976855413257, 0.8289489564520036, 0.9691954658096003, 0.7115704128737642, 0.8801739418498118, 0.8568299146224185, 0.8561207699595221, 1.0638219716828194], 'arima': [0.8225268124663608, 0.7361820799444914, 0.7312188919339955, 0.7581420443530513, 1.1817383382589077, 1.081030055485576, 1.0151480498344545, 1.0820231319598221, 1.0168320342187331, 0.9363589508116101, 0.8480709219159835, 0.9757463169089671, 0.7627134976468782, 0.8699863687874629, 0.9988296836089954, 1.073171253067536, 0.9090301445805898, 0.8481836599108006, 0.991757890662698, 0.7627927316298377, 0.8282917294773462, 1.1421389672803253, 1.0679937534185864, 0.6983452736934317, 1.1524717226440147, 0.9285658902384992, 1.219927436939486, 0.9407360216262703, 1.2413436506571052, 0.8937803563106483, 0.814123510292312, 0.8566425656777865, 1.0901739169305735, 0.959262682985823, 0.9083418381041577, 0.8577204342722358, 0.98940721231239, 1.1716704174532158, 0.8580383600013416, 0.9507474587755702, 0.7458343079027726, 0.806026637280955, 0.9425884210372448, 0.8204769954593677, 0.978288000697048, 0.7139924358050376, 0.7893305396437625, 0.8590261940793618, 0.8643107692501564, 1.0551145626539618], 'ses': [0.8136484674475261, 0.7484591013589579, 0.7347334458583775, 0.7327344742058252, 1.1643831221428667, 0.9792402734911992, 1.0248542120001323, 1.086226231452802, 1.0281150743137417, 0.926068542552742, 0.7915946923647822, 0.9764545517246384, 0.7611341772171237, 0.8786222935791174, 0.9825682821382005, 0.9849657904350696, 0.9679664703652358, 0.8846096229747127, 1.0023527038451203, 0.7439130105898548, 0.8327827392926522, 1.171952898055145, 1.0230122922372218, 0.7044676349059354, 1.1131750002118481, 0.9291569948216216, 1.2075733379705227, 0.9282824231532709, 1.2392913992120056, 0.9034189209389205, 0.8134141069113046, 0.8743015133158066, 1.0724461158171432, 0.9626523969409244, 0.9137076795230761, 0.852239466222062, 1.0040274539260419, 1.1395614038380597, 0.8494441169834438, 0.953994569944268, 0.7499524251684614, 0.7770938177400165, 0.9436640628681747, 0.8169372486673343, 0.9797535689181466, 0.7119853914994974, 0.779720436003969, 0.7755434207229972, 0.8762267999403681, 1.056249458135756], 'holts': [0.8027775376217591, 0.7232671874738444, 0.7308503241253145, 0.7342912759444739, 0.9811191816924594, 0.9688964939402838, 1.0326254717592664, 1.0863102071811521, 1.0276216276888623, 0.9264105023816953, 0.776152968436438, 0.9585364945062059, 0.7742548362850831, 0.9462512509883801, 0.9819049309992784, 0.9459030120027795, 0.9770229878487611, 0.885421358150456, 1.0110303306302972, 0.7501691032871902, 0.83804186694732, 1.17649531856568, 1.008821926931163, 0.7324865271701002, 1.111510393730324, 0.9349903725416431, 1.2132253671567381, 0.9211284861274142, 1.24317081562625, 0.9071350671219758, 0.8136199759842658, 0.8734872248076724, 1.0723731339357747, 0.9589444188793299, 0.9071871733061649, 0.8478243313639193, 1.0647030407062894, 1.142310493032115, 0.8769756495870256, 0.952705791710026, 0.7555299946319535, 0.8387802604352524, 0.9327252533185681, 0.8220945770882313, 0.9724768278867897, 0.7111304280974413, 0.7629823369153018, 0.7597583311178289, 0.9100410864320445, 1.051841631076434], 'Es': [0.8257844594993765, 0.762225203074603, 0.7795008906301975, 0.7400678228967555, 1.5379330694352609, 1.0370732744535538, 1.035052299097932, 1.0883523302903109, 1.0365067662222684, 0.9288841988882827, 0.8138350806141603, 0.9644298792196707, 0.7637861002579336, 0.8916602317266186, 1.0125443712813593, 0.980015918535345, 1.0090962110307418, 1.437770434160551, 1.0144656594089736, 0.7630206674683224, 0.8596800517467349, 1.1713032153944238, 1.0429290381308072, 0.6963059473594482, 1.1384311531473947, 0.9275291395889277, 1.251390253070901, 0.9137081582059577, 1.26018980120052, 0.9006396982812843, 1.081777198694503, 0.8696578207191126, 1.1046841796376095, 1.0762055888153552, 0.9155282286750056, 0.8475414162575001, 1.0450162525028222, 1.2048463041837847, 1.4227982601005382, 1.5376542275296603, 0.7577372784004526, 0.7720793333258082, 0.9262299332439757, 0.8992440079332928, 0.9531326284243038, 0.7202378982222577, 0.8865768311252908, 0.7792128117299097, 0.8941126638196276, 1.1058806960742669], 'auto_ets': [0.8136510906341514, 0.7484690077406004, 0.7347380229651551, 0.7327361359198155, 1.1643839407418881, 0.979235614843486, 1.024854187017164, 1.0862266711449713, 1.0281162500633838, 0.9260688591672601, 0.7916041489419648, 0.9764945607276889, 0.7611343797754306, 0.8786359973509755, 0.9825681686828528, 0.9849925669539045, 0.9679588733705331, 0.8429963002253893, 1.0023527154727665, 0.7439136585203165, 0.8327822963701229, 1.1719598661247963, 1.0230126715640384, 0.70446691911633, 1.1131769675488343, 0.9291570610181785, 1.2075741501131083, 0.9282667616534737, 1.239292417087946, 0.90341875989946, 0.8134136679955108, 0.8743015059076045, 1.0724459693650181, 0.9626514471141039, 0.9137217587853292, 0.8522445442048122, 1.0040269514874067, 1.1395677853364303, 0.849488204431259, 0.9527553637545829, 0.74995003591133, 0.7770923605133983, 0.9436652765978903, 0.8169359219443295, 0.9797528861322217, 0.7119859905445489, 0.7797204411848033, 0.775543610752955, 0.8762227584004231, 1.0562490926507873], 'theta': [0.8089905478315116, 0.7496499024083854, 0.7352140148497305, 0.73787790325845, 1.1292684602825898, 0.9799244016474662, 1.0210474960177833, 1.082956090327816, 1.0213933281385268, 0.9261778029428789, 0.7941855735342737, 0.9721394231634642, 0.7604972237814934, 0.8560997569400313, 0.9815968007429439, 0.9990610185921307, 0.9619090632686165, 0.9213688597788487, 0.9982031317634121, 0.7497241140746094, 0.8366314869540785, 1.180290826137512, 1.0190432548450665, 0.7033790396111264, 1.1101910835523274, 0.9294058361196585, 1.20556215833939, 0.9385593255122751, 1.2387388852269292, 0.9026466165721934, 0.8186328484890447, 0.8706854479725306, 1.0704224382740293, 0.9548783976151388, 0.9222123915520656, 0.8509313945143053, 0.9985498305649956, 1.1391641598458118, 0.7830228608303804, 0.9365727056067223, 0.7494348315344116, 0.780984894164392, 0.9401916571419704, 0.8263740233638597, 0.9835386427581969, 0.711166043910723, 0.781826036410117, 0.7740733515076388, 0.8742717000741302, 1.047957337790152], 'croston': [0.8198579202720263, 0.8357618281557053, 0.7691667877943174, 0.7278678885599681, 1.074678000999856, 1.08801706563156, 1.0236424118021783, 1.0887589156709843, 1.0276810661852682, 0.9386396768018826, 0.8739776799361474, 1.001927398199849, 0.7571830164712315, 0.8307075868276864, 1.0079419631294309, 1.0576959653903333, 0.944947338472389, 0.9206366112848303, 1.0241163627519365, 0.7311701637475491, 0.8310206993428921, 1.2010655335698508, 1.0892145452791542, 0.6932953501619163, 1.1789953877880635, 0.9314933198409499, 1.2318062941648622, 0.9563553431177534, 1.2531495823193615, 0.8882541156163541, 0.8128067940381385, 0.872631852771165, 1.1005992581126098, 0.961744365076187, 1.0127595264933003, 0.8582114838433699, 0.9936761002263712, 1.171267274233302, 0.8375028086160645, 0.9538449771066599, 0.7435848626728463, 0.799190494924374, 0.9625052061049473, 0.8158572609955084, 0.979075133670236, 0.7088433992746489, 0.8524761619683603, 0.8417813001855781, 0.8545540619349492, 1.0547019712547776], 'prophet': [0.8528247775550695, 0.8952083143596175, 0.7878968098435402, 1.5109064419432308, 1.241769070266242, 1.1556130142164376, 1.0176451108813893, 1.0784182136678266, 1.0473892269970941, 1.038183437579598, 0.8936059845981951, 0.9398253603514887, 0.7614537795223413, 0.8135511794993004, 1.0412240710247869, 1.1397159072312883, 1.0763995494988894, 1.1626591782024236, 1.0740538815751803, 0.7668127242817881, 0.8419929187892096, 1.3055275564490194, 1.1159265556931488, 0.6886435800630428, 1.1807855980855746, 0.9212708080965119, 1.2946604517124782, 0.9083575768384995, 1.3687066692261942, 0.8805870002912883, 0.8045383438761337, 0.8512345887201598, 1.1535876303788204, 0.9830853380599626, 1.2731063193722196, 0.8533068326159865, 1.477559843585425, 1.225830440647293, 1.4557464276176444, 0.9645833138111113, 0.7422672855952458, 0.7773247015438116, 1.0417259460547956, 1.026379947376065, 0.9681751587550333, 0.7045591161048967, 0.8401334027430516, 0.8979422773897194, 1.0245866390398375, 1.072524578402509], 'Avg_ensemble': [0.805143384370124, 0.7663974535890072, 0.7457941470819027, 0.7454401905939522, 1.0517912342759395, 1.0334489239969258, 1.0253725733852217, 1.0883912726872602, 1.0276046887469672, 0.93800820619229, 0.8094696608108731, 0.9621645753820162, 0.761507357486115, 0.8517416701983131, 0.9963541729501573, 1.0143290415071788, 0.9675045089925216, 0.8333983135334928, 1.0121852519045886, 0.7451815705865115, 0.8337304373444897, 1.1868209387301154, 1.0465475645881532, 0.6981173997270934, 1.1332179901360029, 0.9304767624179244, 1.2249422962442507, 0.9247188521493385, 1.2571355940562878, 0.8922466195209955, 0.8142901666982689, 0.8642913006600312, 1.09192069803195, 0.9659417852138206, 0.9592338239655261, 0.8534360461856665, 1.0781796716203331, 1.162104008467623, 0.9442381125345852, 0.9432162163225042, 0.7444437726030796, 0.7657614222470588, 0.952311080073443, 0.8179390205999345, 0.9795612459601886, 0.7118713401292966, 0.7995435717761528, 0.7994734455725591, 0.8642920434557753, 1.0569200378207122], 'Weighted_ensemble': [0.8416442272905388, 0.9088549509040442, 0.7727670051476105, 0.7500149995054628, 1.015748843232065, 1.094354318402098, 1.0256380213679828, 1.1040818333015228, 1.0203044714886587, 0.9378522137240175, 0.8894576938481186, 1.0353903002246438, 0.7585602626029542, 0.9893013621212154, 1.0271823380906464, 1.0736410415885733, 0.8451266719703416, 1.1585559087045332, 1.0534839656017607, 0.8128329920021028, 0.852941826090419, 1.3232357566456576, 1.0943214090021822, 0.7015540184573312, 1.230383979080249, 0.9574808012328095, 1.2482536137127993, 1.0509340152751299, 1.2520037673635511, 0.8890820384855718, 0.8096839970082575, 0.8727584630472817, 1.1270799540329854, 0.9996682249823192, 0.9350145672906389, 0.8633463567177058, 1.5484912759267806, 1.1956067228051712, 1.4817532382905736, 0.9658181460812838, 0.7460303855720607, 1.1927710604176098, 0.9216629615072633, 0.8139636103186471, 0.9759901560028853, 0.7068494321097217, 0.9125445256581235, 0.8646860546004385, 0.852079583517543, 1.0763330596485419]}\n====================\nSMAPE_dist_90 is:  {'Lstm': [0.9387321932604608, 1.037017681498811, 0.9962022714247235, 0.8385606691151082, 1.2799856837360788, 1.2839226631356713, 1.1045752302557648, 1.3200920273042418, 1.2187914231913635, 1.109712782738943, 1.057515801493842, 1.2146551971900772, 0.9582530288367963, 0.9435708738109972, 1.2804198962521642, 1.194751530086793, 0.9800424564897385, 1.2526979988714113, 1.1259610184171345, 1.0864793175511436, 1.007554377955253, 1.311524038642437, 1.295648916213469, 0.820179040267953, 1.4365993036038067, 1.061573135658119, 1.3663576932387265, 1.2350409214253406, 1.3335055638573983, 1.0819635974244661, 1.0166198942442075, 1.0155813229156303, 1.2212116155830965, 1.073234155131288, 1.1793651235585747, 0.9577488604268701, 1.133239668691434, 1.3073344402740112, 1.0119316778126342, 1.050874156587916, 0.857665895756587, 1.3538884739073076, 1.1410008176325885, 0.9723819186135523, 1.1011031599787269, 0.8150435151738884, 0.9754821589403754, 1.127811563432972, 0.9881642166488682, 1.2387415917034614], 'naive': [0.9400862448332032, 0.8731405714247055, 0.8818463160955197, 0.8449884993397941, 1.2805579872736943, 1.1460748905515656, 1.1514127498077087, 1.2355135315919168, 1.2017600332101261, 1.042571466289224, 0.9520071842568414, 1.0997977311636382, 0.9399113154561021, 1.033159796087189, 1.0599368051820166, 1.0743457892946218, 1.04489308587779, 0.9829228507115175, 1.0416784479560888, 0.8202936466560813, 1.0208021538750782, 1.249136496878363, 1.1462522773488715, 0.8502039595958887, 1.1472818710710733, 1.0475194921961644, 1.2473760939496257, 1.0461613908634724, 1.2804581862258244, 1.0791035270486788, 1.0227731906609832, 1.016531836421364, 1.1383623204899496, 1.0569549396963642, 1.0709990238921803, 0.9684028488174905, 1.1311700091995962, 1.2143699751326444, 0.9947154296745346, 1.0518601398910514, 0.8719910977007852, 1.0877011872245839, 1.0559547052249258, 0.9035928018452806, 1.0913944417816666, 0.8234645210965086, 0.9040836534425878, 0.9304355984408492, 1.0363161637871836, 1.1764331418244869], 'snaive': [0.9606108133416397, 0.9964284835058403, 0.9252437125283977, 0.8389868672935282, 1.2447287819916368, 1.2727923290188234, 1.1414646879556238, 1.236650013882666, 1.2006498788551658, 1.0497534368655415, 1.0876933536488949, 1.1299416857902544, 0.960348441028356, 0.9242607251233195, 1.0844386637130308, 1.2158386358488247, 1.0207052207306049, 1.0390160474428762, 1.0512380526716156, 0.804537619991646, 1.004194345710458, 1.275456667375939, 1.1906689939462403, 0.8234683394445461, 1.2186295968162892, 1.0455346900115448, 1.2590997176534275, 1.114387900692018, 1.2906692191022708, 1.0725007533993578, 1.0308830996978615, 1.0104863958138608, 1.1686000434031207, 1.0680488371180312, 1.1840715689924273, 0.9719013108474807, 1.0523931396130841, 1.2308273992066758, 1.0899531275997891, 1.0440018242952758, 0.8548086974896459, 1.0825738404572935, 1.080524900768729, 0.9411991357266432, 1.0954815815668573, 0.8237647043565826, 0.977083078666261, 0.9491744115066669, 0.963251370060072, 1.188907101315345], 'arima': [0.9525522921842527, 0.8648946512545542, 0.8799239699549757, 0.8676418311724153, 1.1911305912784982, 1.238242123833548, 1.131397272185863, 1.2245094577152758, 1.1946462231855828, 1.054682271115904, 1.0384387607447303, 1.1095199017747523, 0.9383072437959683, 0.9735084656551771, 1.0786917112137833, 1.2003081710828754, 0.9882802923468357, 0.9627261655329802, 1.031375151087994, 0.8295596958939071, 1.0084901474466768, 1.2166232437404072, 1.167037810850164, 0.8157260051035066, 1.1889555887651237, 1.0375200158174323, 1.2589684098138303, 1.0819471745401346, 1.28548593187324, 1.082410135755038, 1.0175012746751668, 0.9860538179756608, 1.1538908295399033, 1.0529031991844997, 1.0653129867692388, 0.9636794221937904, 1.06093478113297, 1.2442815813872556, 1.0877392865953532, 1.052811242305057, 0.8583342297045131, 1.0934286456089437, 1.0705462244723887, 0.9027512179875634, 1.089996341082282, 0.8255682932894611, 0.9233088687760252, 0.9536682161654437, 0.9811719674556719, 1.1777646951813747], 'ses': [0.9484111720487977, 0.8731405690993879, 0.8818463173141212, 0.8384175465049211, 1.1907000059650805, 1.1475237262791447, 1.148015178935527, 1.2316769793752473, 1.2013501948305643, 1.0444958074032522, 0.9520071840535271, 1.1091888376870715, 0.9470344515431295, 0.9810328258439394, 1.0616731641649242, 1.1239725502579667, 1.054815639265097, 0.9829228512303281, 1.041738331145036, 0.8204839686607774, 1.0132183060328623, 1.2491364980072983, 1.1403263150086635, 0.8291083680446372, 1.1473057973457195, 1.0383882484240057, 1.248837363280503, 1.0519781580479552, 1.2846195003424996, 1.088814539798108, 1.020647011628845, 1.0171300392841363, 1.1396756922083024, 1.0680217354509742, 1.0709990260893252, 0.965897117655263, 1.0790788962011508, 1.214369976045134, 1.0827749383680052, 1.051736953493287, 0.8735533425985076, 1.0723600830771365, 1.0713884869126293, 0.906059398979677, 1.0920064660588944, 0.8234645210482298, 0.9055306132927343, 0.9338693259849435, 1.0104172783793661, 1.179055875616819], 'holts': [0.9386636455022924, 0.865621565429209, 0.8796384560526997, 0.8447824002875268, 1.421458613543405, 1.1345515716571468, 1.151402601220833, 1.2329743483771929, 1.2008947898590667, 1.0452326700172192, 0.9589729928287998, 1.0916836378437167, 0.9433141131207228, 1.0413847503525782, 1.0602063316898636, 1.0695248032440539, 1.0647473344060656, 0.9828651056130753, 1.0490663746768618, 0.8225833353884118, 1.018677260225139, 1.2533334400339116, 1.1500115289332946, 0.8557330579789673, 1.144381033663298, 1.0414894422487946, 1.254513451798229, 1.0340904063323562, 1.2875562039029054, 1.0812486438571915, 1.0198808875658585, 1.0169705465928622, 1.1378817599979234, 1.0579505928560058, 1.0676964049114783, 0.9671580095782685, 1.1364638426309093, 1.2174126941760235, 0.9833887165362498, 1.0533896820093591, 0.8759957295312273, 1.119051347141389, 1.0612326428943728, 0.903786440864486, 1.0923184085817148, 0.8240065420993697, 0.8914539363377847, 0.9257728261997721, 1.0520309741590086, 1.1756953913126005], 'Es': [0.9603080616067433, 0.879545319267423, 0.9315313285663671, 0.8410369042748416, 1.590431159260228, 1.1920977416208611, 1.149275232270775, 1.2201899566876637, 1.2138553203253686, 1.0485930249344815, 1.0031740518641903, 1.083755492537144, 0.9388216621223928, 0.9910076686638069, 1.0894340618231428, 1.1247793274591138, 1.0898891212094193, 1.4805184062725856, 1.047881722935782, 0.8347415723248053, 1.0546134105974312, 1.243168569865305, 1.1493628910172955, 0.8111419535752635, 1.1758605998125113, 1.0470998744986024, 1.2879245416723182, 1.0014087924883939, 1.2926732090651274, 1.0394575732488107, 1.2292285010970858, 0.9825863225140394, 1.1712328627679027, 1.2246731058456246, 1.073115329671543, 0.9768460964618938, 1.1591512218520372, 1.2740340679676843, 1.4923216510320811, 1.5856438691963608, 0.8843118524451383, 1.068451291953532, 1.0582353812781229, 1.0160607235677461, 1.084849648550025, 0.8312511976655663, 0.9851203991392041, 0.9375213234513543, 1.0334180195718654, 1.2325028956379027], 'auto_ets': [0.9484107726113956, 0.8731249658021685, 0.8818544942322386, 0.8384174072710318, 1.190697466619614, 1.1475214215339327, 1.1480149236404962, 1.2316765778587686, 1.2013486341235216, 1.0444961581087664, 0.9520058196658511, 1.1092094710029208, 0.9470341989083204, 0.9809342185688834, 1.0616730490893234, 1.1239910920389098, 1.0548071303173108, 0.9281608573972003, 1.041738341879741, 0.820484151195746, 1.0132180776507123, 1.2491440730813976, 1.1403259687415805, 0.8291444266163863, 1.147309666954994, 1.0383882452027298, 1.2488381620780167, 1.0520256559251944, 1.2846203014439164, 1.0888145221954888, 1.0206487078939723, 1.017130053280615, 1.139675598221448, 1.0680226060614975, 1.0710137688143613, 0.9658875919083798, 1.0790770109608665, 1.2143760986177763, 1.082792875111452, 1.0523844427084945, 0.8735626237860001, 1.0723589033642191, 1.0713896914417165, 0.9060604035177152, 1.0920062951709877, 0.823464197110636, 0.905530617083833, 0.9338692030180802, 1.0104112352393442, 1.1790565687561718], 'theta': [0.9482947635465648, 0.8706580080310139, 0.8825042089100954, 0.8387243630140027, 1.1948601613531884, 1.1480303606564142, 1.134992515599045, 1.2251821467655866, 1.1973424224867377, 1.0446004153786128, 0.9518708119777977, 1.1072980520763513, 0.9481137833295721, 0.9581853116624378, 1.0606494123789156, 1.1344341576571386, 1.0485291730072186, 1.0030331417466742, 1.0378775669376776, 0.8223576746709216, 0.9865586650851712, 1.2562465637836921, 1.1432366010906, 0.8242746256745825, 1.1443682374627104, 1.0376358625789477, 1.246814457075467, 1.0681550898384544, 1.2840812139441575, 1.0886914662745826, 1.0257152127299176, 1.0107421701156847, 1.1378466534475693, 1.0525965996477933, 1.0790256511320888, 0.9690274746812715, 1.052989788838339, 1.2140223421932403, 1.0348580016601323, 1.032215640918884, 0.8728773789155562, 1.0751283806290326, 1.0682925412583444, 0.9056575846365793, 1.0906714436956237, 0.8239349040311624, 0.9079233276132139, 0.9343304431038391, 1.0066065018583543, 1.171861859721958], 'croston': [0.9494391703117474, 0.9799229443025449, 0.9214333070286678, 0.8369738678494808, 1.1945436813101789, 1.2597892510744397, 1.1387203892081037, 1.235967875852412, 1.2009107981401332, 1.0564571576166537, 1.0573524599894781, 1.1288692668074443, 0.9529748660388733, 0.9367307749203302, 1.0848775191279112, 1.1955272090271714, 1.0291084821796779, 1.0004499199053034, 1.0600255927181164, 0.8135515456746882, 1.0116577709784869, 1.2780059267040957, 1.1858789087829478, 0.8188511101370758, 1.2151914563312656, 1.0400009347335806, 1.2696069510651922, 1.0951189650282245, 1.295901449937707, 1.0689334267189947, 1.0229707708708156, 1.0118855037835481, 1.165799111865035, 1.067853167587818, 1.1722064385691895, 0.9643154686813318, 1.0547102514541973, 1.2444083627477318, 1.0716337502716144, 1.0468069398575712, 0.8565801977333718, 1.0314102319351017, 1.077736833821349, 0.9189434153043177, 1.091887185529587, 0.8170635005376913, 0.9612401419037344, 0.9490897439222652, 0.9583848722168733, 1.185774639024266], 'prophet': [0.975796128997009, 1.0212581835426136, 0.9508486030918859, 1.5635403962329038, 1.433245905448573, 1.3112542020131621, 1.1321704384888363, 1.2162950283182679, 1.2207843709793638, 1.1474216086091609, 1.0844250345869266, 1.0515222954242298, 0.9535792090325149, 0.9185260889288561, 1.1124652114750369, 1.2459254739956296, 1.152581497504738, 1.2623575893543657, 1.1087388350313614, 0.8347940511970272, 1.0100815356373554, 1.3755589784665196, 1.190150106222625, 0.8236981958294601, 1.2163165897462243, 1.041822661674794, 1.3255099481972619, 1.0240681806240097, 1.4104844546050939, 1.0466317314927722, 1.0286159206292298, 0.9784963530531237, 1.2197506400906262, 1.078183369457491, 1.3845375202664525, 0.9659409886128736, 1.53584013338056, 1.2997807807481703, 1.5695808198205985, 1.0724614251086662, 0.8608243662507358, 1.0547546894942381, 1.1637960920203043, 1.1221097181494057, 1.1104194797981284, 0.8119066452840772, 0.9394195194508331, 1.0020762837197188, 1.1781920746581553, 1.2059993436399747], 'Avg_ensemble': [0.9418520119801733, 0.8823683242568958, 0.8986709511427022, 0.8468702914499409, 1.2217592733877969, 1.1980935097990086, 1.1475373996800284, 1.2289279671565132, 1.201413980199028, 1.0565573525342895, 0.9976586219220153, 1.1002937347094672, 0.9434601080594894, 0.9516917891097202, 1.0748760033323865, 1.1498554225025945, 1.0543019616847993, 0.931502940798501, 1.0498285276642982, 0.8208635261616385, 0.9995792399242567, 1.263174145115808, 1.1576869807626307, 0.8181991421427104, 1.1725277423043876, 1.0392786344005325, 1.2629117618906331, 1.0584552313434925, 1.2990323982537215, 1.0805387406704523, 1.0262880769053815, 1.0008513789132938, 1.1557541924118895, 1.0505055206306189, 1.118689028868789, 0.9660791737991178, 1.138735882681822, 1.2358957895612028, 1.1380512692932523, 1.0389312693571677, 0.8669989922494206, 1.0756247199541158, 1.0769195641039253, 0.9185933131746724, 1.0930640669003748, 0.822876157307677, 0.9375326324121707, 0.9293341331894394, 0.9847107563265909, 1.1826832498462443], 'Weighted_ensemble': [0.9670576717911864, 1.040903460180118, 0.9256956397210645, 0.8470880944518366, 1.3956746493630057, 1.2667321779271545, 1.1448527761328458, 1.2623851791832623, 1.19418818489584, 1.0574728373464843, 1.0830781822748392, 1.1685454942329148, 0.9491065535119141, 1.0867334167587408, 1.1033793456643166, 1.2075931297634808, 0.9343940860303122, 1.2447338602056797, 1.0924635370442077, 0.9993863744096585, 1.0269911076327336, 1.4000481260983406, 1.1929219295386846, 0.8249107745347988, 1.258501148040875, 1.065839899942132, 1.2780392018672486, 1.1951993159463532, 1.2962559497874035, 1.0600644385332352, 1.0207897406999, 1.013803540009783, 1.2013537102438878, 1.0913563903321983, 1.0976383468104745, 0.963308953657616, 1.5753816365606415, 1.2685881055118613, 1.5408715546562362, 1.0620644916935493, 0.8516681321601751, 1.2755455906418636, 1.0440687957274923, 0.9229840164197755, 1.08920550104542, 0.8160234236199942, 1.0374859908429614, 0.9627533144711615, 0.9594377647113768, 1.2043026549613525]}\n====================\nWAPE_dist_median is:  {'Lstm': [0.5427279683100066, 0.6247616428966513, 0.578603127483893, 0.49189284807981165, 0.9076921113594492, 0.7110541659746721, 0.7300973341081253, 0.9574390324002704, 0.7865194120761066, 0.6233658320592131, 0.585415914691552, 1.2102227559156207, 0.48476589932426545, 0.6345042914790358, 0.7623211718076116, 0.652479499224424, 0.5631560384063736, 0.7140692965533236, 0.7288698566242793, 0.6441052483570622, 0.5832713103206184, 0.8085983372330642, 0.7661644313453942, 0.4897687134516478, 0.8597723410564776, 0.7353483077603917, 0.8389193552938221, 1.2429995782282797, 0.7953656817911732, 0.6193741302473688, 0.5810247464704242, 0.6696133322007785, 0.7330839147247714, 0.7279557189837264, 0.6930641111182015, 0.6430927212776439, 0.6848457564762244, 0.7362370118717498, 0.5922124485244027, 0.7282585437444336, 0.5200803266281717, 0.7294182541808956, 0.6519950379708948, 0.6763622168526615, 0.6625432403807633, 0.49408113418059724, 0.5738008989644721, 0.6291540664293587, 0.5629430763636241, 0.8933858787308875], 'naive': [0.5553374453451911, 0.5270305655330687, 0.5120747705430386, 0.47714244392814, 0.7864723264399952, 0.6339674640606461, 0.8022754386625601, 0.7187171763272733, 0.7540557713151823, 0.5984706163545146, 0.5286054281047986, 0.6510236822541439, 0.5211510619597203, 0.7990091199807985, 0.6565396344338768, 0.6039940127036945, 0.6022617564244946, 0.5943934417181049, 0.6788674837244285, 0.49895221057059763, 0.605628990372493, 0.76222703767065, 0.6652309993668838, 0.5289162633132288, 0.746355179783361, 0.7054028838259309, 0.7882572237371454, 0.6705760727410377, 0.771820401274863, 0.6166263957520534, 0.5506707503061512, 0.6551026094232301, 0.682591909831072, 0.7016223219270925, 0.6231315269939035, 0.6242702419776947, 0.66473577065854, 0.6961679017530552, 0.5709054932512901, 0.711479439521019, 0.5364732822444829, 0.5172219866089975, 0.5997904608967946, 0.5504481164006035, 0.660793160936227, 0.5056305604298934, 0.5457501882094926, 0.5093900877076123, 0.7442907921518357, 0.7661279949935942], 'snaive': [0.5875673529394828, 0.5808339179010029, 0.5282586210006305, 0.4835329724715467, 1.2822395629292989, 0.7014418985097206, 0.780022822530515, 0.7321881625785711, 0.7562289662419024, 0.6051366369903504, 0.5935960570596469, 0.750317489019793, 0.4952442342626337, 0.6180688320382854, 0.6700261384409676, 0.6552064652973714, 0.5874542448459258, 0.6358813036762503, 0.6841854459850736, 0.4664978402459369, 0.5916606959475141, 0.7767931079914662, 0.6943760419216174, 0.4961619931283867, 0.7812757267523593, 0.7060801570702347, 0.79415066062192, 0.7937072515447611, 0.7771010222563699, 0.6236597516181266, 0.5528657876825126, 0.646184969553846, 0.6995010629124111, 0.7134282191727305, 0.6615875796341905, 0.6360559109859092, 0.6583650565664323, 0.7077699804011888, 0.9066850154786068, 0.7229503344787843, 0.51935532209413, 0.5511051061681593, 0.6188120079817269, 0.600567403249606, 0.6661406152905045, 0.5089066208627779, 0.5873471246526254, 0.5547213753086238, 0.6430219947837834, 0.7904419149203268], 'arima': [0.5761531211298461, 0.5237379303242156, 0.5110302435040696, 0.5105841197158034, 0.933228376920755, 0.6888184474967226, 0.7649136438680059, 0.7132249419710077, 0.7201481144165602, 0.6019867553543143, 0.5775493403572889, 0.6519489014067557, 0.4959981845076596, 0.6708244318593646, 0.6669171335251041, 0.6538048468391346, 0.5710253804594692, 0.592249378508053, 0.6719267847016981, 0.526249892598033, 0.5748285390257067, 0.7491294664927785, 0.6895955164221687, 0.4944116221048944, 0.7693917836574745, 0.6828990161960191, 0.7933024655272598, 0.7185345279811519, 0.7734602425119782, 0.6198953078165514, 0.5427338788725586, 0.6227842139659245, 0.69200207036541, 0.7009379094170891, 0.6213438550540415, 0.6327589837276411, 0.6419470280678636, 0.7111933238262785, 0.577660636585265, 0.7048685597433517, 0.5205447808871833, 0.518977308970034, 0.6065900108989845, 0.5512477911389365, 0.6600222787078448, 0.5092649352619334, 0.5479783444465975, 0.5542826152935019, 0.6517882119936214, 0.7670332810954614], 'ses': [0.5700221472366023, 0.5270305667898942, 0.5120747709201593, 0.48034576243480487, 0.9316356461170274, 0.6352084167419072, 0.7852250377185914, 0.7174659194688143, 0.7506683269816805, 0.6000000000000001, 0.528605428830206, 0.6578833870991856, 0.4959491675449307, 0.6832569255749636, 0.6575031553674582, 0.6132548717843391, 0.6073199700522777, 0.5943934418535408, 0.6789027084076356, 0.4991504087234826, 0.5807671791955679, 0.7622270381423326, 0.6696896538809609, 0.5065269373244343, 0.7463724543467807, 0.6828976432846151, 0.7888437215832343, 0.6744250252258495, 0.7730871153784236, 0.6264434138148267, 0.5474186300870372, 0.6536639177127339, 0.6834055057623205, 0.7092030053662582, 0.6231315279293177, 0.6252499568975045, 0.6602432778064709, 0.6961679021879892, 0.5717632169916982, 0.7116127045832348, 0.5319120980470403, 0.521436689672977, 0.607373304163669, 0.5505949622791422, 0.65921614092704, 0.505630560293463, 0.5469007126845129, 0.512628757301383, 0.6792904241188376, 0.767005135848885], 'holts': [0.5468780981135389, 0.5197598769281448, 0.511313018780786, 0.47581986347613003, 0.7467417185134785, 0.6297598248447199, 0.8026425455142987, 0.7173734347888103, 0.7483141150984421, 0.5994403416149259, 0.5223163286855623, 0.6430770303711743, 0.5244396712302383, 0.8190789172345898, 0.6565706730888976, 0.5988806824134963, 0.613623768817872, 0.5942715666563634, 0.6832374552045397, 0.5006624390258356, 0.5994471230568309, 0.7642897863073478, 0.6601090040093203, 0.5441756263843804, 0.7449416382446866, 0.6988063399020684, 0.790828594352053, 0.6607450062432043, 0.7739823774369388, 0.617382699094831, 0.5460782640753221, 0.6579223755736214, 0.6824353144112205, 0.7016258496161166, 0.6176130238606568, 0.625142425039166, 0.6987951940617261, 0.6969021504492158, 0.598934562971012, 0.7096176217366226, 0.5420784933960441, 0.5146875153197057, 0.6017308725301482, 0.5550283038413427, 0.6589595662896857, 0.506049648829696, 0.5476499803634343, 0.5042524875884199, 0.7669882696849917, 0.7651534474183594], 'Es': [0.593330210886281, 0.5361577161198321, 0.5270294257547493, 0.47473472932300786, 1.454760996971745, 0.6643334269552554, 0.8069661900064727, 0.7154342589142282, 0.7799983463041139, 0.598079278486802, 0.5472951865623126, 0.6306529932261128, 0.4941249314591171, 0.5955470015497883, 0.6694205910330089, 0.6089969847320715, 0.6253942486353858, 0.8383387203029904, 0.6821131624685106, 0.5319577862370655, 0.5920498958592166, 0.7504177595813093, 0.6769598061060909, 0.48481790126116736, 0.7602685206858422, 0.6776389617656383, 0.8076256587944879, 0.6338801379972807, 0.7777649943140754, 0.6475292571126974, 0.6638978994563232, 0.6028425699162633, 0.7000776393631618, 0.7375492860016343, 0.6247685763210169, 0.6246317730261043, 0.7191535019972548, 0.7243349076547135, 1.2787390643089391, 0.8928835732342868, 0.5467291132269216, 0.518804590573718, 0.6016218834467477, 0.6002843551904521, 0.6608151095967996, 0.5163328947007522, 0.5910947528417689, 0.5125502210612134, 0.7495045729442419, 0.9215284947428715], 'auto_ets': [0.570023345893671, 0.527039000152556, 0.5120773014227402, 0.48034625638366113, 0.9316358792705505, 0.6352064427047364, 0.7852243936797139, 0.7174659731082669, 0.7506620832752049, 0.6, 0.5286102965776985, 0.6579799167285993, 0.4959490423316072, 0.6832267036137107, 0.6575030915507363, 0.6132695241848354, 0.6073156475609285, 0.5873697977670824, 0.6789027147219151, 0.4991505991131373, 0.5807669059496278, 0.7622302030858004, 0.6696899105931774, 0.506571791246474, 0.746375248156167, 0.6828972029623929, 0.7888440420211839, 0.6744986274231629, 0.7730873663660641, 0.6264435321834234, 0.5474223675530975, 0.6536638634432147, 0.6834054475614237, 0.7092041274610429, 0.6231378044728215, 0.6252399631579327, 0.6602439517785932, 0.6961708204783627, 0.5718514905211098, 0.7050278522455735, 0.5319159462324439, 0.5214358708902829, 0.6073723160698339, 0.5505937773809007, 0.6592159538872406, 0.5056296448836255, 0.546900713477908, 0.5126288635193716, 0.6792847346341695, 0.7670067868297354], 'theta': [0.567092373797725, 0.5276155385819139, 0.512473631681697, 0.4786968723428159, 0.9390641965171266, 0.6356116795959044, 0.7715102817094542, 0.7150195547744108, 0.7310003017726365, 0.6000302379871855, 0.5296945090636388, 0.6520952730714711, 0.49646300614389166, 0.6551348833863346, 0.6568800418609575, 0.6203818340057279, 0.6034055641132663, 0.6248276259234301, 0.6764124624070649, 0.5001891053466057, 0.5693966315421164, 0.7667336056036586, 0.6670618490503871, 0.5057899796622856, 0.7445087719774867, 0.6810565536863489, 0.7877152665743579, 0.7048848073098615, 0.7729259432113598, 0.6264741787756376, 0.5314998295054681, 0.6441192973187695, 0.6822408375814873, 0.6895862164730119, 0.628599083114608, 0.6237668313010502, 0.6198933831518749, 0.6959320801762776, 0.5587487065219257, 0.6968460917157784, 0.5297581685608272, 0.5221948694831074, 0.6055537518329911, 0.5528121907787121, 0.6545903232067667, 0.5060651503442157, 0.5472209949904401, 0.5116928360995815, 0.6771656919505242, 0.7563667647025459], 'croston': [0.5737097195524604, 0.5656017994053951, 0.5249480952834372, 0.4824605806363743, 0.9408194820188998, 0.694842107899062, 0.7747103470618693, 0.7208180194992371, 0.7489226633431221, 0.6026493974059508, 0.5746270624777398, 0.7028595780998144, 0.48846707501568287, 0.6157598364542172, 0.6701080147590903, 0.6440786207604895, 0.5941098989528494, 0.6155434503860031, 0.6903726374949217, 0.4912009244068311, 0.5786621548606112, 0.7778571464361805, 0.6888907782033449, 0.48565776818508355, 0.7795023646289854, 0.689450713186632, 0.7989498898460907, 0.7409387394190888, 0.7766467398530024, 0.6298214456547979, 0.5507765412655463, 0.6488853402018991, 0.6994766166579787, 0.7110229343130336, 0.6549481842879954, 0.6331659620285467, 0.6729212731132285, 0.7102952943086851, 0.5610949531149307, 0.7188936616231163, 0.5195352637247297, 0.5367879893100014, 0.6158362057777936, 0.5599976032408243, 0.6591045386962836, 0.50319331133578, 0.5691313084928471, 0.553235775561997, 0.6410162197367117, 0.7758196154233227], 'prophet': [0.5399313038952156, 0.5918220499131319, 0.5315698449159157, 0.9513474773981114, 1.2206475672085382, 0.7293916957337068, 0.7687148050163497, 0.7096578453545141, 0.7997838743666096, 0.6362944683976156, 0.5811153395357689, 0.5504724605738959, 0.4928794732017619, 0.5821733250423764, 0.6884280834314517, 0.6784690765805125, 0.6624802682832347, 1.065595175989896, 0.7146248041813321, 0.5344072894977243, 0.595944061784091, 0.8299122990447044, 0.7044346584772689, 0.483587265952909, 0.7784313636842812, 0.6657108887927026, 0.8248726214172905, 0.6135603231043969, 0.8321290557802237, 0.6300534265497877, 0.5446032039721885, 0.5933755858789397, 0.7271041318036098, 0.741280016883304, 0.7557948137636936, 0.6257331969316677, 2.7497380137248357, 0.7284731118928034, 3.4542322627726865, 0.6869453843205532, 0.5156078994865831, 0.5186966175804469, 0.662657881383183, 1.1671185187296451, 0.6704110209776375, 0.5001435619479957, 0.5519604003446487, 0.5757216488910288, 0.658041018809421, 0.8098006498501391], 'Avg_ensemble': [0.5503962005176505, 0.5384105979486775, 0.5157348290164325, 0.48238435521420375, 0.9384464458655881, 0.664433357275217, 0.7842895992750402, 0.7167494926053541, 0.7529743469434358, 0.6046409987780978, 0.5441736937205224, 0.6479095236453043, 0.49730228588363884, 0.6417003489450043, 0.6649751988698477, 0.6267209867059957, 0.6075546344391074, 0.5837772786863583, 0.6836504741899866, 0.4978125717857367, 0.5703640130430887, 0.7693815990556341, 0.6786741395500292, 0.496198985810777, 0.7593820660727307, 0.6771265587910094, 0.796188454325723, 0.6704610408810514, 0.7777704842019368, 0.6210251782439729, 0.5352978405941166, 0.6329956356689909, 0.6931334580014638, 0.6987902270425564, 0.6255091242032156, 0.6249121222758491, 0.7326925715766553, 0.7059415956869693, 0.6712106051598026, 0.6934839144025959, 0.5245435459928406, 0.5167000241978871, 0.6101840736719429, 0.5676367679313837, 0.6580942815193518, 0.5065995024871999, 0.5441872643652903, 0.5300774570910767, 0.646287462939063, 0.780092650765869], 'Weighted_ensemble': [0.6020322803118466, 0.6030761623554171, 0.5224712084486003, 0.4903320062131551, 0.8768413162090438, 0.6967152303716171, 0.7851921747105645, 0.7741799973888541, 0.7316047420459337, 0.602703216952506, 0.582193923525476, 0.8418625712451177, 0.498524529357044, 0.8968873309436061, 0.6785447981823476, 0.649878537790892, 0.5397219199613201, 0.7120369605229077, 0.704288247436203, 0.5302394846395421, 0.6111020780590595, 0.8290912858103348, 0.691818526906853, 0.5050271584315336, 0.7977008407406991, 0.7365597045265173, 0.8007219762057964, 0.9900329499359816, 0.7775170599547783, 0.6364262547770433, 0.570837592597882, 0.6548353983523046, 0.7181926879261645, 0.7643978229045814, 0.6211851537529255, 0.6394767474499323, 0.9622247874997736, 0.719178781402424, 0.8502398946486267, 0.7389894802404753, 0.5189807665199753, 0.6759420386163533, 0.5981439493831968, 0.5703982034029078, 0.6584173753581706, 0.4999894771836396, 0.5989355661709308, 0.5585539368828243, 0.6360151734242283, 0.8305292535034252]}\n====================\nWAPE_dist_80 is:  {'Lstm': [0.7897630161604843, 0.7256592711585479, 0.6947835530758372, 0.672472769132755, 0.9595254393599582, 0.7954343065526014, 0.928340561243481, 1.6655635123394485, 1.2195693144012472, 0.7548279447292243, 0.7109674103575941, 1.778649324880059, 0.6833489802005505, 0.8646655879859566, 0.8237224282426098, 0.7736699225380661, 0.7016421262384229, 0.7977947254976305, 0.7838658836123323, 0.7339788535298757, 0.7922942405908907, 0.8578297931506103, 0.8408952561240036, 0.6289023997510849, 0.8953142779057772, 0.9802768370300218, 0.8792192357525416, 1.8041162310864811, 0.864303686656243, 0.8994254081602466, 0.8275168466145346, 0.9075203001161056, 0.8109220883838107, 1.0139737339383321, 0.7551413888166204, 0.8569912886686817, 0.9062596275532231, 0.8293179246773597, 0.8127293208942904, 0.9296209049719009, 0.6704405294191191, 0.8364340568885698, 0.76423862870519, 0.9163013882836601, 1.0415033452575173, 0.654555786991688, 0.7012753297992218, 0.7478248348104121, 0.72394770919621, 1.5067332406187106], 'naive': [0.8005348833629503, 0.652995295704079, 0.653392968478017, 0.703098342437793, 0.8822174226067294, 0.7425908633094471, 1.1301483824365726, 1.172336336935105, 1.1296924237171462, 0.7379959386491846, 0.7011126632548397, 0.9075976933059684, 0.7309823411832341, 1.14639511981273, 0.7270393313782566, 0.7396764595104992, 0.7255618234292431, 0.7171652118175023, 0.7450645777891458, 0.8281442729776232, 0.8158535930419691, 0.8257285857205734, 0.7730529261155626, 0.7413045562566963, 0.8083541063680364, 0.9071864019824689, 0.8436045404600901, 0.9040845891143003, 0.8472566104922729, 0.9189104545718396, 0.7892619790888512, 0.8839098460577841, 0.7727477507140681, 0.9081260263264584, 0.7002995862597561, 0.8046558255779631, 1.252518492452244, 0.7968868173274699, 0.7419132512632651, 0.8917004530716308, 0.7458695159499595, 0.8506740844401185, 0.7341441181019271, 0.7290263465653746, 0.9636327685468059, 0.6828748995951784, 0.6962821259896972, 0.6623463883405942, 1.0879820457472953, 1.1422070097037609], 'snaive': [0.8667007157851465, 0.695427592502594, 0.6658587655958026, 0.677592729823479, 1.87680165765308, 0.7885841734935948, 1.05360741828552, 1.1925841057611448, 1.1162909117783073, 0.7419881112748686, 0.7149657090645719, 1.0575265799125964, 0.6981463174747026, 0.8213108136068223, 0.739048471674131, 0.7758956392229158, 0.7160088385869359, 0.743518707424758, 0.7505891321557185, 0.7160978270229486, 0.8031228417288795, 0.8358122777198425, 0.7907331684496894, 0.6496701220139105, 0.8340302478661227, 0.9221969144739628, 0.8491511953494357, 1.1068353381934306, 0.8510223019748919, 0.8691154726228821, 0.7905083504123993, 0.8603696489934183, 0.787327255785269, 0.9370525808526852, 0.7437606869284039, 0.8359711606948764, 0.9195982039516831, 0.802928448479125, 1.288857611258209, 0.9239648948382143, 0.6751309150946307, 0.690039823845614, 0.7384960053382081, 0.7842476463368041, 0.9527861000734801, 0.6821453166368514, 0.7164857210249824, 0.6824993048656636, 0.8486970984813609, 1.2211999771509872], 'arima': [0.840850157742372, 0.6491755021413571, 0.6502601058064031, 0.6595939772559181, 1.5014623093197148, 0.7791377731923688, 1.015753325202905, 1.1330163271566207, 1.053096176973417, 0.7434773176433489, 0.7015610614733898, 0.9293666329432129, 0.7069521718137457, 0.9303767990463652, 0.7362091084303151, 0.7753759290430381, 0.7056691044885939, 0.7101169377551921, 0.7396562189206416, 0.8609730069937045, 0.7929380938366882, 0.8144450673145416, 0.7897061260376388, 0.6801598485282203, 0.8242996577136675, 0.8658454774022757, 0.8468650516164811, 0.9765383334806765, 0.849142718176455, 0.8957447897886299, 0.7807032041785208, 0.8234008327370448, 0.7799601071694223, 0.8969291468827905, 0.6980788253696012, 0.8269833202314425, 0.9016263138621055, 0.8081256126655743, 0.8862309844022673, 0.885023291613267, 0.6802268910307789, 0.8570011054333915, 0.7310463255049482, 0.7249249339160877, 0.9322460542743576, 0.6880462276852747, 0.6900387760935465, 0.6826455560038739, 0.9199176233536612, 1.1502166331335395], 'ses': [0.8250879591697962, 0.652995296884312, 0.6533929689923687, 0.6900767434094066, 1.432031561285526, 0.7435642409725126, 1.0750455755110067, 1.1586668717444615, 1.1253686743953395, 0.7389160484238246, 0.7011126639290877, 0.9317549275647077, 0.7145633524853361, 0.9498767769078941, 0.728011478155917, 0.7463508708713273, 0.7293622106960812, 0.717165211710378, 0.7450886637719398, 0.8291881024100323, 0.8009478672985781, 0.8257285860810762, 0.7761316308788744, 0.7027069155446733, 0.8083646637778099, 0.8726587684170144, 0.8439941766598148, 0.9049442287769974, 0.8487293781090253, 0.9107635099887565, 0.7832986148716536, 0.8799952945763163, 0.773366499539231, 0.9303485523763262, 0.7002995872351464, 0.8106343036322871, 0.95914540883635, 0.796886817659729, 0.8647757491220838, 0.8917383000996042, 0.7341058627258511, 0.8326954619453154, 0.731747136043264, 0.7380367553131996, 0.9286650814373889, 0.6828748993186351, 0.6969997336455346, 0.6658409129510405, 1.0078926807624002, 1.1563379209945865], 'holts': [0.7935121984249356, 0.6489752707301557, 0.6503385621064277, 0.7020929827139166, 0.9052225370183733, 0.7388562955741449, 1.1273381761158718, 1.1625749905798775, 1.1194468765912395, 0.7392491767328193, 0.6981235854178933, 0.8880837765447815, 0.7311459044730442, 1.1914799710531623, 0.7278308323405202, 0.7373437970407271, 0.7335008954853632, 0.7172146163095288, 0.7482312306021732, 0.838225750911942, 0.8133584388992705, 0.8275288978120914, 0.7699671523924777, 0.7658468160742649, 0.8070049172505029, 0.8945643488297901, 0.8451860156948936, 0.878763351020931, 0.8497285973338669, 0.9189286969528152, 0.7807560835435874, 0.8855630790026281, 0.7725693470922412, 0.910840176918531, 0.6976480031606289, 0.7904160037185026, 1.2832459534767982, 0.797659070758891, 0.7190684647371588, 0.8911047361758424, 0.7624773886734149, 0.9313130743451103, 0.7350969368320294, 0.7253432785868507, 0.9468635598708917, 0.684951009499149, 0.7027149465744086, 0.657289818285296, 1.1311096773193479, 1.1180389732450084], 'Es': [0.8639265124723032, 0.6576117483536005, 0.6649519684979865, 0.6892242322195199, 1.9420421587941685, 0.7592935487804041, 1.1314356018778646, 1.1335289620956117, 1.191724053368743, 0.7400730190669896, 0.6922370800303983, 0.8657146979773348, 0.7026699390794544, 0.7280278160551719, 0.7397570046573148, 0.7446176622533756, 0.744482873822019, 0.8859054708840605, 0.7476835225896256, 0.8679540858925321, 0.7234910627746457, 0.8155874584234861, 0.7795978675771721, 0.6629806250810422, 0.8188599983611391, 0.8608327007334902, 0.8558655389270371, 0.8096625103032287, 0.852382609111893, 0.8057570434474053, 0.7768421222742483, 0.7794432797268769, 0.7865708473321168, 0.8417664738418674, 0.7016038055680572, 0.7788283538122636, 1.3006183784961391, 0.8185612634276589, 1.5436871287891822, 0.9486958248676496, 0.7535131440164275, 0.8330338629444444, 0.7355525909307279, 0.7182772583486902, 0.8707369799538798, 0.7089510518792824, 0.722713955811215, 0.6638558407773065, 1.0924936001287322, 1.50906399942119], 'auto_ets': [0.8250923635974198, 0.6530032163077601, 0.6533964203238448, 0.6900780467476277, 1.4320261937589078, 0.7435626925785108, 1.0750450871936188, 1.1586657511881069, 1.1253486295666262, 0.738916226447016, 0.7011171883793587, 0.9317996188186872, 0.7145635593996813, 0.949819849772769, 0.728011413767881, 0.7463568725032608, 0.7293587184600303, 0.7201832527512732, 0.7450886680895268, 0.8291891051152658, 0.8009478672985781, 0.8257310050196772, 0.7761318088354466, 0.7027378224358716, 0.8083663712241749, 0.8726580197157134, 0.8439943895407755, 0.9049501651705423, 0.848729661133818, 0.910763444145017, 0.7833068401743322, 0.8799952149025654, 0.7733664552767964, 0.9303502064317977, 0.700306132016226, 0.8106504342811984, 0.9591407157722852, 0.7968890470293104, 0.8648181275965953, 0.8840410230994784, 0.7341344374136551, 0.8326957032045492, 0.7317475975264097, 0.7380397407592716, 0.9286790761155642, 0.6828730437918105, 0.6969997355249105, 0.6658410055231142, 1.0078738346610043, 1.1563416325566926], 'theta': [0.8190897510601394, 0.6545959312339871, 0.6537067460173251, 0.673192546649243, 1.3071773981569543, 0.7438349000525273, 1.0378695703662417, 1.136827190067209, 1.0722699424828612, 0.7389703844676998, 0.7020902027253219, 0.9314334640101646, 0.7141281853511747, 0.9010237276564248, 0.7275232872008341, 0.7472590115064562, 0.7265051875563611, 0.7309212654534932, 0.7431145794055158, 0.8379050726330228, 0.7858736044096386, 0.8288835282507229, 0.7743414539191699, 0.6994239529702118, 0.8070310480114269, 0.8651200477613692, 0.8432411284652426, 0.9351433474755004, 0.8485420209912459, 0.9101007978750045, 0.7865095635826205, 0.8609776596168243, 0.7724956173977058, 0.8453709243588416, 0.7044762445515691, 0.8027913640131064, 0.891541034188464, 0.7967266834311053, 0.7753713509411995, 0.8444610614583821, 0.7272162114798775, 0.8382503910665192, 0.7312207644487769, 0.7206206613916191, 0.9169979990079883, 0.685603353853176, 0.6977711021966845, 0.6651075096079939, 0.9978247706967837, 1.0960191076737351], 'croston': [0.8374141767676734, 0.6846176346791739, 0.6654329563050041, 0.6808434365115377, 1.210878962595029, 0.7831163432692576, 1.0452490778377252, 1.1763590872212273, 1.1202972086822158, 0.7431174144019876, 0.705462770448436, 1.0075439880664185, 0.6989334997829869, 0.8258887610395023, 0.7406284259984629, 0.7686159082160657, 0.7197523107714223, 0.7265290031645285, 0.7535125043700013, 0.7708864027110353, 0.7996050597290449, 0.8364222031781048, 0.7880216445354582, 0.6443286324586645, 0.8327077763256789, 0.8834443672056526, 0.8505716047318159, 1.0169099446375987, 0.8528007980103084, 0.8595201915435007, 0.7898646862038451, 0.869810809562511, 0.7854601765612105, 0.937014830311432, 0.7369903623001303, 0.8211289008568987, 0.9198494097114625, 0.8077703076113506, 0.8393994127843535, 0.8990791064061943, 0.6815150172519773, 0.6777121540116389, 0.740376202741635, 0.7547760118291833, 0.9370153753789927, 0.6782905522266728, 0.7139039379084349, 0.6738853750831204, 0.8690930348526837, 1.1863469516530063], 'prophet': [0.7038676431596508, 0.6975516979715044, 0.6655996711822517, 0.9926568409886299, 1.7517843420412618, 0.809640866901659, 1.0343355108215104, 1.1065456720792397, 1.2234739475225547, 0.7636079084625406, 0.7116361322831617, 0.7908127552343525, 0.6965104170633774, 0.724903331194769, 0.7567422961658029, 0.7948242384186577, 0.7748720525208985, 2.510577424334356, 0.772338444865659, 0.8663736059397418, 0.7912868759273239, 0.8736405714599605, 0.7970923861615229, 0.6241250469528188, 0.8321812425575885, 0.8283440857971134, 0.8697518361102876, 0.808698619270316, 0.8897719481812696, 0.8026739648210968, 0.7842592214343658, 0.7785402027908628, 0.8084039653035526, 1.0197977049450393, 0.8241804149015027, 0.8034656989706034, 5.869661625020713, 0.8247590983149844, 4.799962006564612, 0.8118619008883643, 0.6862052493249013, 0.7044223630855919, 0.7719559821779487, 1.6434291465093278, 0.9837730032210952, 0.6674705449667414, 0.698288263032089, 0.7023871356104823, 0.757562128048425, 1.3015925567659374], 'Avg_ensemble': [0.7945294983143133, 0.6564608622576708, 0.6610696888174172, 0.6728675886951412, 1.275063299837578, 0.7591198495813781, 1.0718496711849195, 1.1550107921904793, 1.127134892392797, 0.7442668543501103, 0.6861093922200008, 0.912946441936866, 0.7142676037794686, 0.8896344176189364, 0.7350801649769437, 0.7508593806403048, 0.7294026158426853, 0.7320796460604349, 0.7485214444544163, 0.8293099088411892, 0.7931318188846872, 0.8307305653806181, 0.7822140594832226, 0.6823811415727937, 0.8170105771123908, 0.866861314444338, 0.8489497470812524, 0.9019709913389878, 0.8538441477625185, 0.8937588530793077, 0.7885659651084491, 0.8425425641843487, 0.780896492601443, 0.8883529192473203, 0.7201955614343631, 0.809488912715082, 1.3534542913983678, 0.8046564644033647, 1.1311416113893609, 0.8378696574611995, 0.7137215617507082, 0.8291423185844682, 0.7355361696932502, 0.7584285655345836, 0.9333473814977192, 0.6845080419740313, 0.6911840203014474, 0.6670156366049532, 0.9070036187747451, 1.1964318517647865], 'Weighted_ensemble': [0.8814027540944478, 0.7049984450945392, 0.6634450360183698, 0.7341937658628033, 0.9231764860626417, 0.7844954775001998, 1.0813993890867848, 1.3042795635519189, 1.087197096086298, 0.7435207297757477, 0.7107497476102584, 1.2108578936530292, 0.7180457652309586, 1.344139787419043, 0.7498470242061466, 0.7744021805340306, 0.6776904968405306, 0.7933146176241034, 0.7657355575431641, 0.6372881424560382, 0.8266944004850971, 0.8705718093790468, 0.7890045548877428, 0.699582908224154, 0.8465800054265427, 0.9985949489315952, 0.852751293977071, 1.425402251709788, 0.8536083926682717, 0.8143168093251916, 0.8218569097162182, 0.8821708790757886, 0.8006794993744495, 1.0990276180179648, 0.7091875367477735, 0.8562142844252552, 1.021812239066182, 0.8150833845484029, 0.9111021890927822, 0.9666101462572059, 0.6758617131445428, 0.7824297091747839, 0.7323206076501851, 0.7571416702419116, 0.9404280340696234, 0.6761094775380596, 0.7240584611223966, 0.6870217823950489, 0.8396897839393498, 1.3492089167017753]}\n====================\nWAPE_dist_90 is:  {'Lstm': [0.9266831406524474, 0.77689311840247, 0.7607736392941296, 0.7840772211652337, 1.043976387607475, 0.8489651986330305, 1.1224752033291827, 2.2808190844428458, 1.553287145210414, 0.7958204371406481, 0.7916619741073431, 2.06305480417748, 0.8110016717893551, 1.0262162958407621, 0.8426745328970017, 0.8320623366245777, 0.7425823727434925, 0.8228754812240384, 0.7968787369331671, 0.8059842543812019, 0.9393961766953003, 0.8801816364678879, 0.8756179860019832, 0.7357994440258608, 0.9036079401327841, 1.3565759790588159, 0.8917614714784504, 2.223664981920967, 0.8786931888142104, 1.2349675799137025, 0.9521383869821575, 1.1477412888687364, 0.8311392892386436, 1.1954503984693008, 0.8166822774342739, 1.0371221873554661, 0.9661723746083439, 0.8627937233261178, 1.0833798940886061, 1.1274280539760666, 0.7907365434246774, 0.8660571032844123, 0.8147287459125478, 1.2074613943536752, 1.3606508304300264, 0.7553093316729067, 0.7739168583762001, 0.8028915417584404, 0.8334027307561109, 1.8991566228685854], 'naive': [0.9730131321616016, 0.8192134665506006, 0.7504545691939881, 0.8494545223568497, 0.960566028572222, 0.808712747189007, 1.4267848861430112, 1.561346594564298, 1.4259922189701535, 0.7798142007131232, 0.8223256528431336, 1.0819626453902147, 0.865959703429346, 1.4438604114204066, 0.7648889953012632, 0.8047436307572595, 0.7672609247203517, 0.8001148678145467, 0.7611462990710399, 0.8979038265052051, 0.9756032384581754, 0.8519142906416559, 0.8427008605711508, 0.9200616891386043, 0.8260056713348362, 1.2364054218307214, 0.8572879443714437, 1.1311363103975225, 0.8627044077342542, 1.3987507658243494, 0.9058466470329523, 1.1133784995540563, 0.799453108104654, 1.0630957085746253, 0.7923714912822653, 0.9399609890165774, 1.57048748360823, 0.8358507692575623, 0.9104300078447332, 1.0491654111765727, 0.906676999602506, 1.1587987873814802, 0.7827460223301911, 0.822611891194503, 1.2027831453660423, 0.8032499467515909, 0.8034945935981053, 0.7621735007455928, 1.3515559322304922, 1.4453390977456606], 'snaive': [1.0709049251313574, 0.7608921513215292, 0.7407854813353575, 0.8143451275099308, 2.0784110053480918, 0.8429838411379279, 1.3255637560636178, 1.5942084804228869, 1.4090293856602625, 0.7837902326780659, 0.7945046354350529, 1.2555337281028007, 0.81348769731795, 0.964707883522958, 0.7757376900300137, 0.835575175555715, 0.7600394401018165, 0.7857788041128431, 0.7657587983108465, 0.8631485087248068, 0.9395103085190273, 0.8615764223746871, 0.8392025158140952, 0.7537443445288113, 0.8478237156373104, 1.2588918576269983, 0.8637512671906866, 1.3515502702829507, 0.8657840112315149, 1.2043964466990391, 0.9060972406963539, 1.076132334237405, 0.8098365643303432, 1.113214476030186, 0.8082104053867231, 1.0072731706659948, 1.0030831277227772, 0.8410701108676069, 1.8212831922921546, 1.1235069373387045, 0.8192888184252201, 0.8840582508619897, 0.7949218255479499, 0.9062065237779643, 1.1997778684806026, 0.8072842359949381, 0.7756250419302604, 0.7636650372838736, 1.0301207661977698, 1.5661358155902827], 'arima': [1.0401836016556412, 0.853477001786861, 0.7489875167796503, 0.7350947242491764, 1.659306228202431, 0.8329596064300963, 1.2800341584089139, 1.5091186937013599, 1.3397631811607873, 0.7843211512529146, 0.7952408315136146, 1.1315433638739907, 0.8232005997686939, 1.1452644690111393, 0.7724620354128772, 0.8329563213611816, 0.7477881362134781, 0.8248901086286922, 0.75627631316662, 0.9448029800042241, 0.9347269300494951, 0.8414433608901695, 0.8318098497562976, 0.7978369281956381, 0.8402865106289543, 1.1735686797689706, 0.8614160796520005, 1.2311996654353403, 0.864473945838833, 1.2366490244248856, 0.9053965734194659, 1.015244421919058, 0.8051040440947401, 1.056590390750467, 0.7905158782372377, 0.9796885757627989, 0.995064053616997, 0.8450860434754395, 1.2540499513747423, 1.0339981989474716, 0.8228234446501811, 1.183142474220443, 0.7886636729787447, 0.8235846862003071, 1.156857179357695, 0.8191578863176623, 0.7934680919936681, 0.7651702597456929, 1.105069820201078, 1.4601085556776123], 'ses': [1.01756803927271, 0.819213463859675, 0.7504545692153284, 0.817191660624927, 1.547101540155524, 0.8095340601705524, 1.3592930892195039, 1.5387232661967656, 1.4199313199014394, 0.7805237939073222, 0.822325651206915, 1.1326067723633764, 0.8290148652247902, 1.1705666662518357, 0.7655818756151438, 0.8117204295766254, 0.7709274152381088, 0.8001148674541942, 0.7611721095634331, 0.8993580547480912, 0.9396121591870636, 0.8519142909645594, 0.8449089431314105, 0.845535216023658, 0.8260166067382518, 1.1850069719059695, 0.857758603752817, 1.1444183730198165, 0.8640964117221077, 1.3200896116173078, 0.9046465282287554, 1.1060693209165735, 0.7999606051020611, 1.0993161024237208, 0.7923714920298698, 0.952818252316135, 1.1196845607291068, 0.8358507695337185, 1.2133333804436388, 1.0497147349409173, 0.8914808736810682, 1.1115473485900447, 0.7892838623078338, 0.8219633270418092, 1.1523730290495322, 0.8032499463791338, 0.8021458035316509, 0.7577495269622923, 1.2112007926662147, 1.472520448819971], 'holts': [0.9460244988035452, 0.8699930501294686, 0.7487456364331829, 0.8472999191078256, 0.9732703089580302, 0.8050497395317007, 1.4233324673922527, 1.5451608401613321, 1.413521371160369, 0.7806495985479833, 0.8374004418692087, 1.036174104291237, 0.872237675757138, 1.506807182331628, 0.764949684050788, 0.8033407119179597, 0.7736315344631713, 0.8000284385925249, 0.7642594536074908, 0.9124759165078433, 0.9660640809453347, 0.8536967534455686, 0.8406423171732694, 0.9592290556299771, 0.8247840894107009, 1.2099029505243994, 0.8595030077564035, 1.07665101984974, 0.8650778373943254, 1.3844834798531642, 0.9048216477988009, 1.1176849072228223, 0.7992735417288204, 1.066443855071669, 0.7907469279502214, 0.9168310947418485, 1.610424513709244, 0.8365362522120798, 0.859280382558097, 1.0438574069172937, 0.9311447064371987, 1.270417880022543, 0.7853294200521401, 0.8071946176037125, 1.1781432025820704, 0.8062353987921101, 0.8027083304504579, 0.7638088884979003, 1.4085487080111003, 1.4152497793338332], 'Es': [1.0669549311696667, 0.7839103399658219, 0.7430100042320612, 0.8255943209401068, 2.088071044375146, 0.819386758765155, 1.431747173100424, 1.4975838061627367, 1.499364106897146, 0.7815243272763881, 0.793160812844296, 0.9693073457955621, 0.8204115997568784, 0.796627579146122, 0.7745306700058273, 0.8095495263242214, 0.7836550360166098, 0.9228472444189533, 0.7627812591577123, 0.9490830152681561, 0.835876663034459, 0.8424783508522038, 0.8498926002932857, 0.7720697736457389, 0.8358235004488754, 1.1518255416760423, 0.8697138441573973, 0.9589140223590809, 0.8674820449834535, 1.0816731405044804, 0.8545899466641605, 0.9345631296953212, 0.8096718051278586, 0.9088309132291483, 0.7936212919826358, 0.9155264103447083, 1.5260253623026272, 0.8534743347607843, 1.9213475028102147, 0.9712731029594598, 0.934738293670414, 1.1000029366303754, 0.7829527921184303, 0.7877570475833288, 1.09231363392473, 0.8372116957263067, 0.7868089498489442, 0.7537194131742835, 1.338591799533471, 1.949659154816231], 'auto_ets': [1.0175718601921417, 0.8191954076125959, 0.7504547124102139, 0.8171878770032684, 1.5470927499033507, 0.8095327536724248, 1.359292184439472, 1.5387217382687666, 1.4199083913206114, 0.7805239231297489, 0.8223146715850456, 1.1327062467154825, 0.8290150162885762, 1.170355254563836, 0.7655818297237168, 0.8117270739639787, 0.770924148245917, 0.8241133594993233, 0.7611721141901506, 0.899359451683476, 0.9396115562071987, 0.8519164576155784, 0.8449090702643307, 0.8456115907998488, 0.8260183753174026, 1.185006551373723, 0.8577588609014631, 1.1445109358740444, 0.8640966792263871, 1.3200884897973009, 0.9046461071233827, 1.106069224097732, 0.7999605687980689, 1.0993195694173745, 0.7923765083890468, 0.9528836183746685, 1.1196780838704794, 0.8358526224667986, 1.2135033135943005, 1.0338453508298098, 0.8915228846849819, 1.111543663679846, 0.789284359826607, 0.8219641847161031, 1.1523925845598635, 0.8032474472952447, 0.8021457999992422, 0.75774944976992, 1.211175857162332, 1.4725277528012193], 'theta': [1.0079516947272797, 0.8183808755063466, 0.7505928308747609, 0.7553311221588379, 1.3448879248946224, 0.8097392581854228, 1.3086576987195346, 1.513004781265891, 1.3590444383357534, 0.7805690497651596, 0.8204445192097096, 1.1214000636345056, 0.8284239339178694, 1.1050991681568223, 0.7651518389166504, 0.8121502294146014, 0.7684695765773489, 0.7947088016970485, 0.7593981960659247, 0.910940525358241, 0.9193259179740184, 0.8547201641039701, 0.8437000225076644, 0.8361817282494886, 0.8247370315845253, 1.1739924399186032, 0.8570447156850393, 1.1988091085017856, 0.8639269146177931, 1.3149368736908134, 0.8835783675723943, 1.0725820963009802, 0.7992076797395455, 0.9922339668008457, 0.7954164744125456, 0.9393656074630343, 0.9583844551731271, 0.835721580517467, 1.0358687347521514, 0.963867679695258, 0.8803370206364767, 1.121077010637321, 0.7880468182605787, 0.7933983610471429, 1.1392059789780904, 0.8068686663848936, 0.8018072261602357, 0.7575792598976121, 1.1965023595255442, 1.3859246209383087], 'croston': [1.0293227913906766, 0.7485907051752847, 0.7390077709232203, 0.8116570507597334, 1.2206065606897638, 0.8394110289809845, 1.327203721812366, 1.5661443941391882, 1.4140891549641634, 0.7850255993897057, 0.7918674246631097, 1.2147306087754077, 0.8124959311784565, 0.9747657656640518, 0.7751021124149275, 0.8291988347298918, 0.7621002547857962, 0.7943725788714573, 0.7690077521877179, 0.8726492681932112, 0.9377660815885998, 0.8622362945091129, 0.8359141737541547, 0.7493138889837558, 0.8468458016462923, 1.1920700975167196, 0.8652570817723502, 1.270066538021753, 0.8678833658290003, 1.152484001591485, 0.9060177329415205, 1.08484371018668, 0.8096645028277711, 1.1094382077246423, 0.805850281605061, 0.9755485586230175, 1.0107637956680402, 0.8448207858266451, 1.1669717125308419, 1.073780118235431, 0.8223507775010217, 0.8688766052225967, 0.7966828205503884, 0.8797919094688573, 1.164041340203144, 0.7906374910075371, 0.767586251001081, 0.7601237084526393, 1.0370062820629145, 1.523165313001745], 'prophet': [0.775720647752179, 0.7688811372937023, 0.7457425046394275, 1.015461586788813, 1.7807429089471367, 0.8571379900515621, 1.297873654059329, 1.4709376020364529, 1.5545154443022888, 0.8093357191390144, 0.7899550425230164, 0.9170363488518156, 0.8142350530500359, 0.8373333492483238, 0.7875457732025655, 0.8480616670043352, 0.8092769031540177, 3.163630840267457, 0.7868782493420923, 0.963433881784056, 0.9462605202952956, 0.8938528711259303, 0.8440478402510906, 0.7317501161358342, 0.8458938838475012, 1.0365673645391595, 0.8836185253186684, 0.9842482448993529, 0.9005374265241473, 1.0187568913654612, 0.9054343447941464, 0.9405952582521894, 0.8270675822565179, 1.256251994075649, 0.8660476896160891, 0.9371356497150178, 6.653804537421308, 0.8593142074051476, 7.061500105049196, 0.8702837412571547, 0.836245124138788, 0.8745192069359312, 0.822155309618532, 2.1821753326353335, 1.2914916410625572, 0.7767185432401368, 0.7612122746935697, 0.7641444710610947, 0.8115518641008643, 1.670354653002479], 'Avg_ensemble': [0.9554858816879249, 0.7543725878742721, 0.7440881293594598, 0.7402991024419538, 1.3205650540791807, 0.8233868922315802, 1.3540136082225993, 1.5294815458544728, 1.420228172715973, 0.7851745807105674, 0.7958785988388323, 1.0903615343870492, 0.8243305360077232, 1.0621407350288585, 0.7710178506681944, 0.817528758000642, 0.770698554888252, 0.8201007376636086, 0.7645479470660761, 0.8998698114268855, 0.9327179735859784, 0.8564032204261534, 0.8514458891527409, 0.8091874559145378, 0.8341745491879742, 1.1771960491719962, 0.8633109930558268, 1.1318996978678553, 0.8688034955496597, 1.2316442610640077, 0.8703998873635959, 1.054846826850064, 0.8058139381288477, 1.0395795943796693, 0.8083130915475087, 0.953203139034664, 1.6885277651654718, 0.8422937108639605, 1.6089874176984058, 0.9684861861054247, 0.8700736119049833, 1.0005229440919434, 0.7923443192452613, 0.8747801508302915, 1.1644800944172395, 0.8047911948931086, 0.7730674160668569, 0.7528435771547403, 1.0865848414320434, 1.5363497321117], 'Weighted_ensemble': [1.1041587787363312, 0.7768142767635449, 0.7398077901888259, 0.889356030860528, 0.9689938150241846, 0.8398112170965936, 1.371402438336795, 1.7640493015222436, 1.3662915571658485, 0.7856156139977999, 0.792626588170445, 1.4530050484644847, 0.8291588092865801, 1.7072230425910366, 0.781708669853787, 0.8335406821493101, 0.7262788271825436, 0.8195889459612073, 0.7802554325713233, 0.7460497539162051, 1.0038609876279694, 0.8940095646124984, 0.8370494390377513, 0.8353778603392568, 0.8584630051989025, 1.3817398887254266, 0.8678755095882784, 1.8046301101918367, 0.8682195884769456, 1.1050834529780191, 0.9406439473814462, 1.1142206688134975, 0.8209554194806905, 1.3548256925245696, 0.800718154219406, 1.0177074945488378, 1.1352899753342052, 0.8509258822402939, 0.9441307248496507, 1.16025126390497, 0.812696316059687, 0.8496015917903894, 0.7787869651055932, 0.8560016841288927, 1.1695638716673633, 0.783594108821687, 0.7908182371043924, 0.7593417786816398, 1.0052451369581585, 1.7310989085747175]}\n====================\nPE_dist_median is:  {'Lstm': [0.2318303961112487, 0.5112922459213256, 0.3985654077309583, 0.2255768764012969, 0.3742816682843032, 0.6853686161917598, 0.27618041196649523, 0.6427168276517221, 0.37329546124420376, 0.510907127043798, 0.46610708567409675, 0.9162323688066067, 0.21527947134088124, 0.3085051317860171, 0.7344231285132261, 0.5567568549089658, 0.4909243551966169, 0.6496326235504952, 0.7002305392262632, 0.6201303484626974, 0.2214537240311219, 0.7960018147727314, 0.7580101959957156, 0.22095162023398957, 0.8565535876790003, 0.2825044113085666, 0.831677863924085, 1.0157245840142437, 0.773361710512467, 0.42214953922398524, 0.28273548369503415, 0.30923720587209813, 0.7207754821901828, 0.30228834946398425, 0.5960265437572405, 0.2814313585294092, 0.3891127876988436, 0.7113093850987533, 0.29663045073593597, 0.27845504903781065, 0.18865339677009277, 0.6987433416220364, 0.5990245126114722, 0.3300208597250598, 0.3535791755096092, 0.17361214061008376, 0.42880767210317594, 0.5614959228767961, 0.2634607654959389, 0.47285108005023635], 'naive': [0.24264807836242128, 0.3552262660353621, 0.3073237483553778, 0.21844456971219944, 0.5572361321703612, 0.5792754154962466, 0.3560543099476792, 0.2993197350113463, 0.32566778763833987, 0.47425298417549744, 0.3698143993967207, 0.23249163705687953, 0.2619772174988607, 0.480866504673925, 0.5842373339822028, 0.45970380652057147, 0.5493467837235813, 0.46121059519477225, 0.6404595244732101, 0.3124651160089329, 0.2419257784298855, 0.7537644731916394, 0.6291229389530891, 0.2727867010937944, 0.7332749133612987, 0.27130643305155716, 0.7770172524010611, 0.2568847527209358, 0.7417513974184453, 0.41995833037436414, 0.2386972457584072, 0.2795468609862707, 0.6638501264702674, 0.2604468473773311, 0.5250261893495274, 0.2812866061215773, 0.3904885685591066, 0.6522267964071855, 0.35591762355402584, 0.2770943116717208, 0.25418666505582954, 0.41736005839970314, 0.519828759688015, 0.23134211781066652, 0.3541566282777398, 0.19256730688897022, 0.3299731260527811, 0.36421888795054225, 0.5077762263746965, 0.3186941398113527], 'snaive': [0.2764935606906464, 0.47081112162189076, 0.35380543904868916, 0.22015544474606658, 0.6697128168888827, 0.6755079100001131, 0.31071379725045395, 0.31410984953823995, 0.31999283670488404, 0.46846276154338323, 0.49911215586113855, 0.3760753168522751, 0.2178554742251984, 0.27284926079877536, 0.6051773177251426, 0.5636153299082195, 0.5273505463700842, 0.5187608009767857, 0.6478833868795395, 0.3116557929182127, 0.23331084026721322, 0.7704246153963556, 0.6736910061046054, 0.21223653530752526, 0.7691924837889237, 0.2709248402833427, 0.7851360489639209, 0.37488658504184696, 0.7475232609061295, 0.43597189549726784, 0.23892124534364423, 0.2577371954487476, 0.6854181106537911, 0.27726641495362875, 0.6126800177826329, 0.2840322614788626, 0.33440514774200236, 0.663540280550011, 0.3533596281166362, 0.2801435701038497, 0.19891120749286, 0.43944974790383884, 0.5467285382520218, 0.23897448926203924, 0.3551931670224791, 0.19342997988607755, 0.43183017910293797, 0.4475215847059553, 0.26580579572397395, 0.3271605036777926], 'arima': [0.2614541623344734, 0.34423444440225115, 0.3041219067015237, 0.27833876363834364, 0.4544937462533031, 0.65185231801887, 0.29901197690171955, 0.2942774852585801, 0.29746489043569024, 0.4742836019096525, 0.44756762730106125, 0.2568125490787075, 0.22665817957855108, 0.3305206882128706, 0.5991639924803532, 0.5580255289026418, 0.5060797673757881, 0.441296292605583, 0.6336157644206415, 0.3611417137517895, 0.22370167656081993, 0.7352235624108595, 0.6516168818820786, 0.21779260483501678, 0.7546691788643791, 0.26872468036491887, 0.7834475193154825, 0.3145774925724799, 0.7451226148390588, 0.42110529753648895, 0.22747702121838836, 0.22001715674194927, 0.6757777319095355, 0.26014598888036883, 0.527774844616465, 0.28531438669469916, 0.33076966793324836, 0.6722760391577612, 0.32367029263963104, 0.2703542645548657, 0.20772097080973356, 0.4233887700669084, 0.5343624097405334, 0.22934412669045368, 0.3605751586072985, 0.19613706517046148, 0.33375698901207873, 0.4545054349634515, 0.34804095394914564, 0.32295164498099416], 'ses': [0.25313714762906664, 0.35522626460541734, 0.3073237459172368, 0.2126590448955765, 0.46682155749494303, 0.5811225194905338, 0.3157264442924354, 0.29148676713602834, 0.32315253321592013, 0.47509332142084254, 0.36981440132861826, 0.2441184455128693, 0.23620729892194614, 0.345383036633691, 0.5858281327464343, 0.4937734351757634, 0.5573285426420893, 0.4612105960670576, 0.640509872591225, 0.3140426380172401, 0.23345711528116664, 0.7537644737900816, 0.6354059260011519, 0.2404022320670547, 0.733295674344663, 0.2643620797773224, 0.7777770641532952, 0.2658841191052539, 0.7444009131665653, 0.40352115199000615, 0.2362942619138582, 0.2771261658940292, 0.6649112996970181, 0.2731076017295664, 0.5250261911989342, 0.28376105479191427, 0.2955338785391788, 0.6522267970133511, 0.2955134104038504, 0.2774186827432948, 0.24735142031494084, 0.4123277044273597, 0.5358852870682911, 0.22359422470130152, 0.3627683443929371, 0.19256730651630477, 0.33253553722955553, 0.3720565312702583, 0.42177771126787267, 0.3259084715957013], 'holts': [0.23285748311306595, 0.3431984610498766, 0.30443684944291133, 0.21767931926911754, 0.6998795464131969, 0.5736935827244458, 0.35602728555675445, 0.2926283710765111, 0.32510627748694504, 0.47622700607712903, 0.35584753638108, 0.21248108843248448, 0.2660428209696494, 0.5142042983104658, 0.5856415633211889, 0.45599947139871166, 0.5648501967234445, 0.4617927410380499, 0.6469514154968313, 0.327855500608065, 0.23951782554460022, 0.7571628987736099, 0.6249293259648336, 0.29081824285309077, 0.7324101501224203, 0.255991325548996, 0.7807813806143907, 0.2519721479791448, 0.7462805314100023, 0.4124451427699417, 0.2350076479103219, 0.2823404216999008, 0.664445023993556, 0.2609474327075062, 0.5250356412301361, 0.29301741474064613, 0.4248883052839774, 0.6538508902311408, 0.43898498933485286, 0.2815267716130082, 0.25869173010071655, 0.402628890461707, 0.5269158951776363, 0.23516355774002126, 0.3617620930016004, 0.19319072124636888, 0.3245959900636384, 0.3564383356466224, 0.5200532521661017, 0.31614187190664594], 'Es': [0.2577539726323156, 0.37130508093917114, 0.3489026289556776, 0.21141487133217132, 0.2692473740318487, 0.6146601654091596, 0.34027828601144283, 0.2886787677740098, 0.36233743814200503, 0.47663728315520015, 0.4169136706044496, 0.20463445829159904, 0.2251644482653656, 0.36020823349793385, 0.6040055598372318, 0.4909692372818179, 0.5850835703713332, 0.7076823598191904, 0.6445428713560144, 0.3683823624218542, 0.38077678121758085, 0.7382545157481124, 0.6469955565279962, 0.20867281431394505, 0.747299502135363, 0.2581664665059732, 0.7958700053537227, 0.22747790890662412, 0.7508540381190977, 0.40902352206321824, 0.5525314162353486, 0.21173304219800243, 0.6861634383212101, 0.4505639508513022, 0.5243189750144254, 0.29286681068574655, 0.38628011319375594, 0.6896437524823502, 0.2839092996028553, 0.7923903721415338, 0.2526798338551516, 0.4151697245184619, 0.5184320457784082, 0.38237028717471944, 0.39994552102010517, 0.2056047112336983, 0.4068497718508766, 0.3802689724330015, 0.4936009520481546, 0.47505416405415357], 'auto_ets': [0.2531340566250046, 0.3552166696055782, 0.3073073858426637, 0.2126635096769692, 0.46682417808089094, 0.5811195812222192, 0.3157253794133393, 0.29148750823350533, 0.32316433301028796, 0.47509372174329445, 0.36982736506337305, 0.2442695703277213, 0.23620617848400613, 0.345089917084658, 0.5858280273833243, 0.49379261430389826, 0.5573217218380633, 0.44108989954381295, 0.6405098816164901, 0.31404415338893227, 0.2334561513486413, 0.7537684892798546, 0.6354062877514347, 0.24047595211401557, 0.7332990320115853, 0.26436274515312513, 0.7777774792827311, 0.26591608355985497, 0.7444014223294755, 0.4035214844477638, 0.23630039325954666, 0.2771260841782581, 0.6649112237855845, 0.27310539860096883, 0.525038600551502, 0.28373310112186373, 0.295536786260217, 0.6522308642162408, 0.29541535732094054, 0.26998207426802323, 0.24736025478489593, 0.4123288955370103, 0.535886481795876, 0.22358702608274425, 0.36275234717540505, 0.19256480603623943, 0.33253554394037343, 0.372056738894177, 0.4217590567057329, 0.3259146508122718], 'theta': [0.25249100790021545, 0.357642586143181, 0.3060064564158471, 0.2362782704562698, 0.39197401720227665, 0.5815745471740799, 0.30417818017520315, 0.2946255490807653, 0.3047936492604111, 0.47520105309885613, 0.3722900316414206, 0.2438086096501609, 0.23916700823759393, 0.32211065131892724, 0.585068822899741, 0.5006388223677098, 0.5526253901196236, 0.5003579732375609, 0.6377510313113093, 0.3269837690908326, 0.21812938390618924, 0.7582666671533783, 0.6324861939706177, 0.23446596935814662, 0.7312853657711327, 0.2686398187836092, 0.7766925802567642, 0.2738683030010489, 0.7440813242242632, 0.40500591865752467, 0.20773902029261296, 0.2609507563998336, 0.6636657370851529, 0.2730868809909016, 0.531770307898228, 0.28141303000117185, 0.37798625079739046, 0.651952217254617, 0.2689193731533736, 0.26136207781942766, 0.2427160431686441, 0.40912297018722477, 0.5331564044950061, 0.24305042052108866, 0.3754149961354344, 0.19330193230428772, 0.33500255693896674, 0.37071442159411716, 0.4110676140833491, 0.3162771603410126], 'croston': [0.25788107197873167, 0.4462787377660071, 0.3511570479821748, 0.2215743708682562, 0.2659976360843608, 0.6651956050028335, 0.3106508983557033, 0.30003917800175506, 0.3255082657002954, 0.47654190320837053, 0.4676854381616208, 0.32551497802010587, 0.21524064694574943, 0.28765450995411196, 0.6064742177796034, 0.5521221393257024, 0.5371640447271238, 0.49575203650289834, 0.6557947363421999, 0.2996229703239732, 0.22847125263958756, 0.7715190127645563, 0.6640828097065281, 0.21343452297049864, 0.767131825910566, 0.2577320811428208, 0.7898822764333258, 0.34367513255287035, 0.7516221004428634, 0.44199109158528405, 0.23941627559330386, 0.26442225530637653, 0.6850968976008154, 0.2731355540205774, 0.6024462953175089, 0.2873763557309033, 0.30896581117499533, 0.6719160543203027, 0.25676181465710396, 0.2763207125408699, 0.20761617092282025, 0.43700641999673406, 0.550685859095557, 0.2275097451254701, 0.35469676023795976, 0.18818788563729155, 0.41382605156672325, 0.44176471112378, 0.28092765595813995, 0.3280885156328953], 'prophet': [0.3630969389624128, 0.4993299438987907, 0.3760958293799378, 0.9187672322422982, 0.4939891025697033, 0.7092839563499003, 0.3026902200093238, 0.285103960645237, 0.3668336065844457, 0.5378169321022731, 0.48890663270621926, 0.22683920150951767, 0.21448233721384066, 0.2619750263874857, 0.6310883753449327, 0.5936570188166518, 0.6305233868758318, 0.9004263472568348, 0.6867485123848825, 0.36823583052104586, 0.227590467065317, 0.8210114775884773, 0.6832790815775835, 0.2272269657156501, 0.7662441519969715, 0.25289790913238414, 0.8179220491737422, 0.23864941308716453, 0.8172293932213238, 0.4314938209544479, 0.2131810768590552, 0.21332386132572267, 0.7203299113382631, 0.2928027773726406, 0.7428052375588001, 0.27996227451187683, 2.749738013724836, 0.7039882652845983, 3.4542322627726856, 0.3561296829864657, 0.20663073965582868, 0.41088496997532664, 0.6113938359462741, 1.0233170203227249, 0.3354486804615503, 0.1813767951914768, 0.3962787282402099, 0.4842158770571793, 0.4942597888125894, 0.3660833532405846], 'Avg_ensemble': [0.23892341815157275, 0.384670046751955, 0.3230459781200727, 0.23678812352752698, 0.35217111999018974, 0.621224809383954, 0.3143439713000035, 0.2887112466258305, 0.323867987512754, 0.4756224060657445, 0.4166122244434484, 0.2338344320702189, 0.22916033005911504, 0.3166116760775266, 0.5972513343500157, 0.508344374792993, 0.5567251474330728, 0.43776198784788956, 0.6474766996872343, 0.3145424139272468, 0.21325522009762293, 0.7613160186096924, 0.6539876729778983, 0.22090421459102663, 0.7486205656972343, 0.2690826441900829, 0.7862303655948436, 0.26486824874219883, 0.7533266996077225, 0.4230359947656796, 0.19579686514037928, 0.24062601062829575, 0.6774569500855194, 0.26931990748998147, 0.5644149293847525, 0.28299685669939684, 0.47599191424783327, 0.6663851956917559, 0.34651151310813355, 0.2633932993213733, 0.2175246315845559, 0.4206417714338404, 0.542829375690188, 0.22586199493395986, 0.35432318542862545, 0.1929926901609753, 0.3409778485297441, 0.4074999048493596, 0.3348515758529839, 0.32512124949718824], 'Weighted_ensemble': [0.3125132871047918, 0.5120742580736721, 0.3568291501630401, 0.2127323969171418, 0.5783859639308344, 0.6707397959696382, 0.3230021496378668, 0.3663147665503653, 0.30391662216031845, 0.4789450784407389, 0.48822457015284626, 0.5290622518420488, 0.2418734625599534, 0.6541742853048442, 0.6221546715535825, 0.5571533097916228, 0.4525105258700343, 0.6468821259701689, 0.6775559396281792, 0.455909740479778, 0.25552352463022776, 0.8242149539418944, 0.669561732177501, 0.23657574252038982, 0.7866449312037709, 0.3000056709554091, 0.7946945801194437, 0.7208556483306962, 0.7530493733074665, 0.4289078918491611, 0.2741268581584759, 0.2838319407522115, 0.7106855824678631, 0.35097188816795816, 0.5494062029432816, 0.28237679666293686, 0.8099301596970024, 0.6857374901911438, 0.7250676463055885, 0.2845074158130542, 0.19904207990871609, 0.6351351125903788, 0.5109725299622078, 0.21834169984075044, 0.358771570272785, 0.1854941509235241, 0.47614226465857723, 0.4703039429960811, 0.25984518955059366, 0.3989715819000939]}\n====================\nPE_dist_80 is:  {'Lstm': [0.4879603846417363, 0.6802366227453446, 0.5945216808797941, 0.4334342956632178, 0.6659339223619051, 0.7844450809433191, 0.5187659042193853, 1.3786269875776624, 0.8377599135590001, 0.7282805113876338, 0.6536974416185806, 1.5384004545750927, 0.47746160745716243, 0.5688030858162457, 0.8196372479397466, 0.7473876794276338, 0.6629351095107754, 0.7738988280599342, 0.7768806260529512, 0.721325085005952, 0.46777088483462975, 0.8453226857154361, 0.8323546767852816, 0.4180921883978913, 0.8953142779057772, 0.6283003077913711, 0.8787654005385116, 1.5936424976004444, 0.8560508082440885, 0.7516356884688689, 0.6606479977712371, 0.6367201989551909, 0.7964872298930704, 0.6809222180161768, 0.7238417890269206, 0.5868788931564857, 0.5163597754810112, 0.825860907474428, 0.5553287842284637, 0.5575502775810776, 0.40274232611540983, 0.8095436346113694, 0.7393317044196153, 0.6485331639740153, 0.7583515516398679, 0.3806252480145982, 0.6292646363753831, 0.7131595806194846, 0.44796959279574494, 1.1724441149556784], 'naive': [0.5131911248747827, 0.5479727697991992, 0.5414800910535464, 0.46021605594454934, 0.7636118663166718, 0.7150909237600213, 0.7970954929947557, 0.7139474590251403, 0.7198554192071209, 0.699844600386195, 0.619786518174669, 0.519253015806112, 0.6136235495893332, 0.9892897413711945, 0.7176407033222678, 0.680976636173516, 0.7017529507767877, 0.6456092788042063, 0.7323928675018994, 0.7099420791529475, 0.5242151603025118, 0.81329711376731, 0.7466601151718245, 0.5738396014521032, 0.8044474401717007, 0.5353914438867835, 0.8393959064315388, 0.5038829532977356, 0.8359735343142302, 0.7478049272861047, 0.5241088037801599, 0.598958225067247, 0.7549971883925851, 0.5338077579342513, 0.6820188720816995, 0.574061037010903, 1.078708075656564, 0.7902221029974252, 0.5550011450662728, 0.4833314489533571, 0.47099540878949947, 0.8347028432868886, 0.6920685170301675, 0.4309768599372723, 0.6713327264989805, 0.3912332030652155, 0.5636908592267668, 0.5925211223903479, 0.8834269085765207, 0.7042048031087977], 'snaive': [0.6022054004834199, 0.6473599273987292, 0.569454962483476, 0.43809093398767784, 0.9021459143270495, 0.7776894811069164, 0.6811858603647466, 0.7377499025309927, 0.6959754292169202, 0.7028854288674641, 0.6779800763981486, 0.7667649787189089, 0.4857106951626872, 0.4982333430174203, 0.7318617952224398, 0.7546140964649725, 0.685819074541951, 0.6876269601981597, 0.7379184721718809, 0.5518102674082718, 0.47706531868960167, 0.8259293146320066, 0.7751423876389056, 0.4139591229127176, 0.8315590393068802, 0.5351707102780339, 0.8452434976774322, 0.7172310502900466, 0.8396395304080778, 0.7288722774806307, 0.5280891782638704, 0.5732303799808727, 0.7707170121431665, 0.5822954326276975, 0.7352261845169435, 0.56991743074415, 0.5979085774842123, 0.7970464324360805, 0.7066487431751574, 0.5358728487083445, 0.40758447341972476, 0.6597287637051037, 0.7111352813074341, 0.44975519183763873, 0.6718602171848462, 0.39231109533764413, 0.6294124918958807, 0.6264444289224164, 0.5682209641514221, 0.8135815490244244], 'arima': [0.5836532890513646, 0.5458599741906661, 0.5374473287137476, 0.4523886708140409, 0.9054375614477876, 0.7611727979518284, 0.6408261815018287, 0.6579983834364413, 0.6237907289938107, 0.7011643085917219, 0.6395975983696434, 0.5429271213557402, 0.5357970801438254, 0.6859974195833587, 0.7277779309758591, 0.7487463176479127, 0.6716799001954716, 0.6332520955669411, 0.7272990348798849, 0.7733602696520736, 0.4734541792833215, 0.799238859862915, 0.7665929588693655, 0.45951840327034543, 0.8209600801651327, 0.4906255212661478, 0.8440273283702951, 0.6318038777793712, 0.8381147613064921, 0.7533554026936767, 0.5024350851603752, 0.5062131932621642, 0.7636906227755258, 0.5292143167108395, 0.6787372918191611, 0.5607634874916465, 0.5957347395553164, 0.8023158696742685, 0.4657149528349523, 0.4752418930971227, 0.4126667333737368, 0.8324464680889264, 0.7013888761810586, 0.4217162186033336, 0.6410545894805942, 0.39300493046565205, 0.5786269301600581, 0.6311665250243443, 0.6741038350026247, 0.7247690300967063], 'ses': [0.5624027486380456, 0.5479727700086323, 0.5414800919086797, 0.4510476451032829, 0.9110558421861494, 0.7163417579449365, 0.7130758304687513, 0.6911216280809578, 0.7166472778888241, 0.7010963877479899, 0.6197865132090512, 0.5328358282233447, 0.5566553206024056, 0.7233843405514823, 0.7187210716596333, 0.7051373967639061, 0.7070353629710813, 0.6456092793779551, 0.732430341768097, 0.7119973499787321, 0.47724050384410377, 0.8132971142210662, 0.7509519179882077, 0.5055383498361473, 0.8044626613250336, 0.49586447056268856, 0.8399431634685259, 0.5203517870979699, 0.837656372864366, 0.7459400406046789, 0.5146432961350077, 0.5972157090280598, 0.7557706244240386, 0.5703885967502773, 0.6820188733198238, 0.5712850466657474, 0.7470226664305967, 0.7902221033630661, 0.47502095566304336, 0.4834887481975618, 0.45700973646023235, 0.8154066475742965, 0.7023654900107683, 0.44085366730731823, 0.6354258785420785, 0.39123320263046935, 0.5653345224132986, 0.5929265642954802, 0.7701084832871606, 0.7313214016962734], 'holts': [0.48790431569155807, 0.5476807900489276, 0.5374128131720352, 0.46104306126892103, 0.8397680590980934, 0.7113109810675456, 0.7939239902347554, 0.6986233765015354, 0.7075978155091068, 0.7017419547654188, 0.6295893174027848, 0.4808868933707185, 0.612754618689569, 1.0368137313145378, 0.7185943656900988, 0.6797939955615925, 0.7120132729338183, 0.6459921873829575, 0.7372248028737822, 0.7299934054087105, 0.5140211184091786, 0.8158738981697278, 0.7437955286469539, 0.6118857167834197, 0.8038134290738352, 0.529082774576642, 0.8421070327688311, 0.47336842051830186, 0.8388502114146136, 0.7406881447116394, 0.5116525919320907, 0.6029055124288283, 0.7554307794104652, 0.5370073673855029, 0.6804046768927899, 0.5637066218693082, 1.130133892558136, 0.7912017615317931, 0.6046521886808712, 0.48025625681511386, 0.5051208444300048, 0.8871998982783691, 0.6966134625789872, 0.41278974738083024, 0.6763588080784035, 0.3934851448886636, 0.5502339884022697, 0.5887779292499324, 0.9165811765343581, 0.6949185864205484], 'Es': [0.5771169067652677, 0.5610564051522967, 0.5685171717238686, 0.44964359427180034, 0.6567090096497038, 0.7361470291942371, 0.7703266127346057, 0.6230735703812825, 0.783059147186141, 0.7019755835334727, 0.626524422058361, 0.38319924775884806, 0.5376284200451633, 0.5489665478184057, 0.7310660125316175, 0.7018362976135961, 0.7254039329099631, 0.8130087374814626, 0.7354321157220093, 0.787811593878307, 0.5317484772036791, 0.8015370161989913, 0.7532512569645885, 0.4430472989371732, 0.8144122109575377, 0.4954524738227928, 0.8529746668147333, 0.46204605128089815, 0.8417550718234755, 0.6499707004863974, 0.6845313251404518, 0.4238344390631231, 0.7712602441609834, 0.6162989381667014, 0.6815454126889644, 0.5472225970250749, 1.0724166538126152, 0.8127921293151399, 0.5472864546361759, 0.8493241224021574, 0.48591549635230624, 0.8162993526077482, 0.6911728111874136, 0.557170749035489, 0.5994612553715641, 0.41138863656436725, 0.6115866733102839, 0.5931138822302502, 0.8800807562012993, 1.180527610970403], 'auto_ets': [0.5624092148764265, 0.5479741753136816, 0.5414858299058005, 0.45105075805230954, 0.9110562793490043, 0.7163397681888296, 0.7130744439961684, 0.6911198591853115, 0.7166173507843064, 0.7010966157081818, 0.6197531921005381, 0.532827669697072, 0.5566539097260249, 0.7230088655923386, 0.7187210001037766, 0.7051485680615872, 0.7070308488910735, 0.6148737724702629, 0.7324303484856305, 0.7119993242771654, 0.4772393493976696, 0.8133001588813554, 0.7509521650937441, 0.5056040546026811, 0.8044651230370907, 0.49586311757095825, 0.8399434624669349, 0.5203902996281271, 0.8376566962589236, 0.7459401822093723, 0.5146508079401882, 0.5972156068316464, 0.7557705690959955, 0.5703933564203942, 0.6820271810198225, 0.5712683146010765, 0.7470154555024817, 0.7902245567127206, 0.47494788711281055, 0.4743820509529596, 0.4570617754124477, 0.8154070217129539, 0.7023662561837627, 0.44084848305792845, 0.6354669346686946, 0.39123028560358825, 0.5653345267835104, 0.5929266988906883, 0.7700852584198459, 0.7313294702848092], 'theta': [0.5525138708849066, 0.5488905371994857, 0.5419421071233453, 0.43514967548550343, 0.8302665983024156, 0.7166478650622705, 0.665677387778855, 0.6614655659908638, 0.6567922850714595, 0.7011577346293051, 0.615972215142918, 0.5244395663327891, 0.5620493583197455, 0.6378043278192651, 0.7182053971852356, 0.7118527406620018, 0.7039227670520682, 0.6686791607674155, 0.7303769273288632, 0.7288576720586524, 0.46318227382370797, 0.8167108072254854, 0.7489574981277382, 0.49582667776427003, 0.8029887805118232, 0.4888198912995987, 0.8391620601990244, 0.5694236018208836, 0.8374533861137927, 0.7465724515619487, 0.47520863999190044, 0.5782593145451289, 0.7548627962022527, 0.5105582008930675, 0.6865338629608677, 0.5739248773119533, 0.5389488984912659, 0.790056475983637, 0.5378098021060925, 0.47054477193225797, 0.443774311302632, 0.8186770390155047, 0.7006154708778102, 0.424207819858025, 0.6029688256719008, 0.39407702052494104, 0.5671459972840432, 0.5920565222866603, 0.7567744482035402, 0.669253681753018], 'croston': [0.576025650647478, 0.6296699444006882, 0.5700111772919026, 0.4365305252840698, 0.6640765782978091, 0.7706244895537175, 0.6767344808059904, 0.7190773013913916, 0.7082800270629055, 0.7024480064799028, 0.6547212190274779, 0.6805114592859994, 0.49255749781488617, 0.5371159398497054, 0.7327425664344065, 0.7447462708200837, 0.6932397731051614, 0.6656248884231355, 0.7438069150260295, 0.6336124027380199, 0.477535611166351, 0.8267591183162147, 0.7761804405486746, 0.41583263172661195, 0.8300551922988262, 0.515326054791407, 0.8486619843935639, 0.6556590976028565, 0.8422429062091578, 0.7122040966615726, 0.5249935072097491, 0.5801947150027019, 0.7704828960321659, 0.5785017730585783, 0.7282303622818673, 0.5621998004202353, 0.6669033416980351, 0.8020987256810721, 0.4968699809232062, 0.49783169348949263, 0.41116317348335946, 0.6196297240959783, 0.7112809006686807, 0.43714027748252915, 0.6599230996322973, 0.3799315951027044, 0.6161549595281033, 0.6234021157623293, 0.5827286280764368, 0.7826684663681389], 'prophet': [0.5145161069471806, 0.6691887933915635, 0.5799586834639687, 0.9381699691497888, 0.902869810837386, 0.8008295533046107, 0.6511020421656639, 0.6140647317643942, 0.8105689899478353, 0.7424769429107337, 0.6697923805161954, 0.3982169054743234, 0.4924788582903081, 0.4729824585783896, 0.7494589211880187, 0.7754762799749652, 0.7581214381263277, 2.4532275626044724, 0.7668459101061634, 0.7871004538839375, 0.4785585167416753, 0.8642857341915409, 0.7817494741683164, 0.4232285907503904, 0.8294073769709608, 0.47136448433351685, 0.8688577274872851, 0.4679985188972152, 0.8839133441131304, 0.6091797148457929, 0.46911925809626315, 0.45195471621331035, 0.7961624756090829, 0.6450768488330106, 0.8241804149015026, 0.5737922783786842, 5.8343117081696425, 0.8214447848334336, 4.7838341641513145, 0.5403410781052513, 0.4205255288109303, 0.6171030682985983, 0.7489349195554933, 1.5582266588274059, 0.6807677731764363, 0.3782559941234531, 0.611590404933192, 0.6574391722627455, 0.6246859469118387, 0.936750162114211], 'Avg_ensemble': [0.49933645323837184, 0.5634110900129337, 0.5497244353736163, 0.4320172880464306, 0.7779309135966278, 0.7402896307939386, 0.7040916644328362, 0.6822782771883453, 0.7146110072661602, 0.7019253319630667, 0.6261772296672369, 0.5201454598549371, 0.5397588671863398, 0.5937874537828524, 0.7264789764313353, 0.7206256825535708, 0.7066360296608885, 0.6140699848588556, 0.7376157735864239, 0.7126484818436819, 0.4794817150767881, 0.8190229135466613, 0.7581387193664814, 0.4585296714618784, 0.8129541187938989, 0.48864501606158695, 0.8460316830078165, 0.5282608758304187, 0.843325581482626, 0.7501758250838255, 0.4693599593806555, 0.5450537827727472, 0.7649145208246262, 0.5232726869235851, 0.6986758499752891, 0.5708276250863101, 1.206531129508053, 0.7987624942528637, 0.7024498172200455, 0.46300900063137673, 0.4386108964164156, 0.8277804962526015, 0.7068187002984675, 0.43099595583505584, 0.6581748303359012, 0.39331726770261294, 0.5815161410585499, 0.6122095961667653, 0.669011181818395, 0.7905410089492944], 'Weighted_ensemble': [0.6522769244742905, 0.6807482963269516, 0.5716578965198268, 0.5059767399247508, 0.7749035945285554, 0.774422831666381, 0.7319707195428148, 0.8760813721729384, 0.6602196539206935, 0.7017829468286615, 0.6693517129855623, 0.9730032943017085, 0.5681621997799405, 1.2354752777002573, 0.7433917234200863, 0.750880196964659, 0.6295523034786246, 0.7700747953416327, 0.7600038486328419, 0.6008517495217748, 0.5525375742915726, 0.8667147024599038, 0.7798310129193525, 0.4903122788622951, 0.8442956566287989, 0.6586565825597508, 0.8521280627328409, 1.207426322974289, 0.8431494378271793, 0.6924498096720129, 0.6360383781801436, 0.6047698879646728, 0.7891332071922808, 0.8125214827189089, 0.6882932816940993, 0.583314655177748, 0.8494840376268407, 0.8104358595389637, 0.8124435762956024, 0.5777683908542053, 0.40759338881737683, 0.769330109787039, 0.6863890599447796, 0.43250588797574213, 0.6709834326583254, 0.37275190475795716, 0.6672002159856993, 0.6410405272010209, 0.5708707629959565, 1.022623758068319]}\n====================\nSmapePE_dist_90 is:  {'Lstm': [0.7151413938236394, 0.7379929867094573, 0.6999961715635583, 0.5850632706455594, 0.9442715166988115, 0.8356167513419249, 0.682170341961486, 2.096742087111955, 1.2177587755954276, 0.7829518085893936, 0.7570126561007469, 1.8727859015346437, 0.7151366542286506, 0.8201817062105021, 0.8381260568421005, 0.8172712729421262, 0.7131513290347954, 0.8126122201670195, 0.7883786896434618, 0.765064429860133, 0.7711760277150039, 0.876998317232563, 0.8655319235220157, 0.5265853221282305, 0.903607940132784, 1.0069324247011924, 0.8898441449710134, 2.0191348070132142, 0.8745971052624147, 0.9249288200470739, 0.8736205114696752, 0.8610081109583392, 0.8225603261697287, 0.9661293497553329, 0.7826320639443636, 0.8326899280878575, 0.759200897025226, 0.8627937233261178, 0.6006934109381604, 0.7768214801962008, 0.5298269440667533, 0.8518128760488959, 0.7871364271613105, 0.9698793278844853, 1.0698216349711893, 0.49660768863187466, 0.7160128911783297, 0.7690891768834591, 0.653116335457321, 1.6435720386821877], 'naive': [0.7546192225996954, 0.6957288648621419, 0.674663898806509, 0.646139751619329, 0.9605660285722221, 0.7888506032063249, 1.1287433841029828, 1.152333197049209, 1.0605334344828772, 0.7583315422423353, 0.7514513049499705, 0.6774816465696428, 0.7674638639538889, 1.321784034585126, 0.7465850780251645, 0.7747459645704952, 0.7457017116234549, 0.7072362405898056, 0.7461835293001328, 0.7466986791503201, 0.7932380836828892, 0.8515311098438823, 0.8158929401648333, 0.7445875487160913, 0.819263039651688, 0.8453853686876327, 0.854072341338422, 0.6794698203230847, 0.8571065710084057, 1.10444424784956, 0.7973772940778495, 0.8166467026085638, 0.7863857931063617, 0.7277763204018955, 0.7535038985847058, 0.6780925799575643, 1.3279630752597809, 0.8347134824308694, 0.6419728247008752, 0.7353214297095518, 0.657915441356429, 1.101799379230884, 0.7708586681882093, 0.527755651849391, 0.8379253415778033, 0.5609924842582215, 0.6930492387277059, 0.7132998814469642, 1.2018325960878664, 1.0684446808380375], 'snaive': [0.8404262544210573, 0.7196380883247966, 0.6820743026701106, 0.5813700880699183, 0.9154523289734592, 0.8304648974542861, 0.9683280574205791, 1.2018697804627239, 1.018440595699707, 0.7608258464007229, 0.7710787228081182, 0.9625554434241577, 0.7190402786228366, 0.6648573751044666, 0.7593483797357231, 0.8226761485280006, 0.7321158020083351, 0.7448836675796476, 0.751424381674751, 0.5851676951050025, 0.7779797610027719, 0.8615764223746871, 0.8275096333997289, 0.5305039101370626, 0.8449036712703862, 0.8448314615469524, 0.8593855639366954, 0.9794075372822597, 0.8603002431413116, 0.9074351099706601, 0.8012445472230428, 0.7716616715301448, 0.8000916671775904, 0.7912201766785826, 0.7915928786244261, 0.7426724602635512, 0.794637956810778, 0.8400904533309351, 1.369464197952761, 0.7509764139612666, 0.5334492165108407, 0.814424265566873, 0.7723777742052726, 0.6787847777949316, 0.8359385478197157, 0.567169825703883, 0.7175156302424487, 0.7120395172441422, 0.7402544266936497, 1.206884605162195], 'arima': [0.8115560813230234, 0.7392756794195408, 0.6724195228835064, 0.5607474165100969, 0.9147589009217547, 0.8178691930928581, 0.9096770756777771, 1.076225277965303, 0.9241159383783892, 0.7634450317881503, 0.7479544840658134, 0.7907420962937615, 0.7230807493345464, 0.9401720098493994, 0.755683148409608, 0.8182540525308699, 0.7200601325734424, 0.686737457414318, 0.7413521983621776, 0.8114800953920916, 0.7741620662599573, 0.840351778892795, 0.8190732278098424, 0.6312846660918806, 0.8351444081762086, 0.7549165891998565, 0.8582805314584856, 0.840981221338706, 0.8589719240530773, 0.9284073859664586, 0.7925479041290987, 0.7193979360504485, 0.7939654656770971, 0.7257870610265826, 0.7612800944159772, 0.7188496717949887, 0.790838377544251, 0.8442423060432278, 0.9472821143375818, 0.7092538487192099, 0.5531346151831421, 1.1321595974278043, 0.7772283034050541, 0.5256345640652871, 0.8060975348522696, 0.5761544426196565, 0.6978350890838992, 0.7156796103880034, 0.919013569760762, 1.096985030741147], 'ses': [0.776912962078105, 0.6957288588411653, 0.6746638994132566, 0.5990580226985471, 0.9198233687210242, 0.789777610822647, 1.0198669413758354, 1.1195073526684862, 1.0510813760137638, 0.7582013726039456, 0.7514513057119215, 0.7726551782034525, 0.7348858999205437, 0.977575127866367, 0.747554698863362, 0.7936915341717183, 0.7502057239280422, 0.7072362410637822, 0.7462190723960866, 0.7487981296992381, 0.7779534286196474, 0.8515311102047161, 0.8087560266035629, 0.6522267650076196, 0.8192771076086036, 0.7666257078204727, 0.8545695885524823, 0.6950617096930616, 0.8585725940057481, 1.0114627559740914, 0.7908622280033638, 0.8115233306760268, 0.7870601401612299, 0.7737266521323913, 0.753503895650785, 0.6792438587061245, 0.859515468838804, 0.8347134827189622, 0.8984164151391646, 0.7355075926210604, 0.6411904142383366, 1.0467556119234571, 0.7760482875860877, 0.5512698666269598, 0.8067626077472109, 0.5609924843329089, 0.6942231230777023, 0.7168342004544848, 1.0445005510499297, 1.1049514453279026], 'holts': [0.7173679955579468, 0.7543583894041166, 0.6720246804551802, 0.6461328611806934, 0.9732703089580304, 0.786049244155346, 1.124986591242319, 1.1289094031604585, 1.0402689138445973, 0.7587236052311471, 0.764842050135276, 0.6012127612997751, 0.7692362008605711, 1.3772512894628288, 0.7474409813985536, 0.772868323003225, 0.7544501045062024, 0.707552563078858, 0.7507664594010329, 0.7671810250222361, 0.7887527831160199, 0.8535802068242136, 0.8308999060793596, 0.7947906736944358, 0.8186770629774072, 0.8163069163068323, 0.8565357176443824, 0.6270336834274661, 0.8596126193662533, 1.077104456975047, 0.7912751761973675, 0.8181689948721184, 0.7867638344879808, 0.7299737305168672, 0.7573289758019691, 0.6690559337860187, 1.3855552904745685, 0.8354853671235192, 0.6909230645890764, 0.7331915598437819, 0.6832122942002943, 1.2353855664413484, 0.7742407026136098, 0.519376453709282, 0.815945410585588, 0.5603739370229661, 0.7252752943089764, 0.7097913071504564, 1.2650508835817653, 1.0290695011326791], 'Es': [0.800783035736018, 0.6787686851521854, 0.6808140270222431, 0.5980325367677831, 0.9285930889525955, 0.8032570015277946, 1.0922411761077264, 1.0295393796252816, 1.151747375651314, 0.7589125996531617, 0.7449538258062562, 0.5943064078170994, 0.7227420946844302, 0.6436329764297842, 0.7586341719484238, 0.7913818323181327, 0.7658675583285782, 0.8450256704516199, 0.7490661552212285, 0.8262420626222557, 0.6264398935365799, 0.8421793067232821, 0.8083290611724491, 0.6071631231649632, 0.8268676155220847, 0.6793439669393311, 0.8664102380023168, 0.5689977207511425, 0.8621432199179923, 0.7923681841368422, 0.7601744733320489, 0.6248169564761398, 0.8005653028713355, 0.6971121706342187, 0.7546258315138166, 0.6693238216876299, 1.3209172577565367, 0.8524966764889779, 0.6153396261153655, 0.8759167778968397, 0.6683436738958713, 1.0368573733736974, 0.7701921457925751, 0.6271624104178555, 0.7416207823202436, 0.5970480960464152, 0.7127393455390115, 0.7205375313813481, 1.1811321894063576, 1.653408565151252], 'auto_ets': [0.7769203160992143, 0.6956884577796731, 0.6746679707252661, 0.5990602963238022, 0.9198237627912577, 0.7897761361914866, 1.0198653066028238, 1.1195051356858252, 1.051045618565216, 0.7582015570127624, 0.7514564186807885, 0.7728705051175156, 0.7348854151260794, 0.9771442721814989, 0.7475546346426261, 0.793699350467378, 0.7502018750288744, 0.6776034424127398, 0.7462190787674441, 0.7488001464368269, 0.7779536021464849, 0.8515335313635055, 0.8087562163558559, 0.6522060960193249, 0.8192793828145881, 0.7666241099284039, 0.8545698602276107, 0.6950250501657096, 0.8585728757344135, 1.0114616348506873, 0.7908611907940069, 0.8115232659546141, 0.7870600919215549, 0.773729603387678, 0.7534842093116038, 0.679231340102661, 0.859509444611797, 0.8347154157425389, 0.8986249703496647, 0.7058547080197739, 0.6412490314372508, 1.046751463505401, 0.7760488640842702, 0.5512731235035325, 0.8067577566690008, 0.5609929854648159, 0.6942231261520426, 0.716834294080736, 1.0444737259968486, 1.1049612551685724], 'theta': [0.7713647917186135, 0.6933267264491708, 0.674991715557547, 0.5744230345535679, 0.9211124685042607, 0.790004470331766, 0.9481515181281537, 1.0823389783733302, 0.9632212361895425, 0.7582509992765216, 0.7524277080338475, 0.7547995792523072, 0.737715396195149, 0.8946085676741223, 0.7470918856738143, 0.7982886848178635, 0.7475517904971017, 0.729408922700018, 0.7442714770499343, 0.7660208781572635, 0.718519204753929, 0.8542457292549439, 0.8072245119833503, 0.6485161065385959, 0.8179148920573001, 0.7520160950198148, 0.8538598646048629, 0.7411807183849344, 0.8583957599033488, 1.0064557840370498, 0.744373522874534, 0.7779526921101982, 0.7862686186096194, 0.6639364934311609, 0.7450485856914812, 0.6781492044552553, 0.7339903741137068, 0.8345829829229825, 0.6409462465740989, 0.6499861912445496, 0.6285957771749047, 1.0579171178173628, 0.777218720654084, 0.5245960748150564, 0.8032885037507075, 0.5612721728966454, 0.6954974527798786, 0.716228987440181, 1.0290995503013456, 0.9977368403649308], 'croston': [0.7995370180843587, 0.7093882163731058, 0.6819192056982343, 0.5906480793979644, 0.92827553450216, 0.8250770999113717, 0.9618872889015754, 1.1587751273209754, 1.0410840326115802, 0.7644611925810636, 0.7577310017996488, 0.8749497244943818, 0.7172163675089834, 0.7311415936412713, 0.7601388639541795, 0.8153605932436708, 0.7384430094294904, 0.7269144860508758, 0.7570093740073767, 0.6687282340695313, 0.7788787589264605, 0.862236294509113, 0.825882272388656, 0.5463726154081024, 0.8435189656206414, 0.7934633938389603, 0.8624916601237029, 0.9046018862767555, 0.8625682021174118, 0.8771832323297781, 0.7984206223838027, 0.7890490311647543, 0.7998875449185671, 0.7833239881599569, 0.7860863704713916, 0.7037919624013594, 0.8368639532470236, 0.8440712156902436, 0.836054043806862, 0.7377567954249837, 0.5514815494907342, 0.8678173695065514, 0.7743650472956978, 0.6248770450940575, 0.8091967071249214, 0.5407783116018748, 0.7053050974696564, 0.7090389437375639, 0.7675101724394661, 1.1692675450145762], 'prophet': [0.5943854086848541, 0.7371955780481003, 0.6947536098173078, 0.9490083071548989, 0.9124442621757517, 0.8481116310973218, 0.9249054055951723, 1.0152348903860677, 1.184945507110088, 0.79375343773054, 0.7666388330391157, 0.5862481811606415, 0.7169431148491611, 0.5960070299053635, 0.7751416415692027, 0.8347248956353055, 0.7920147106224827, 3.163630840267456, 0.7788610950923994, 0.8255156360935448, 0.7773538331285781, 0.8920780132031708, 0.8231548940585757, 0.5281063441487037, 0.8429224730654266, 0.6234256853023609, 0.8808418617849152, 0.5874221441050388, 0.8988698546268736, 0.7135632516275624, 0.7574151281569984, 0.6679962695708644, 0.8222771778729636, 0.8928350465413459, 0.8616099800315399, 0.6779479269248982, 6.653804537421309, 0.8593142074051476, 7.061500105049195, 0.6286764216672817, 0.5672465250370777, 0.860391622889467, 0.7949784805265276, 2.1407552934628815, 0.9079950437751843, 0.5206522375249674, 0.7069214109798534, 0.7319416944792878, 0.6970991374412259, 1.3567642266290372], 'Avg_ensemble': [0.7385540522610406, 0.6871272904829725, 0.6830121073233888, 0.5560391601151871, 0.9233682234742788, 0.806608712468541, 1.0092738201321543, 1.1084238522697665, 1.0486484028547076, 0.7640474493192311, 0.7435994240159698, 0.7123053930991249, 0.7266418240469708, 0.8581949571188899, 0.7545173484220657, 0.7983593817108877, 0.7498652351368262, 0.6699938711756532, 0.7511372821272565, 0.7494632581748308, 0.7483161662895825, 0.856084350319431, 0.8121255729419973, 0.6301138978433971, 0.8277727141920375, 0.7509552808555335, 0.8601017227673877, 0.6939217674815079, 0.8635113863874835, 0.9219758695248812, 0.7491720082745364, 0.7402750244989352, 0.79503256368043, 0.7061302044352462, 0.7628236062806211, 0.6789016248287574, 1.4711132140493377, 0.8414425589897404, 1.349826915013616, 0.6295328628280848, 0.6068284721450916, 0.9095554154342259, 0.7689763557782272, 0.6155682570760793, 0.808765731714427, 0.5605361668677559, 0.6919304304724485, 0.7088100946406977, 0.9053795689469502, 1.1788473695530328], 'Weighted_ensemble': [0.8819668464225366, 0.7384122406842167, 0.6837189146932325, 0.7157295452187391, 0.9624497004839185, 0.8279737344151299, 1.0421456759904164, 1.4424763421906635, 0.9708786487036302, 0.7655425418461148, 0.7663274087014489, 1.2329085378690314, 0.7413268220365288, 1.6091175667032787, 0.7696963862967285, 0.819977911054427, 0.6930938169051797, 0.8094429442409261, 0.7723716271949685, 0.6634999537061483, 0.7893077666349644, 0.8940095646124985, 0.8253268557662461, 0.6490745553403111, 0.856631237884439, 1.0478065977129982, 0.8656409988830708, 1.569559086024643, 0.863357936937624, 0.8249007966293664, 0.8394146273410802, 0.8188880011654531, 0.8161484661735088, 1.1203121672597733, 0.754651343611745, 0.803441683823987, 0.8950176020953329, 0.8506401433111628, 0.8472053465414143, 0.8026403496600212, 0.5329248432179406, 0.8205242048853245, 0.7666324086711167, 0.5900728764834393, 0.8127301029425387, 0.5317373124239914, 0.7395581659612673, 0.7167176617161733, 0.7295746025563191, 1.46126083280756]}\n====================\nOriginal LSTM time is [12.383778095245361, 7.5017406940460205, 12.371533632278442, 8.789638042449951, 0.3542673587799072, 6.755014657974243, 54.67467403411865, 42.80170202255249, 42.697455167770386, 8.166824579238892, 8.799713850021362, 3.9071707725524902, 13.266402244567871, 9.203044414520264, 0.9089932441711426, 12.037763833999634, 5.964967250823975, 8.503984212875366, 1.3453102111816406, 0.87699294090271, 4.815322399139404, 1.8010976314544678, 5.236680269241333, 11.841272830963135, 1.7394015789031982, 19.04154133796692, 1.2748408317565918, 5.729899644851685, 3.9860925674438477, 4.930696964263916, 4.387789011001587, 13.881878137588501, 0.7340834140777588, 20.11744451522827, 1.9564063549041748, 25.311565399169922, 1.0567433834075928, 1.1602137088775635, 1.5472617149353027, 13.454665899276733, 11.725611209869385, 1.9247524738311768, 6.610523223876953, 5.222144603729248, 14.969639778137207, 16.568434715270996, 7.067835807800293, 5.97577428817749, 3.0322377681732178, 24.471866846084595]\n"}], "source": "print('Smape is: ',Smape)\nprint('====================')\nprint('Wape is: ',Wape)\nprint('====================')\nprint('Pe is: ',Pe)\nprint('====================')\nprint('Time is: ',Time)\nprint('====================')\nprint('AggSmapes is: ',AggSmapes)\nprint('====================')\nprint('AggWapes is: ',AggWapes)\nprint('====================')\nprint('AggPes is: ',AggPes)\nprint('====================')\nprint('SMAPE_dist_median is: ',SMAPE_dist_median)\nprint('====================')\nprint('SMAPE_dist_80 is: ',SMAPE_dist_80)\nprint('====================')\nprint('SMAPE_dist_90 is: ',SMAPE_dist_90)\nprint('====================')\nprint('WAPE_dist_median is: ',WAPE_dist_median)\nprint('====================')\nprint('WAPE_dist_80 is: ',WAPE_dist_80)\nprint('====================')\nprint('WAPE_dist_90 is: ',WAPE_dist_90)\nprint('====================')\nprint('PE_dist_median is: ',PE_dist_median)\nprint('====================')\nprint('PE_dist_80 is: ',PE_dist_80)\nprint('====================')\nprint('SmapePE_dist_90 is: ',PE_dist_90)\nprint('====================')\nprint('Original LSTM time is', times_org)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"image/png": "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\n", "text/plain": "<Figure size 432x288 with 1 Axes>"}, "metadata": {"needs_background": "light"}, "output_type": "display_data"}], "source": "%matplotlib inline\nimport numpy as np\navg_,med_,e80_,e90_,name=[],[],[],[],[]\n#a=[MAPE_clus,MAPE_agg,WAPE_clus,WAPE_agg]\nfor k,v in Time.items():\n    avg_.append(np.mean(v))\n    med_.append(np.percentile(v,50))\n    e80_.append(np.percentile(v,80))\n    e90_.append(np.percentile(v,90))\n    name.append(k)\n\n\n\nx_axis = np.arange(len(Time))\n\n# Multi bar Chart\n\nbar_w=.1\nplt.bar(x_axis -0.15, avg_, width=bar_w, label = 'Avg',color='red')\nplt.bar(x_axis -0.05, med_, width=bar_w, label = 'Median',color='green')\nplt.bar(x_axis +0.05, e80_, width=bar_w, label = '80-%ile',color='cyan')\nplt.bar(x_axis +0.15, e90_, width=bar_w, label = '90-%tile',color='black')\n\nplt.xticks(x_axis, Time.keys(),rotation = 70)\nplt.legend(bbox_to_anchor=(0.41,0.65))\nplt.ylabel('Time (seconds)')\nplt.xlabel('Model')\nplt.show()  "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"image/png": "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\n", "text/plain": "<Figure size 432x288 with 1 Axes>"}, "metadata": {"needs_background": "light"}, "output_type": "display_data"}], "source": "import numpy as np\navg_,med_,e80_,e90_,name=[],[],[],[],[]\n#a=[MAPE_clus,MAPE_agg,WAPE_clus,WAPE_agg]\nfor k,v in Pe.items():\n    avg_.append(np.mean(v))\n    med_.append(np.percentile(v,50))\n    e80_.append(np.percentile(v,80))\n    e90_.append(np.percentile(v,90))\n    name.append(k)\n\n\n\nx_axis = np.arange(len(Smape))\n\n# Multi bar Chart\nplt.bar(x_axis -0.15, avg_, width=bar_w, label = 'Avg',color='red')\nplt.bar(x_axis -0.05, med_, width=bar_w, label = 'Median',color='green')\nplt.bar(x_axis +0.05, e80_, width=bar_w, label = '80-%ile',color='cyan')\nplt.bar(x_axis +0.15, e90_, width=bar_w, label = '90-%tile',color='black')\n\nplt.xticks(x_axis, Smape.keys(),rotation = 70)\nplt.legend(bbox_to_anchor=(0.41,0.65))\nplt.ylabel('Weighted PE')\nplt.xlabel('Model')\nplt.show()  "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAFjCAYAAADSPhfXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi40LCBodHRwOi8vbWF0cGxvdGxpYi5vcmcv7US4rQAAIABJREFUeJzt3XecVOX1x/HPoQkoigIqERVsSBEVVsFoEAtFFBU1CvYWYjTWaNTYNV39aSyIGo0tooaoIGIw9ooKFsCOioqxIJZEihTP749zZxiWZZnduXd32f2+X6997U67z72zM/fcp53H3B0RERGARrW9AyIiUncoKIiISJ6CgoiI5CkoiIhInoKCiIjkKSiIiEiegoKIiOQpKIiISJ6CgoiI5DWp7R2oqrZt23rHjh1rezdERFYpU6ZM+dLd263seatcUOjYsSOTJ0+u7d0QEVmlmNmHxTxPzUciIpKnoCAiInkKCiIikqegICIieQoKIiKSp6AgIiJ5q9yQ1Pps0aJFzJo1iwULFtT2rqwSmjdvTocOHWjatGlt74pIvaGgUIfMmjWLVq1a0bFjR8ystnenTnN35syZw6xZs+jUqVNt745IvaHmozpkwYIFtGnTRgGhCGZGmzZtVKuSSplZ/keKo6BQx+jDWzy9VyLpU1AQEZE8BYW6zCzdnyLdf//9mBlvvfVWhgcnInVRZkHBzG42sy/MbPoKHj/EzKaa2TQze87Mts5qX4qltscwevRodtppJ0aPHl3buyIiNSzLmsItwKBKHv8A2NndtwIuAW7IcF+kSN999x3PPPMMN910E3fddRcAw4YN48EHH8w/58gjj2TMmDHMmzePAw88kK5duzJ06FB69+6tDLYiq7jMgoK7PwV8Vcnjz7n718nNSUCHrPZFijd27FgGDRrEFltsQZs2bZgyZQoHHXQQ99xzDwALFy7k0UcfZc8992TkyJGsvfbavPHGG1xyySVMmTKllvdeREpVV/oUjgEeWtGDZjbCzCab2eTZs2fX4G41PKNHj2bYsGFA1BBGjx7NHnvsweOPP87333/PQw89RN++fWnRogXPPPNM/rndu3enR48etbnrIpKCWp+8Zma7EEFhpxU9x91vIGleKisr8xratQbnq6++4rHHHmPatGmYGUuWLMHMuPTSS+nXrx8TJ07k7rvvzgcCEal/arWmYGY9gL8C+7j7nNrcF4ExY8Zw2GGH8eGHHzJz5kw+/vhjOnXqxNNPP81BBx3E3/72N55++mkGDYquoh133DHfrPTGG28wbdq02tx9EUlBrQUFM9sIuBc4zN3fqa39qNPc0/1ZidGjRzN06NBl7tt///0ZPXo0AwYM4Mknn2T33XenWbNmABx//PHMnj2brl27cu6559KtWzfWWmutTN4KkYaqpmdlZ9Z8ZGajgX5AWzObBVwANAVw91HA+UAbYGRysIvdvSyr/ZGVe/zxx5e776STTsr//dVXy44baN68OXfccQfNmzfnvffeY/fdd2fjjTfOfD9FJDuZBQV3H76Sx48Fjs2qfMnevHnz2GWXXVi0aBHuzsiRI/O1iIau8KrOi6ilidQVtd7RLKuuVq1aaV6CSD1TV4akiohIHaCgICIieQoKIiKSp6AgIiJ56miuw+yidMcl+wUrHwVjZhxyyCHccccdACxevJj27dvTu3dvxo8fX3RZ/fr147LLLqOsrIzBgwdz55130rp162rvu4jUDAUFWcbqq6/O9OnTmT9/Pi1atODf//43G2ywQUnbnDBhQkp7JyJZU/ORLGfw4MH5VNmjR49m+PClU07mzp3L0Ucfzfbbb8+2227L2LFjAZg/fz7Dhg2jS5cuDB06lPnz5+df07FjR7788ksA9t13X3r16kW3bt244Yal2dLXWGMNzjnnHLbeemv69OnD559/XhOHKiLlKCjIcoYNG8Zdd93FggULmDp1Kr17984/9rvf/Y5dd92VF198kccff5wzzjiDuXPnct1119GyZUvefPNNLrroohWm0b755puZMmUKkydP5qqrrmLOnEh5NXfuXPr06cNrr71G3759ufHGG2vkWEVkWQoKspwePXowc+ZMRo8ezeDBg5d57OGHH+aPf/wj22yzDf369WPBggV89NFHPPXUUxx66KH5168ojfZVV12Vrw18/PHHvPvuuwA0a9aMvfbaC4BevXoxc+bM7A5Q6oyscvpY8iNVpz4FqdDee+/N6aefzhNPPJG/modI2fDPf/6Tzp07V3mbTzzxBI888gjPP/88LVu2zAcVgKZNm+ZPDo0bN2bx4sXpHIiIVEmDrykYuqqoyNFHH80FF1zAVltttcz9AwcO5Oqrr87n83nllVcA6Nu3L3feeScA06dPZ+rUqctt89tvv2XttdemZcuWvPXWW0yaNCnjoxCRqlJNoQ4rZghpeblcRGVlpSWc7dChwzIZUnPOO+88TjnlFHr06MEPP/xAp06dGD9+PL/4xS846qij6NKlC126dKFXr17LvXbQoEGMGjWKLl260LlzZ/r06VPSPopI+mxVy+BYVlbmaSZhW6aGkDRf1NZ78uabb9KlS5eStpFWUFhVpPGeZUFZUotjGX3n8u/+Kvx/SPsYzGxKMcsTNPjmIxERWUpBQURE8hQURERKUJNLZdYEdTRLlRT25zSUfouqqD+nBmmoVFMQEZE8BQUREclTUKjDrBo/25WVsV1ZWYWPFeuKK66gW7dudO/eneHDh7NgwQI++OADevfuzdChQzn77LNZtGjRcq/7/vvvGTRoEN27d2fkyJH5+0eMGMHLL7+cvz1q1Chuu+02AI488kjGjBlThb0TkSwpKNSgXIdUXe6U+uSTT7jqqquYPHky06dPZ8mSJdx1112ceeaZnHrqqdx3332sueaa+eyohSZOnMhOO+3E1KlTuf322wF47bXXWLJkCT179sw/77jjjuPwww9f4T5MnjyZNOeiVKSu/x+KkfUxrAqfV0mfgoIsZ/HixcyfP5/Fixczb9482rdvz2OPPcYBBxwAwJ577smTTz653OuaNm3KvHnzWLRoUX6SzXnnnccll1yyzPMuvPBCLrvssuVeP2XKFHbeeWcOO+wwTjzxRD799NMMjk5EKqOgIMvYYIMNOP3009loo41o3749a621Fr169aJ169Y0aRKD1dZdd12++OKL5V7bv39/Zs6cSZ8+fTjppJMYN24cPXv25Ec/+tFKy120aBEnnngiY8aM4fbbb2fIkCGcc845qR+fiFQusyGpZnYzsBfwhbt3r+BxA/4CDAbmAUe6+8vlnyc16+uvv2bs2LF88MEHtG7dmp/+9Kf861//Kuq1TZo0ySfFW7RoEQMHDmTs2LGcdtppfPTRRxx++OHsvffeFb727bffZvr06fTv35958+bxww8/sMkmm6R2XCJSnCznKdwCXAPctoLH9wA2T356A9clv6UWPfLII3Tq1Il27doBsN9++/Hss8/yzTff5NNZf/HFF6y77rosWbIkn/hu77335uKLL85vZ+TIkRx++OFMmjSJtdZai7vvvptdd911hUHB3enWrRvPP/98g8vfJFKXZBYU3P0pM+tYyVP2AW7zaHyeZGatzay9u6shuRZttNFGTJo0iXnz5tGiRQseffRRysrK2GWXXRgzZgybbbYZDz74IH379qVx48a8+uqry23j66+/Zvz48UycOJEHHniARo0aYWbLLNFZXufOnZk9ezbPP/88TZs2ZfHixbz++ut069Yty8MVkXJqc0bzBsDHBbdnJfctFxTMbAQwAuKk1VBUJx9iqVfZvXv35oADDqBnz540adKEbbfdlhEjRrDnnnsybNgwPv30Uzp37sw+++yzwm1cfPHFnHPOOTRq1IiBAwdy7bXXstVWW3Hcccet8DXNmjVjzJgxnHTSSXz22WcsXryYs88+W0FBpIZlmjo7qSmMX0Gfwnjgj+7+THL7UeBMd690LOKqnDp7ZemUV4XU2TWR5qIqx1Dd96w+pGzO6hjKbz/tMmriO1cf/g+1lTq7NmsKnwAbFtzukNwnVZTtiH4RaUhqc0jqOOBwC32Ab9WfICJSu7Ickjoa6Ae0NbNZwAVAUwB3HwVMIIajziCGpB6V1b5I6VQbEVmqPs/xznL00fCVPO7ACVmVL1Jeff4ii6RF6ynUAJ2MiqPayMrVxGdJn9eGTWkuREQkT0GhDivMUlnZz3aFP9ttx3bbbVfh84r1l7/8he7du9OtWzeuvPJKAL796itO6N+f/fbbjxNOOIH//ve/Fb72kEMOoUePHvzmN7/J3/fb3/6W+++/P3/7iSee4LnnnsvfLkylfeGRR/Loo49W6X0SkfQoKMgypk+fzo033siLL77Ia6+9xvjx45kxYwa3/vGPbLfbbtx7771st9123Hrrrcu9durUqbRo0YKpU6fy0ksv8e233/Lpp5/ywgsvsO++++afVz4orCyVtojUHAUFWcabb75J7969admyJU2aNGHnnXfm3nvv5cmxY9nriCMA2GuvvXjiiSeWe23Tpk2ZP38+P/zwA4sWLaJx48acf/75XHTRRfnnzJw5k1GjRnHFFVewzTbb8PTTT680lXavXr0YOHCgUmmL1AAFBVlG9+7defrpp5kzZw7z5s1jwoQJfPzxx3z1+ee0bd8egDZt2vDVV18t99ouXbrQrl07evbsyZAhQ5gxYwY//PDDMgvsdOzYkeOOO45TTz2VV199lZ/85CcV7sfixYvzqbSnTJnC0UcfrVTaIjVAo49kGV26dOHMM89kwIABrL766myzzTY0btx4medU1keR64MAGDJkCNdffz2/+93veO211+jfvz8/+9nPitqPmTNn5lNpAyxZsoT2SVASkeyopiDLOeaYY5gyZQpPPfUUa6+9NltssQXrrLceXybNN19++SVrr702AAMHDmSbbbbh2GOPXWYbY8eOpVevXnz33Xe899573HPPPYwZM4Z58+YVvR/dunXj1Vdf5dVXX2XatGk8/PDD6R2kiFRINQVZTm69hI8++oh7772XSZMm8fwHHzD+1ls5cvfdGT9+PDvvvDMQ6zKXt2jRIq688koefPBB3n333XytYsmSJSxcuJBWrVqtcPRSzsYbb5xPpb3DDjuwaNEi3nnnHWVNFcmYgkIdVmxGxGUmfaWQJXX//fdnzpw5NG3alGuvvZbWrVtzxFlncfaBBzJu5EjWX399/vCHP6zw9ddeey1HHHEELVu2pEePHsybN4+tttqKwYMH07p1a4YMGcIBBxzA2LFjufrqqyvcRtOmTfOptL/99lsWL17MKaecoqAgkrFMU2dnYVVMnV1sCtzqpoFOOyhUWkaGqbPLl5F26uy69L8uefsFZegYKilDx7B0e0WmzlafgoiI5K0wKJjZoQV/71jusV9muVMiIlI7KqspnFbwd/mG36Mz2Bchu9Wh6iO9VyLpqywo2Ar+rui2pKB58+bMmTNHJ7siuDtz5syhefPmtb0rIvVKZaOPfAV/V3RbUtChQwdmzZrF7Nmzq/S6L5e5EbfefPPN9HassIwvl5aWdRkr237z5s3p0KFDqvsg0tCtcPSRmc0jVkUzYNPkb5Lbm7j76jWyh+XU59FHJW+/oIxV7RgqKiOz7ddEGfpfr3z7BWXoGCopo4ZHH1VWU6j62EgREVmlrTAouPuHZrYvsBkwzd2Xn7oqIiL1SmVDUkcCpwJtgEvM7Lwa2ysRkWKZLdPEIqWprPmoL7C1uy8xs5bA08AlNbNbIiJSGyoLCgvdfQmAu8+zqqznWFcVHoKGfYrUK3ZRwff7An2/q6uyoLClmU1N/jZg0+S2Ae7uPTLfOxERqVEafSQiUttyrRh1oAVjhR3N7v5hRT/AhsCvi9m4mQ0ys7fNbIaZnVXB4xuZ2eNm9oqZTTWzwdU/FBERKVVR6ymY2bbAwcBPgQ+Ae4t4TWPgWqA/MAt4yczGufsbBU87F7jH3a8zs67ABKBjlY5ApJapLVvqk8qGpG5hZheY2VtEQryPiBnQu7h7xSujLGt7YIa7v+/uC4G7gH3KPceBNZO/1wL+U+UjEKktGgop9VBlNYW3iGGoe7n7DAAzO7UK294A+Ljg9iygd7nnXAg8bGYnAqsDu1dh+yIikrLKsqTuB3wKPG5mN5rZbqSfHXU4cIu7dwAGA7eb2XL7ZGYjzGyymU2uarI4EREpXmUdzfe7+zBgS+Bx4BRgXTO7zswGFLHtT4hO6ZwOyX2FjgHuScp7HmgOtK1gX25w9zJ3L2vXrl0RRYuISHWsdDlOd5/r7ne6+xDixP4KcGYR234J2NzMOplZM2AYMK7ccz4CdgMwsy5EUFBVQESklqywT8HM1lnBQ2OSn0q5++Jk2c6JQGPgZnd/3cwuBia7+zjgV8CNSV+FA0e6VpgREak1lXU0f0l0Di9Obhf2Jziwyco27u4TiGGmhfedX/D3G8CO5V8nIiK1o7KgcBWwC/AsMBp4RlfxdUQdmv0oIvVLZespnJIkwesHHAZcbWYPA9e5+wc1tH+ZyU840mQjEakj6sJEyEo7mj08TqS1GAUcheYSiIjUW5V1NK9OzEA+CGhHpLbo5e4f1dC+iciqTunqVzmV9Sl8AbxLpKd4l+hcLjOzMgB3X2n+I8mWmsBEJG2VBYV/EIGgc/JTyCkiKZ6IiKxaKutoPrIG90PqElX5JQOrbM22gX0fKsuSOsTMNi64fb6ZvWZm48ysU83snoiI1KTKRh/9jiTlhJntBRwKHE2kqhiV/a6JSOaU/rtK7CJbdthoPVRZn4K7+7zk7/2Am9x9CjDFzI7PftekXktORHZhwX2rWrNC1gpO1vn3Se+RZKyyoGBmtgYwj0haN7LgseaZ7tWqrJ6d7NQO3DDUhUlTUjdUFhSuBF4F/gu86e6TIb8056c1sG8iqVhlA5tILahs9NHNZjYRWBd4reChz4iZzSIiUs9UVlPA3T+h3MI47q5agohIPbXSRXZERKThUFAQEZG86qy8BoC7f5X+7oiISG2qrE9hCpHjyICNgK+Tv1sTaytrVrOISD2zwuYjd+/k7psAjwBD3L2tu7cB9gIerqkdFBGRmlNMn0KfZK1lANz9IeDH2e2SiIjUlkqHpCb+Y2bnAncktw8B/pPdLomISG0ppqYwnFh57T5iDYV2yX0iIlLPrLSmkIwyOtnMVnf3uTWwTyIiUktWWlMwsx+b2RvAm8ntrc1s5EpeJiIiq6Bimo+uAAYCcwDc/TWgb5Y7JSIitaOoGc3u/nG5u5YU8zozG2Rmb5vZDDM7awXPOdDM3jCz183szmK2KyIi2Shm9NHHZvZjwM2sKXAySVNSZcysMXAt0B+YBbxkZuPc/Y2C52wOnA3s6O5fm9m61TkIERFJRzE1heOAE4ANiIyp2wDFrLy2PTDD3d9394XAXcA+5Z7zM+Bad/8awN2/KHbHRUQkfcUEhc7ufoi7r+fu67r7oUCXIl63AVDY7DQrua/QFsAWZvasmU0ys0EVbcjMRpjZZDObPHv27CKKFhGR6igmKFxd5H3V0QTYHOhHzH240cxal3+Su9/g7mXuXtauXbuUihYRkfIqy5K6A5HOop2ZnVbw0JpA4yK2/QmwYcHtDpRbsIeoPbzg7ouAD8zsHSJIvFTE9kVEJGWV1RSaAWsQgaNVwc9/gQOK2PZLwOZm1snMmgHDgHHlnnM/UUvAzNoSzUnvV2H/RUQkRZWt0fwk8KSZ3eLuH1Z1w+6+2Mx+CUwkahY3u/vrZnYxMNndxyWPDUgmxy0BznD3OdU6EhERKVkxQ1JXM7MbgI6Fz3f3XVf2wiS76oRy951f8LcDpyU/IiJSy4oJCv8ARgF/pchJayIismoqJigsdvfrMt8TERGpdcWs0fyAmR1PpM7+Pve41mgWEal/il2jGeCMgscc2CSrnRIRkdpR2eijTjW5IyIiUvtW2qdgZvtVcPe3wDTlKhIRqV+K6Wg+BtgBeDy53Y9oWupkZhe7++0Z7ZuIiNSwYoJCE6CLu38OYGbrAbcBvYGnAAUFEZF6opiEeBvmAkLii+S+r4BF2eyWiIjUhmJqCk+Y2XhiEhvA/sl9qwPfZLZnIiJS44oJCicQgWDH5PZtwD+TFBW7ZLVjIiJS81YaFJKT/5jkR0RE6rHKZjQ/4+47mdn/iMlq+YeIWLFm5nsnIiI1qrLJazslv1vV3O6IiEhtKmb0EWa2k5kdlfzd1sw021lEpB5aaVAwswuAM4Gzk7uaAXdkuVMiIlI7iqkpDAX2BuYCuPt/iGU5RUSknikmKCxMRiA5QDI/QURE6qFigsI9ZnY90NrMfgY8AtyY7W6JiEhtKGaewmVm1h/4L9AZON/d/535nomISI2rbJ7CKcBzwMtJEFAgEBGp5yqrKXQArgS2NLNpwLNEkHhOS3GKiNRPlU1eOx3AzJoBZcCPgaOAG8zsG3fvWjO7KCIiNaWYhHgtgDWBtZKf/wDTstwpERGpHSscfWRmN5jZs8DdxMprzwE/dfcydz+qmI2b2SAze9vMZpjZWZU8b38zczMrq+oBiIhIeiobkroRsBrwGfAJMIsqrJ9gZo2Ba4E9gK7AcDNbrsnJzFoBJwMvFL/bIiKShRUGBXcfBGwHXJbc9SvgJTN72MwuKmLb2wMz3P19d18I3AXsU8HzLgH+BCyo0p6LiEjqKp285mE6MAF4iBiBtClxZb8yGwAfF9yeldyXZ2Y9iaU9H6zKTouISDYqm6dwEjHi6MfEWszPJT83k0JHs5k1Av4POLKI544ARgBstNFGpRYtIiIrUNnoo47Eusynuvun1dj2J8CGBbc7JPfltAK6E+s9A6wPjDOzvd19cuGG3P0G4AaAsrKywgV/REQkRZXNUzitxG2/BGyerL3wCTAMOLhg+98CbXO3zewJ4PTyAUFERGpOUYvsVIe7LwZ+CUwE3gTucffXzexiM9s7q3JFRKT6ipm8Vm3uPoHopC687/wVPLdflvsiIiIrl1lNQUREVj0KCiIikqegICIieQoKIiKSp6AgIiJ5CgoiIpKnoCAiInkKCiIikqegICIieQoKIiKSp6AgIiJ5CgoiIpKnoCAiInkKCiIikqegICIieQoKIiKSp6AgIiJ5CgoiIpKnoCAiInkKCiIikqegICIieQoKIiKSp6AgIiJ5CgoiIpKnoCAiInmZBgUzG2Rmb5vZDDM7q4LHTzOzN8xsqpk9amYbZ7k/IiJSucyCgpk1Bq4F9gC6AsPNrGu5p70ClLl7D2AM8Oes9kdERFYuy5rC9sAMd3/f3RcCdwH7FD7B3R9393nJzUlAhwz3R0REViLLoLAB8HHB7VnJfStyDPBQRQ+Y2Qgzm2xmk2fPnp3iLoqISKE60dFsZocCZcClFT3u7je4e5m7l7Vr165md05EpAFpkuG2PwE2LLjdIblvGWa2O3AOsLO7f5/h/oiIyEpkWVN4CdjczDqZWTNgGDCu8Almti1wPbC3u3+R4b6IiEgRMgsK7r4Y+CUwEXgTuMfdXzezi81s7+RplwJrAP8ws1fNbNwKNiciIjUgy+Yj3H0CMKHcfecX/L17luWLiEjV1ImOZhERqRsUFEREJE9BQURE8hQUREQkT0FBRETyFBRERCRPQUFERPIUFEREJE9BQURE8hQUREQkT0FBRETyFBRERCRPQUFERPIUFEREJE9BQURE8hQUREQkT0FBRETyFBRERCRPQUFERPIUFEREJE9BQURE8hQUREQkT0FBRETyFBRERCQv06BgZoPM7G0zm2FmZ1Xw+Gpmdnfy+Atm1jHL/RERkcplFhTMrDFwLbAH0BUYbmZdyz3tGOBrd98MuAL4U1b7IyIiK5dlTWF7YIa7v+/uC4G7gH3KPWcf4Nbk7zHAbmZmGe6TiIhUwtw9mw2bHQAMcvdjk9uHAb3d/ZcFz5mePGdWcvu95DlfltvWCGBEcrMz8HaKu9oW+HKlz6rbZdSHY6iJMnQMdaMMHUPtlLGxu7db2ZOapFhgZtz9BuCGLLZtZpPdvSyLbddUGfXhGGqiDB1D3ShDx1B3yqhIls1HnwAbFtzukNxX4XPMrAmwFjAnw30SEZFKZBkUXgI2N7NOZtYMGAaMK/ecccARyd8HAI95Vu1ZIiKyUpk1H7n7YjP7JTARaAzc7O6vm9nFwGR3HwfcBNxuZjOAr4jAUdMyaZaq4TLqwzHURBk6hrpRho6h7pSxnMw6mkVEZNWjGc0iIpKnoCAiInkKCiIikqegIA1OfZk1X1+OQ9JXymdDQaEeMrOsEx02SvOEZGZbmtla5e7L7IRXOOw5i/eqpk7W5Y4jzf9Ho4K/V/nAY2bDzWzL2t6PmlTK0H4FhYSZtTOz1hmXsZWZrZbh9psCuPsPBfeV9KXOvd5CDzNb191/yH3oUjqp/h9QlmxvOzPrkNV8FTM73Mza5m4XvlfV2Faj5PdmZrZrwTYzG9JXUOYIM9sjizLd/YfcdyGLYyn4TDXL3U4mr6ZZRhMz2yH5vv0CmJncn9n3L9l+7v+zlpntZmYHmNnWWZZZrtwNzex4M7u4uoGwwQaFgjexl5mNBX4HnGRmh5nZtmbWIoUych/+TmZ2OXA2cH9ypd2y1O0n226c/N4BONfMvjaz83OPp/ilHgUcB3yQpDkfYWZNkhNItQOPmW0FtHP3R82sH/A34NXk5J3KVWrB/3pX4CB3/9LM2pjZH8zs5BJOSLn9Ox3YPCnjJ2b2azNrU/qeL68giB1LkiHAzC43s9vMbJPqbrfgc7SPmZ0HnGdmvzKzAWa20nw5VVHwmfyTmb0GjAR+amZdUrwwWx3YG5gPbAXskpT9PYCZ3ZjGd7wCuc/EH4gM0TcBOyZlrp9BecAyn4u/A98Tn8nNknK7Jr+L+j412KBQYAQwFRgLzAW2Bn4O/DSFbefe3xHAbGAy8J/kH/gTM9s7hTJyX7DzgUeAfwCLAMzsaDPbvrobNjNzdzezLkA34GQiGeHtxId+oZl1LDHwbAK8YGa9gCOBo4EBwB4ZXKUeAtycnAAvANoBPYEe1dmYuy9JtrWbu19vZjsBpwJ7Ab83s+Yp7fcyzOwnwDfuPtXMfk0cx7fAybnaYlW5+5Lkz3OBd4B9ieSTvwKuKOVzVJHEjS9nAAAgAElEQVTkAqCM+J69AwwFfg+ck8bVvLt/6+5nA2cCdwIXmdlXSfA8D+jg7vPTuvAoKHdJUgP6ibufTmR2eDp5eJSZ7ZhmebDMxeeWwHx3vwmYBjyc7MvfzGzTYr9PDTYoFETW/wC3ufuD7n45cD0wCXgthTJyX7QdgT8TJ6CxyX37A1ukUMYPZrY20Ap4DugO3J08fDBxxVTdbec+RMOAe4Fdgbfc/Rqi1nC+u8+s7vYTLxL5r/4B3OvuLwIHkWIm3IL/9X+B/sCDRFr3Y4kru61K2Hwn4D0zOx44Hvibu/cF+gDVbppaia+Jc8FdwKZE88j1QFd3X1TVE13BSWUnYJa73028V6cBzwCrAW+lseMFzY29if/3O+5+hbsfCFwGvJO7mk+hrMbA5cDZ7r49EYTmABsAv0melto5sOB97wY8Y2Z7Ac3cfVpy/yZEkEhVwfd0Y+BZMzsNeD1ZsmBT4Ad3f68qG2xwP0Dj5PemxNXJR8BPMygnN2P8GKKKPD13PzAF2DCNMoCWxBf4LOD25P7OwGspHUdXIqHhEcDNwLpEM8/JKW1/NaBJ8vf6xFVOh5S23Sj5vXby82vgiuS+dYA3gRbV2G5/YLXk732JWtqI5PYJwN/T/jwVlN05Kf9Puc8QkUfsyOTvxtXc7pDkWHYhghvEyfufGRzDNcCHRJPqtilvO/f93p24GLsf+E1W/48V7MOxxAXPxUSQ+BNwayn/nyLL/Q3wPnGBskty7KdWpdwGmebCzBp5XGE/DTwBLCSuhtsRJ+vT3P3NlMpYi2jXvBbYgfgizAYWuvuIyrZRxfL6AVcDaxI5U7YFXnH336VVRlLOdUBzopZzkCdrYVRzWycTJ+aDgF+7+7jkSrK9u5fPqFvdMlZz9++TfqPr3P1fyf2NgJ8Au7j7hVXcZk/gXHffL7k6XN/dP00ea0Kc8O7PlZXScbQg3veDgZ09rqxzj7UjTq5newlX2WbWimiPbgM8QHwvFhEBLrU8PMmxbEucLDsS6wbMJS5ibq3kpcVuP9fs+W/gSqLZ858eTXzDgKnu/kap5ZQrsztxwTHK3Z9LjvEE4sT8X+AD4CZ3fy93bkip3IFEABgDTCDex+HAYcS55hngbnf/Lve+rHSjNRk969IPcfL8V7n7NiE6hnZJqQwDXibashsRV9yHJB+URimV0Ql4CuiS3D6caO/vQnIlW83t5q6wtwXOI0466xBJFDctZdvJdhsTAXgrokrdPbn/F0DblN6bNYDfAvcB75HURgoeHw60rsZ2rwAuLni/Lyp4bHWi4zztz2tvolb7LjEoYv1cOcA2QLfqvkfJ76OAfxTc3yo52QwhgytbIhi0Ifpz+hFXuEenuP22wBPJ388UvFcvEO39aR/PxsBVRP/B88AZRI26cVrf9RWU2xn4JXEhOIGokfQqZZsNrqZQcBWxI/EGvgLcAbzh7v/LoLwyoio5yd1vSXnbudrIGURfwjXunlqbZdIm+x5Ry+lHXFlPAsa4+w1FX3lUvO29gcHE1dUEd98pGZH1AlDmKbQrJ1fxqxHt7f2ItvjHgL8CS4CJ7t6xGtt9Fujr0an4MHF1eG/y2PFELfCvpe5/uTIbE6NpziX6W74F3gBeBW4DTnT38dXY7p5EU9RuxJXslWa2hseVZWdgnrt/nOJxtCaC2gBgFvAs8f17F2ju7nNTKmctYgROD6J5cICZdST6MXqmUcYKym1BfNb2IfqVPgSeJEbvLfCUagjlymwGtCZq7z8GegFNiWG4f3D32VXZXoPraC44ia0HjCfegwOAE8zsSDP7URrlFHQ6vUxU7Q40szvMbKM0tg9LO1Dd/VLiWG40swut3ESwEvQAHnf3S919T6KmMBY4zsyaVzcgJF4g0qXfBdyT3HcM8GoaASFxGPFFuYX4ogwjvixjiWaFq6q6weRiYgfgaIvx56vlAkLiCKIGlCp3X+Lu9wH7u/vBxP97C+KYxlQnICQmAq8TwXOomd0A7Glm7ZMyepW+98t0MB8LtHT3zYla4WrEIIM2aQUEiNFHxCi574BPzew+ooZ3R7I/jdMqq5wf3P0homY9iPjuDyKaRFMNCLn31KNDuRHxubuduNC6mWhK+rrK221INYXkRLYgaYM90t0vNbNNiVFBnYH2wCXu/lkKZbUgvrTvEX0VqxFt5z2IavK3JWy7cXKV2hJYDGxJtFluSJzoniWaN5ZUspnKtp+rTW0GnETUDp5O64rRzFYnRjJtQASC74jO5fZEu/9jKZTRgRjNMp/4H7wPPOfJaCmLMeNz3H1RNbbdjjj5n0l08g/36A9Zn+jo71/q/q+g3AOJztONgX09hlS2cPf5yeOl1Nx6EDWPo4ir3AXE+/PzdPY+X85fib6Dqwvuuxx4191Hlbjt3Oe2MdEsOCepHWxH9I98QIzKWVLKe1VBubka+/7EkOqWRI30KXd/Mo0yVlL+BURNrx0R5Ce4+8O5812Vt9fAgsJWxJdqL+Ajdz+q4LH1ge3c/YESy8gFnh2JaP1B8tMbaEZUVkoZAllY1uVEk85koC9x4msJrO7uO5S47bZEX8VnxEn1P8QorfeJALG4hG2fCazn7qdZjH8/juhb2MPdU10M3cx6E+9Rd6I/5F2iyeXlNIKcmW1OjPwaSrQhX+Dul5S63QrK6Ub0d/2SuLLuStR2tyL6xqoc3Aq23Z8YNr2+ux+X3NcO+F91TiorKauMaNa5jvgsNQJGA6d4DEdOo4yfA4cSFxk3EyfJV9PYdgVl5YOLmb1LXCysTsx3+hnwf+5+fQblrkYEunWIGsLmwI+I0WOnAWe4+z0r3kIl225gQWEjor3vb8S4+xnE+OsxxFXf6x5jtEspYwTx5d2MOJmuRbQlzjWzTkQb7eclbL8tUcu5LLndjpiw8p3FBLA5wJfu/l01t1941fNjd/9VMsKhK9E5OM9jUlC1mdnjxBdmAZHi4j1iEt5VadTSkjLyIzzMrJm7L0xOrHsBOydlpTk6qDFxRfq2u1e5yl7JdnNXv38gTqKfAge7+8HJhccZ7r5vCdvtQpw4Lyc65bclrjjbEyskVqu2uYIydwe+JN7/AcnfzYHP3f2kErfdiBhIsNDM3ib+z+sSTYhlRLPhUe7+cinlVFBu7qLjC2C/wv9F8nn7E3BABsH1QKIzfSHQ291/VvDY9sTFyZ7V2XZmy3HWRe7+kZndTbRlf0u0l/Ykag99SaajV5eZrUtcWa9GzGz9gJgE9w7wvrt/UMr2E82BJy1mL95CjE9/jOjILrktu6Ddcygxhh93nwhMTJqTqj0ZDvIdgN8QV3K7EJ3A/yQm3t1FvH8lKQhsnYkmu23MbB1iUt//Ee9btZvvKpKcPCeluc1ku7mrtpeI7+vPifkoEP0JL8HSJsUqbNqIQHw0MXJlPjGEeX7S5HJc0ndRMjPrA3xOjGIb4O6vJt/DLYBP3f3dFIrZARhoZrOI43iXqBU+m+zDAcT3MG0bEM22GwAdLIZs30bURtsTtfYFluIw1ERz4tzVAuhiZrlRdl8DA1ma66mqn4uGU1MoaIc/HXjQ3d9MqmAbEie6ue4+I6Wy2hAjRTYkqvhziSuJSe7+TEpltCNGGmxPjPf+nugw/Ku7/6fEbbcgTtADiZrUtcTIlLTGVncjZnQvdPc/Jlc2V7t775S2n/tf3wDMI/LBrEY0U73i7pdn8CXNVBLURhMjhS4n8h4dSFydflbdNnIzO4II0ocBf3T3yWZ2DXH1XnIzmEXajQOJJo32RNqMZ3JNdxZ5ui7N9YuUUM4Aop+qKfGdeItoZnuzoKzU+hHKlb0O0YzXmRge/ANRO5kH/N7dH6nOybnIsrsTTVW7E83TWxAtIL9294+rc8wNJihAvoo/FRjs7h+a2dXERLJR7v5FCtsvbF/MnZg6ElcxfYkJTRNLLaeCcjckAsSewF+qW2OwmJb/pCdDc5Mq+VFEZ3Af4GaP1BBp7HOu+aJZsv0lnu4EqUZE5/WO7v6NmRnRpDcSON3dS05jUhssEh8eSTS93OTu75fYwdyBCJrbEfmgZhEn7n08xQmERDNKW+L7tgUxVHMRsIW771HJy6tSTlNi0EUnotbfkpg49iUxACDt/qrCJspWRHOoEy0QuxBZAE5K++Kj4NzSOCljibvPSloPBgM/8si7VL3tN4SgUHACOhAY6u7DzexcIrr/L/k5rZSriIImiy2J7Igdku2+CDxOtP1ZiR20uQ9DGfEl3p0YbTCu1Lb45CR6jrtfYmZXElXvf3mSMyXpiN/U3Z8tpZwVlN0cWJTWlVTB//sS4L8eQ3Zz47lfBPqk3cabtSxrNknNsD/Rzr86cGXaQdPM1iRGyrUkrqi3Jkbi3evuD5e47fxVuMVoQidq5hsR/RcdgbPSriUUfOcvJVoEupIMC3X3ZywZGZZhDeVfxACQdYhzzYPAv4Gvks9/9WqPDSEo5JjZtsRMw++Jk8XJZjYYOMTdDylx27kT9k1EX0JufPdiorN5tLv/rZQyCsp6lmhCOJWYMbkecYV3Wqn9CklV+OfEF3Z9oh32UWJ2aMm1qZpkZvsAlxIniSnEFeNqnvIwy5qU1HgaFQbQ5GLnQa/COP/kIuBM4oTyFvGZneTu89Lc1+TkdAQxhHaoma1BDKld7O6pJD0sODlfT3Qu9yK+D3e6+zVmtra7f53FyTm5WJpMdDa3IPri+hPHuGtK/YgVlbsDUfsaQjRT9yRqR5sR73X1J+J6RtOv6+IPMfztaGJWaOvkvoeAPVPafmNgWvL3Y8k/aSgxCagspTK6EVPpjZjoBTGX4GGgcwnbbZr8vgLYKvl7M6Ip4SmSJHJ1+YelFzltidw9VxLDH0cl/+cDcse5Kv0k/2srd18uDUkZ8Gg1tjmcSP9wYfI/H5WcZA4vX1YJ+53bxweI2nNT4EZiAMOfiQSFaZW1AZFhNXd7N+Ji5pCMP2vrE7PJCx9rDGyfcblbAycm5zRLfrcDtix8XnV+GsToI4skZUs8qt83F9zfGfjY3R9MqahNiHz9axL/lNzIhzOJvow0rEuMFikj2mchZgd39hKuvHxpyuXGxEgUPDreLwcut4LVyuoqT74NRCD+HzHUeHPiZPQZEezG1NLuVVvuuJL/jyWf49yM+eFEhtaiWKREeZy4mj7T3Z9N2vx7Ec2RqxW8j6Xu9w9J01RTYo7LzURamXOIgQxbuPsLpZRR0HTUGXg0qYl877Fo01zi8/v3UsqoSMF7dCgwzGK4+/1EX8kXntKci4rKTfoSDiEGB2xEdKi/7ZHOYna5/auyBhEUPGnHT4JDY2Iq+iKiyjUhxXLeTdr52gBvmtkDxEiAzzymoqfhOciny/7AInd6f+LLVqp9iMlRzYmFgfI85U66LCTv/SNEdfpqd3+eyGu/HnHS+29t7l+xCppdctloexEjdt4lWVTJlzYf7UxMWCpmu2sStajfEFfpW5jZue4+lfhcPWcpp3/waFO/g6i1vUvMuG8FrFtqQEi2n3sfjiUGQ/yPWFSmOVE7eRqqNzRzRQr+P9sQnf7XETX4nxNt/DPM7A5Pf25Crl/p0KS8g4lFiv4EzDazx7zEWeFQz/sULBYNeYB402509znlHv+YyCVT7ahe8AFZhxj//g0xwWgJMQLhcUqclFVuVFNroJXHcLMfEx+MSURnc8knPYvx3L8iOuEfJvLqP5BVZ1lakpPZ/sTVbh8iMF9IpH1OrZ28JpnZbcQFwGJiqOOLxGCAL5PHBxBNF0OK3F7us3oWMVBhPtHf9S1x8rzXq5g8rchyGxH5jr5Lbl9ALD5zTgrbzh1TK2Li3aHECKS1iDlCV7j7K2l+fgv6D39GXJT/Nbl/GyLP0erufl4aZZUrN3esVwKPufu45P41iTkrzd39qlKPtV4HBchPKDuU6FR7H7je3W+xGMY5zt23LXH7uU6u44GdiCuhLYnO36YA7v7bEsvIfQjPJzqwfkRMUrmDGEJa7URiBR+0lixdDe4Z4mTUj8hFP9jdHy/lGGqSRTK3/Yl5FhsStbVr3P2J2tyvqrBIyfJX4sJiPtFseB5RY7grec4exKitopqPkg7pNYl+tWPd/Y3kRJZbdvN2L8hJlMIx/IhI+3AE0Xz0T6JPYWMihUZJtc+C714r4hg2INZHWULUeg8ggsNTXsU1M4os/xniWM7I/U+S+3PfqSw6tlsS7+PWRO3rn15uVTUFhSowsy2AU4g25/WIyTq/qfxVRW/7MiJb5aTk9vokTRbu/nSlLy5u+42JkUBHE6ksehMn7W2JFb+eq+52k4DzG+LE05YYmjiNqO7fVt1t1wUWQ4QPJ1KYpN62nBUz2w8Y4svm59oPOMLd96nG9nLrefyZuHh5jOhzudfd55nZk8AJ7j49hX3PnawvI0bk/JYYnfNLYo3yYaWWUa68scRFUmNi1NwD7n5u8lhvYi2F6maRrazcbsS5ZFcidfVzRM30+bTLKiizCREAtyNqj5sQzaJj3f0fqZTRkIJCTnKCLSNGK5Scp8ZidvFLxIfyYlJssig4aXcHDnT38ws6hFsRH4pXS20vNbNHic6rs4imhNeJUVNXuPu1JR2EVFnS+TuBaNq5ghhSO5LoUPxDweCJqs1WjSC5KzE89zCiI/5jYiGdXVM8BJJAc3bhRYWZ3QNc6yVmDy0IPEOIFNx7JlfnnYjRhfdlFAgKJ12uTizk9DXRxn8YMeLq+LTLLSg/t6xsUyK7cHvi//mJu//dUpjP0iA6mstLTqAld3IVbG+2RXKyfYj0FieYWSpNFgUn+7OBXcxsEZEWYAHxYSw535FFWo73iWn5WwGXecyQnEoVRrZIaWzZnE1tieaPk4gmycZEs0Eu42aVAwKAu79FzEvAzMYQFxXbEQv3pO0WYG8ze80jIWQL4iq3+EXkV6DgxLcusfZ5rgP+AzObTnwXx6fZwZxsPxcQ/o9oxm1K1ESnuPsLuY76NE7OOeU+F2cRLRAfuvsQM/uSOAdYsn8ll9kgawpZS6vJolx7/77EFcL+xNXBOGIBnDQzfTYiJvftQbTD93D37dPavlTOzDYmRpVdSrSD/7ngsQ09xRXQslJwAjuAuGg5jmjrn0I0sSx092PSKouYrXw30dR5NTEH4i7gVncfnWZQKDi244iawUSiJrRjUpPv7e43pVHWCsodSST4W0Q0LR6W9BOt6+7XpFVeg6wpZC25Gkujr6IR0Wl2MrC2u//azO4kOpgOJPIdpRYUkg/eNcSwujWIbI9Sc7Ym5h3sAjxskSjwC4+FgY4xs/u8judsSj5DjYn8Un2IuQM7Ec21TxAXGyUpuAofSlwhDyQS7t1ADEm9jwgMpBUQkm3lrsK3ItLv70zU3iBmFm8J3JR2B3NBua2JlRwvYWmNcRBJNuO0aieqKawCLNYfGEFcIVxGXBXNITKiflOb+ybpSk6guZPd+kQK5M+A84ENPL2lSlNXULPtQXQun0X0gaSaHbSgnEuB571gOVQza1vqqKYiyt+HSD45ABjk7m8l/ScXuftjaTYdJeXljncIkfCuNzFjuxWR72iAu3+aVjBSTaGOs6XrDxxCjDa6gbg6eYGYR6CgUA8UnEg2Bv7h7pMskrvtQUwmPNzdv0+7jTwjA4mr2jOBe8zsQ6IjNJXFh5IT5PpE9uHvzezBXLDMOiAkZYy1GNLeATjCzHYmUrI/ljyeauLCghP9v4nRhv8jagudgDuSgJBeH4ZqCnWfZbz+gNQNyYij84nUBfOAJ4mlJFeZwJ+MjrmCaL7pA2xKXLjMBG7xchNISyinNTHnYWtikto44GEvYVXDIss9nZg/Mo9ovm1HdNzPdPf/ZTQ3oRnRX3KCuy9OLhY2I0Y8fusprzmtmsIqwN1fN7M3CkY+9CKW/JR6IhlmvJA44bQnhor2J/oSnvdk3H1dVXClegwxN+dfwL+SIHEo0DWtgJDYleijWEicnLcBrjOzX7r7LSmWU9jRuy1wmLtflozY60OksJmWe26aAaGgVngQsE4SEH5MJHmc6AVrs6RZroLCKqJgyN1CM/sb0b8g9UTBl/oDIqfVNGKY6OksXXKzzq4WV7BfOxFJ6LBYG/trM1tIzHspSUHbehnwB2Lext+ISWNrETP8vy58bqlllnMUcJvFjPmTiYuzOWbWxJNUF2kqaCYcDlxikS57OPG52MPMZhf2p6SlUdoblOy5+4JVoF1ZqsDMOpvZN2Z2sZmt7+7/85gd3w54I3naqtDW+xBwvpn1ZOn+/oJkreQS5c5XvYm5NH8BngfGEjWF/TxZRyCj0T+TiSHDfyWWK+1PLIu6GuSHyGZhHHAC8EciTfrZxPyIb5JyrZLXVplqCiJ1gLu/bWZDiXH9b5vZW8RggqaeLGyfwZVvFm4jmr8OAE61SBQ5zd1LzuJbcCF0NDDXzJ5093eSv78kJvilmhG1nPHEGgbjiaGnaxNNfLvndjHNwgpqO3cQzWSfufsEM+sLrF/QsZ1uuavG50ykfjOz4UQq95eAz4lx/QuInDazVpFRRwAkM5e3J4ZMfg+84Clk8E223YTIGnAMMTv6M2KGdFN3PziNMsqVl2uyakH083zl7rOSx3YkhqSel1XTnsU6DZsTtYJ3ks7sPkQfzc1ZfC4UFERqScEJZzDRxJJbCe05lk7C+ruXsK53fWZLs+EeQGR/fQu4wVPOhpsEhIlEKpgfEVlrJxIJML9InpNmau7c52JrYhTXU0TT2WrEMY73WAMjEwoKIrWkYFTL34hUDesSQzh/R+ScetndT6nNfVwVJG3qnYkU3dM9pWy4BSfnnxLzhIYRQWF7lq7D3D+DIai5ck8m+jD+TGR/7UTkqTJ3PyvNMgupT0GkliQBwYC5wKtER+L9yQizacRkpTo96qguSE7KbxFJI9NkRD/BOkQ67gXA+8lkvIeIxa487f9PQZDZGvhX0jz0ipm9RnR2L4LMRlippiBSV5jZQcRQx6lEGoNunt4yrlINySS56cTqd9cQE/AynzVtZh2I5sQOwL1ECvvM1mkopCGpInXHP4imo8nAKUmNQd/RWpJ04n5DzDA/klgoaJKZPZjkIcqq3EbuPsvdOyZlfwTcZWZzLVZ4zJRqCiIiK2BmmxH5huYmw0HbEMkp/+fu12TRtJdcCOxOrHXxjS9dfnUX4Ht3fy7L0WgKCiIiBQoGAPQg1raYStQSdiRS2GfSfGRLV1kcTiwSBNDM3fczs82J/EqZZzJQ1VREpGLHE6vHvQm8kVyZ72BmF2VUXq7GMYJY7OodYsY2wH7AqRmVuwwFBRGRAgXNQfOBSUQq8FHJffsTayOnntYiGcnUjFhIZ2NiAZ3cimoDSZbezbqfSUFBRKRiY4i8QzsDi5L+he2J2gNkkIsqGW12PzE3AaCvmR1ONFs9mjwn0+HJ6lMQEVmBJKXE/sTkscbA9e7+j6znjiSdygOI1fcWEjO1p9REuhMFBRGRSlis8vYVkV9pbg2Wuw6xNkWNpjlRUBARqSKrgbWgKyizCbCGZ7wSn/oURESKkOvgTVY/OzHF7Va6HoKZNU7+PJ5I/pcpBQURkeLkTt4HAxNS2aBZ08L8RSsIELm+i4FEJ3SmFBREpMErqAW0T9ryl1PQwdubZInUFIwxs/vNbJekE3mZAJF0aLuZdQQa1USTlYKCiDR4BSOJLgP+bWYnmVlXM8sttZlb1W1/4MEURx4dT2TIvQ341MxGJWtQ57Kl5oLECJbOlciUgoKINGgFtYSuwOrEetK7ESfqcWY2sKCWsENyfxrlNnH3T4CHiUyoFxBNVBPMbIaZHVdQc3BiYZ/MafSRiDRoBbmO/gJ84O5XJvd3JWYUrw/MAoamOSS1YDGdR4Gr3f3+5P42wGjgNXc/I63yiqWagog0aAVNQQuAjmbWxsyau/sbwAvAr4HXiZnNaZbrSbPUy0DXZMgp7j4H+AK4HZYZfVQjVFMQkQYvGfWzHrH63bvAK8AWxNrZWxFNPGe4e1odzIVlbwjcAMwGHge6AAPcfZu0yyqGagoi0mAVDAE9hFgj+wxiXeQDgTWBXxHrZrfJKCCsBixJynsZ6Au8QSzqU+O1BNAazSLSgCVNOAZ0BH4L/N7dz8mdrN19sZm1BY5Nq8yCPowuwM+AocDr7r5XBfuXaZ6jCvdPzUciImBmWxFDP19y91RGGK2gnNxiOlcRzVQLgYHufriZHQo0cfdbsip/ZdR8JCINmpm1TnIZTSPSZZ9qZvclE8ZSV3D13x14kGg6ujO5b1dgjWS/auX8rKAgIg2Wma1LNBv9xcwmEeslPEssh9k2ozJz/Rgjiclra7v7v8zsR0BP4J7k8VppxlHzkYg0SMmVuBPrJUwlrtA7Ap8Ai919SoZldyYmyp0NDCZyGv0AfOruv856vYZK901BQUQaGjM7mxj105HIY7Q60Bx4G9gaGOfuU1MusynQnjj5/93dd07u3xTYHXgaeCvphDavpZOzmo9EpEExs57A74ihps8S8xBaEc1IRxAB4vMMim5PrKZ2A9DMzDqZ2Tru/p67Xw90ytUOaisggGoKItLAJDOHDwHOAka5+1/M7HhgJ3c/2Mxauvu8DMptTvQZXAN8CkwDviNGIO0FNHf3o2qzlgAKCiLSQCVDUE8EHgFOAi5y93/XwPrL7YG5QBmwI9AO+BFwubs/X5v9CaCgICINTG70TzJxbQciO+kAoqbwXEZl5uYmbAH0ADYGJrr7dDNbzd2/z6Lc6lBQEJEGz8yOBfoAf0nmK2RVzlTgSWAe0aG9gGg+Gunus7MqtyqU5kJEBG4FWhBJ8aal2a5vZuu5++dJuox33f1EM2tJzIPoCQwhRj7VCaopiIhkyMyuISbD5WoIv3f3mcljjYgO5tQ7tqtLQUFEJGNm1puYJLcnsAi4A7jH3T+q1R2rgIKCiEhGch3M5e4bABxDpNR4Hxhclzqa1acgIpIdh3xHdhkwFnjR3R9O+hX6uvv3tS86IuIAAARGSURBVD0MtZBqCiIiGUrWZniAmIvwCbHe86fA2CwW7imVagoiIhkouPo/DJjh7gPMbD1gbyIRXhczmwGcr+YjEZH6L5ciuxPwPYC7fw7cmKTa6AxsSCzB+e9a2cMKKCGeiEgGCjqY7wA2MrPjzewnyUikw4HLgW+JTK11hvoURERSZmbnAbcDHybpNHoAxwHNiElrrwN/IrK09q5L8xTUfCQikr5XiQ7l581sHnCBux9fmOfIzDYj1lWoMwEBVFMQEclM0rF8JDEvoRVwL3BLXRx1lKOgICKSotyoIzNrTKyy1ijJkNoNOAPo6O79zKyJuy+u3b1dnoKCiEgGzOz3xMijVsDP3f2Tco/XmQlrhTT6SEQkJbm1GsxsP2ATIvvqhu7+iZltYmZHJUnwqIsBARQURETSlDun/hi4CVgbeCi5bzdg36RpySp6cV2goCAikoJkvYQTzawVkeNob+APROcyxDrMo5O/6+y5t87umIjIKqYT0B+4GlgCvJz8DDKzJ4H/AffAMhPb6hx1NIuIpMTMNgBOA/oBLwEfAx8Scxamu/uXaa7qlgUFBRGREpnZmkBjd/86ud2T6EOYC0x09/dqc/+qQkFBRKREZnYmcCDwIvA0kc5iJ6LGsB5wvbufXms7WAVKcyEikg4DtiUyor4ITCTWZP4x0YRU4UpsdY1qCiIiKUiakIYCqwFPuPs7yf2rA4uTFdbqdH8CKCiIiJTEzLYEFrr7+2bWFDgB2JHIgHq3u39aqztYRQoKIiLVlKyz/BdiktomxGI5k4n+hP2At4kUF+poFhGp75KUFVsADmwE7EF0Mn8AdCUCw6bu/lWt7WQVKSiIiKTMzFq7+zcFt+t8X0KOgoKISAZygWBVCgigoCAiIgWU+0hERPIUFEREJE9BQURE8hQURCpgZm5mdxTcbmJms81sfBW3MzPJs1/Sc0RqioKCSMXmAt3NrEVyuz/wSSXPF6kXFBREVmwCsGfy93CWrpqFma1jZveb2VQzm2RmPZL725jZw2b2upn9lUiSlnvNoWb2opm9ambXm1njmjwYkWIoKIis2F3AMDNrDvQAXih47CLgFXfvAfwGuC25/wLgGXfvBtxHzHLFzLoABwE7uvs2xMpch9TIUYhUgVJni6yAu081s45ELWFCuYd3AvZPnvdYUkNYE+hLpDbA3R80s6+T5+8G9AJeStZsbwF8kfUxiFSVgoJI5cYBlxGLpbQpYTsG3OruZ6exUyJZUfORSOVuBi5y92nl7n+apPnHzPoBX7r7f4GngIOT+/cgsmcCPAocYGbrJo+tY2YbZ7/7IlWjmoJIJdx9FnBVBQ9dCNxsZlOJ1bWOSO6/CBhtZq8DzwEfJdt5w8zOBR5OMmsuIvLuf5jtEYhUjXIfiYhInpqPREQkT0FBRETyFBRERCRPQUFERPIUFEREJE9BQURE8hQUREQkT0FBRETy/h9N23n6skCokgAAAABJRU5ErkJggg==\n", "text/plain": "<Figure size 432x288 with 1 Axes>"}, "metadata": {"needs_background": "light"}, "output_type": "display_data"}], "source": "import numpy as np\navg_,med_,e80_,e90_,name=[],[],[],[],[]\n#a=[MAPE_clus,MAPE_agg,WAPE_clus,WAPE_agg]\nfor k,v in Smape.items():\n    avg_.append(np.mean(v))\n    med_.append(np.percentile(v,50))\n    e80_.append(np.percentile(v,80))\n    e90_.append(np.percentile(v,90))\n    name.append(k)\n\n\n\nx_axis = np.arange(len(Smape))\n\n# Multi bar Chart\nplt.bar(x_axis -0.15, avg_, width=bar_w, label = 'Avg',color='red')\nplt.bar(x_axis -0.05, med_, width=bar_w, label = 'Median',color='green')\nplt.bar(x_axis +0.05, e80_, width=bar_w, label = '80-%ile',color='cyan')\nplt.bar(x_axis +0.15, e90_, width=bar_w, label = '90-%tile',color='black')\n\nplt.xticks(x_axis, Smape.keys(),rotation = 70)\nplt.legend(bbox_to_anchor=(0.41,0.65))\nplt.ylabel('Weighted SMAPE')\nplt.xlabel('Model')\nplt.show()  "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"image/png": "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\n", "text/plain": "<Figure size 432x288 with 1 Axes>"}, "metadata": {"needs_background": "light"}, "output_type": "display_data"}], "source": "import numpy as np\navg_,med_,e80_,e90_,name=[],[],[],[],[]\n#a=[MAPE_clus,MAPE_agg,WAPE_clus,WAPE_agg]\nfor k,v in Wape.items():\n    avg_.append(np.mean(v))\n    med_.append(np.percentile(v,50))\n    e80_.append(np.percentile(v,80))\n    e90_.append(np.percentile(v,90))\n    name.append(k)\n\n\n\nx_axis = np.arange(len(Wape))\n\n# Multi bar Chart\nplt.bar(x_axis -0.15, avg_, width=bar_w, label = 'Avg',color='red')\nplt.bar(x_axis -0.05, med_, width=bar_w, label = 'Median',color='green')\nplt.bar(x_axis +0.05, e80_, width=bar_w, label = '80-%ile',color='cyan')\nplt.bar(x_axis +0.15, e90_, width=bar_w, label = '90-%tile',color='black')\n\nplt.xticks(x_axis, Wape.keys(),rotation = 70)\nplt.legend(bbox_to_anchor=(0.41,0.65))\nplt.ylabel('WAPE')\nplt.xlabel('Model')\nplt.show()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"image/png": "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\n", "text/plain": "<Figure size 432x288 with 1 Axes>"}, "metadata": {"needs_background": "light"}, "output_type": "display_data"}], "source": "import numpy as np\navg_,med_,e80_,e90_,name=[],[],[],[],[]\n#a=[MAPE_clus,MAPE_agg,WAPE_clus,WAPE_agg]\nfor k,v in AggPes.items():\n    avg_.append(np.mean(v))\n    med_.append(np.percentile(v,50))\n    e80_.append(np.percentile(v,80))\n    e90_.append(np.percentile(v,90))\n    name.append(k)\n\n\n\nx_axis = np.arange(len(Wape))\n\n# Multi bar Chart\nplt.bar(x_axis -0.15, avg_, width=bar_w, label = 'Avg',color='red')\nplt.bar(x_axis -0.05, med_, width=bar_w, label = 'Median',color='green')\nplt.bar(x_axis +0.05, e80_, width=bar_w, label = '80-%ile',color='cyan')\nplt.bar(x_axis +0.15, e90_, width=bar_w, label = '90-%tile',color='black')\n\nplt.xticks(x_axis, Wape.keys(),rotation = 70)\nplt.legend(bbox_to_anchor=(0.41,0.65))\nplt.ylabel('Agg Weighted PE')\nplt.xlabel('Model')\nplt.show()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"image/png": "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\n", "text/plain": "<Figure size 432x288 with 1 Axes>"}, "metadata": {"needs_background": "light"}, "output_type": "display_data"}], "source": "import numpy as np\navg_,med_,e80_,e90_,name=[],[],[],[],[]\n#a=[MAPE_clus,MAPE_agg,WAPE_clus,WAPE_agg]\nfor k,v in AggSmapes.items():\n    avg_.append(np.mean(v))\n    med_.append(np.percentile(v,50))\n    e80_.append(np.percentile(v,80))\n    e90_.append(np.percentile(v,90))\n    name.append(k)\n\n\n\nx_axis = np.arange(len(Wape))\n\n# Multi bar Chart\nplt.bar(x_axis -0.15, avg_, width=bar_w, label = 'Avg',color='red')\nplt.bar(x_axis -0.05, med_, width=bar_w, label = 'Median',color='green')\nplt.bar(x_axis +0.05, e80_, width=bar_w, label = '80-%ile',color='cyan')\nplt.bar(x_axis +0.15, e90_, width=bar_w, label = '90-%tile',color='black')\n\nplt.xticks(x_axis, Wape.keys(),rotation = 70)\nplt.legend(bbox_to_anchor=(0.41,0.65))\nplt.ylabel('Agg Weighted SMAPE')\nplt.xlabel('Model')\nplt.show()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"image/png": "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\n", "text/plain": "<Figure size 432x288 with 1 Axes>"}, "metadata": {"needs_background": "light"}, "output_type": "display_data"}], "source": "import numpy as np\navg_,med_,e80_,e90_,name=[],[],[],[],[]\n#a=[MAPE_clus,MAPE_agg,WAPE_clus,WAPE_agg]\nfor k,v in AggWapes.items():\n    avg_.append(np.mean(v))\n    med_.append(np.percentile(v,50))\n    e80_.append(np.percentile(v,80))\n    e90_.append(np.percentile(v,90))\n    name.append(k)\n\n\n\nx_axis = np.arange(len(Wape))\n\n# Multi bar Chart\nplt.bar(x_axis -0.15, avg_, width=bar_w, label = 'Avg',color='red')\nplt.bar(x_axis -0.05, med_, width=bar_w, label = 'Median',color='green')\nplt.bar(x_axis +0.05, e80_, width=bar_w, label = '80-%ile',color='cyan')\nplt.bar(x_axis +0.15, e90_, width=bar_w, label = '90-%tile',color='black')\n\nplt.xticks(x_axis, Wape.keys(),rotation = 70)\nplt.legend(bbox_to_anchor=(0.41,0.65))\nplt.ylabel('Agg WAPE')\nplt.xlabel('Model')\nplt.show()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "0.684756641439167 0.6916307104292704\n"}, {"data": {"text/plain": "[<matplotlib.lines.Line2D at 0x7f704c10f390>]"}, "execution_count": 36, "metadata": {}, "output_type": "execute_result"}, {"data": {"image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi40LCBodHRwOi8vbWF0cGxvdGxpYi5vcmcv7US4rQAAIABJREFUeJztnXmUZHd137+/V3t119LV+zrdM9OzgjQzkkYSWkaALAmMEU68iMQmdohln4DBCcbGjmPH2OSYOMc4jnEMDpuxgWBjkFjMZoRWtIyWEZJmn+mZ6enu6b32vX75473fq6VfVb1X+6u+n3M4TFeXql91vb7vvu+993sZ5xwEQRBEdyG1+wAIgiCIxkPBnSAIoguh4E4QBNGFUHAnCILoQii4EwRBdCEU3AmCILoQCu4EQRBdCAV3giCILoSCO0EQRBdibdcPHhgY4NPT0+368QRBEKbk+eefX+WcD1Z7XtuC+/T0NI4fP96uH08QBGFKGGOX9DyPZBmCIIguhII7QRBEF0LBnSAIoguh4E4QBNGFUHAnCILoQii4EwRBdCEU3AmCILoQCu4mh3OOr744j0gy0+5DIQiig6DgbnIur8fwn/7fCXz7laV2HwpBEB0EBXeTsxlLAwBiKcrcCYLIQ8Hd5IQTclBPpLNtPhKCIDoJCu4mJ5KUM/dEOtfmIyEIopOg4G5yQpS5EwShAQV3kxNRgztl7gRB5KHgbnJUzT1DmTtBEHkouJuccEJo7hTcCYLIQ8Hd5IjhpSTJMgRBFEDB3eRQKyRBEFpQcDc5ISHLkOZOEEQBFNxNjpBlqFuGIIhCKLibHJJlCILQgoK7yRHdMnEK7gRBFEDB3eSIISbqliEIohAK7iYmm+OIpuSMnWQZgiAKoeBuYkQxVWIU3AmCKIaCu4kRent/rwOJDMkyBEHkoeBuYkSnzGCvA9kcRzpLAZ4gCBkK7iZGyDIDHgcAkmYIgshDwd3ECFlmsFcEd8rcCYKQoeBuYlRZhjJ3giBKoOBuYkRwH+i1AwCS5C9DEIQCBXcTszVzJ1mGIAgZCu4mJpxIwyIx9LnlzJ1kGYIgBBTcTUwkmYHHaYXTZgFAmTtBEHkouJuYcCKDXocVTpv8MZJ5GEEQgqrBnTH2acbYMmPslTLfZ4yxv2CMnWOMvcwYO9L4wyS0CCfS8DhtBZk7BXeCIGT0ZO6fBXBfhe+/BcCs8r8HAfyf+g+L0EM4ocgyVgruBEEUUzW4c84fA7Be4Sn3A/hbLvM0AD9jbLRRB0iUJ5zIwFMgy5C/DEEQgkZo7uMArhR8Pa88RjQZtaBqlzP3JGXuBEEotLSgyhh7kDF2nDF2fGVlpZU/uitRNXeSZQiCKKERwf0qgMmCryeUx7bAOf8k5/xGzvmNg4ODDfjR2xfOudwt47TCZmGKpzvJMgRByDQiuD8M4F1K18wtAIKc88UGvC5RgWQmh0yOw+O0gjEGp81CmTtBECrWak9gjH0RwF0ABhhj8wD+AIANADjnfw3gWwDeCuAcgBiAX27WwRJ5QoojpMchf4ROmwUJ8pYhCEKhanDnnL+zyvc5gPc07IgIXQhfGY/TBgBwWiWSZQiCUKEJVZMSUYN7QeZOsgxBEAoU3E2KyNx7FVnGYbNQ5k4QhAoFd5MitjCpsoxNIj93guhwgvE07v/LJ/DtV5aa/rMouJuUcLJElrFaEE9RcCeITiYUT+PEfFBNzpoJBXeTEt6iuUvULUMQHU4wLgd1r8vW9J9Fwd2kREo0dydp7gTR8ZQmZc2EgrsG55YjePRMZ9sjhBNpuO0WWC3yR0jdMgTR+Qg5xuukzL0tfOLR8/j1L7wAuYW/MxGLOgSUuRNE5xNSMncK7m0ikswglMhgM1Z/0WMzlkI62/igKxwhBU6bRK6QBNHh5LvcSJZpCzGl6+TSeqyu1+Gc4+4/exSfeuJiIw6riJDiCCkg+wGC6HxCcdLc20osJX8Al9aidb3OZiyN1UgKpxZDjTisIsQWJoHTakE6y5HNda6URBDbndJaWTOh4K5BNKlk7mv1Ze7L4SQA4OpmvO5jKkVLlgHI050gOhn5jrv5WTtAwV2TeLpRwT0BAFjYTNR9TKWEE2l4HMWyDEDBnSA6mXAi05JiKkDBXZNosjGyzHJIztyXQglkGlxUFYs6BLRHlSA6n1I5tZlQcNcg3qCCqpBlsjmOa8q/G0E2xxFLZUtkGcrcCaLTCSXSLZlOBSi4b4FzjmgqA7tFwko4qWbxtbBSENCvbjROdy+dTgUAB+1RJYiOR87cKbi3hWQmhxwHZod7AQCX68jel8MJ2CwMALDQwKJqOLl1yo0Kqp3Bj86v4Ufn19p9GESHEoqn4SVZpj2IHvf9o14A9enuy+Gk+jqN7JjR8qfIyzKkubeLZCaL937hBfzWV0509HQz0R7EUnvK3NuEkGHywb32zH0lnMRUwI0+t60pwb2XNPeO4hsnFrEWTeHKehxzdXZaEd1HMpNDKpujgmq7EG2QI14nAj32uv5Il0MJDHmcGO9zNVSWiSSLF3UAgIsy97bCOcdnn5rDkMcBAHj09HKbj4joNMRSeyqotgmRubvtFkwF3Li8XpssE01mEE1lMeR1YMznamhBVVuWIc29nTx/aQM/vhrE+948i+l+d8e7ihKtJ6yahlHm3haE5u62WzDd78bcam2Zu2iDHOx1qJl7o3RY4SzncWjIMuQv0xY+89QcvE4r/tWRcRzbM4inL6zThZYoIhRvnd0vQMF9C/ngbsVUfw8Wg3GkahgMEm2QQ14Hxv0uRFNZdQtLvUTUzL2gW8ZKsky7WAzG8e1XlvDA0Sm47VYc2zuIeDqL43Mb7T40ooNo5aIOgIL7FoRpmNshZ+45DsxvGM/ehfXAkMeJcb8LQOM6ZsKJNKwSU6UYAHCQLNM2/u7pS+Cc4xdv2QEAuGVnP+wWCY+eId2dyEOae5splGV29LsB1NYxI6wHhjwOjIng3iDdXVgPMMbUxxxWCYyBPN1r4PJaDP/1a6/UdGFMpLP4wjOXcff+YUwG5PPFbbfi6EyAdHeiCMrc20y+oGrFVKAHQG297svhJOwWCX63DeN9cnBvVMdMqSMkADDG4LBK5C1TA4+eXcHnn76EH5wynmk/fGIBG7E0fum26aLHj+0ZxJlrkYZ2SRHmRmju1OfeJgoz94FeO3rslpraIZfDCQx6HGCMob/HDodVaqgsU+gIKaA9qrUhtuM89NJVQ/8d5xyfeXIOe4c9uHVnf9H3ju0dBAA8Rtk7oRBOZCAxoMduacnPo+BeQiyVhd0iwWaRwBjDVH9PTRYEK+EkBpSeZ8YYxv2uhln/hkocIQVOKwX3WhC3y4+cWjFU9H724jpOLobwS7dNF0lkADA71ItRn5OkGUIlrGxPKz1XmgUF9xJiqQzcjvyVdbrfjbkaZJmVcFIdaAGAMb8L842SZRIZzV5Zp02ibpkaEN1HqWwO33llSfd/99mn5uB32/COQ+NbvscYw7E9g3ji3GrD7Z4JcxJKZOB1tUZvByi4byGWysJtywf3qX435tfjhtfXLZcEdzlzb5Ask0wXOUIKnDaLOmFL6CecSGMq4MaOfjceOqFPmrm6Gcd3Xl3CAzdNwVXmNvvOPYMIJzJ46cpmIw+XMCnl5NRmQcG9BDlzzwfO6f4epLI5LAb1B+ZUJof1aApDHqf62JjfhZVwEskGDBlFypgPOUhzrwmxQOH+68fw1Pk1LIeqy2ef/9ElAMAv3rqj7HNu2z0Ai8RImiEAyMuxKXNvI7FUtqjgsUNpb7tsoKi6GskPMAlEx8xinbp73llOS3OXkCRZxjBhpfvo7YfGwDnw9ZcXKz4/GEvji89exj0HRtQZBi18LhsOT/opuDeIr744j2/9uPJn08mEFM29VVBwLyGWzBbdZu8YkNshjXTMCOuBYs1dzuLrlWYS6RwyOa5ZUHXZLWQ/UAPhRAa9Dht2D3lwcMyLh6t0zfzlI2cRSqTxvjfPVn3tY3sG8fJ8UL3gdxLBWBr/7eFX61pI00o+8egFfOx7Z9p9GDXTyhV7AAX3LURTGfTY8x/AiNcJu0XCJQMGYuK2frBEcwdQd1E1rOEIKaBumdoIJ/ILFO4/NIYT80FcXNX+vC+tRfHZp+bwszdM4MCYt+pri5bIJ86uNu6AG8TDJ67is0/N4clznXdsWkSSGZxdjjTMxqPVhBLplvnKABTctxBPFWfuFolhMuDCJQMGYvnMPa+5j/icYKz+zL2Ssxx1y9RGYUb1U9ePgTHg4ZcWNJ/7J/98CjaLhA/cs1fXa79uzIdAj70jpZnHlAvOhTIXsk5DnPsvXjafZ08uxxFJane5NQsK7iWUZu4AsKO/x9Cy7JVwEowBA7129TGH1YLBXkfdFgRhjf2pAhpiMg7n8h+dkLlGfS7cPBPAQyeubnHxfPbiOv75lSX82rFdGPY6tV5uC5LEcOfsAB47s4KcwY6rZpLO5tR1gBdWIm0+muqIzwkAXrhkvuAeSWXAeet8ZQAK7luIpbJFfe4AsKPfjUtrUd2WvcvhJPp77LBain+9430uLBjoutFCyxFSQMHdOPF0FtkcL/p93n9oHBdWonh1IaQ+lstx/PE3X8OI14lfuWOnoZ9xbO8g1qKpotdrNy9e3kQkmYHNwnB+pfMzd/E5AcALl83XWtpqXxmAgnsRnHM5uJf0Le8IuBFLZbEaSel6nZVwAoOerZndmL/+pR1iVF7rJHHYto+3DOcc//Vrr+CD/3ACn37iIp6+sFaTFqv1R/eW143AZmFFdgQPnbiKl+eD+K379pbtay/HHbOKFcHZzpFmHjuzAovEcM/BEVNk7uJzctksePHyhuG5k3bTal8ZgIJ7EclMDtkch7tUllE6ZvRuZSodYBJM+F1YCCbquj2vKMtYLUhlch11+98s1qMpfP7pS3joxAI+/I3X8MAnn8b1f/hd3P7RH+DBvz2uu4Cp9fv0u+04tmcIXz+xiFyOI57K4n98+zReP+7TnEatxkCvA68f9+HR050T3B8/u4LDk34cmvBjI5bGRlRf4tIuxOd0665+RFNZnF4Kt/mIjJGvlXVYcGeM3ccYO80YO8cY+5DG96cYY48wxl5kjL3MGHtr4w+1+cQLTMMKEb3uercyLYe0g/uY34VUJofVaO1tceFk+ZNEbGNK1pi9f/n4Fdz+0R8gbYJx+cWg3JH0Fw8cwrO/+2Z89pdvwm/dtxeHJv146vwaPvn4BV2vI+6ESn+f9x8aw1IogWfn1vGpJy5gMZjA7/3kfkhSbb4gt+0ewAuXNxoyxFYv69EUXr4axB2zg9g5KCcuF1Y7O3sXn9OxPfJd0AsmK6pWuuNuFlWDO2PMAuDjAN4C4ACAdzLGDpQ87fcAfJlzfhjAAwD+qtEH2gqiyqKO0oLqRJ8bEoOuomoux7EaSRa1QQpEO2Q9BmLiJNE0DqtzYcdLVzYxvxE3RVYkgvuoz4UhrxN37R3Cf7xrN/7y3xzB4Sm/bommnBZ69/5huO0WfOqJi/irH57HfQdHcHOJ86MRDo55kclxnF9uv7795LlVcA7cuWcAuwZ7AaDjdXfxOe0f9WLQ4zBdUbXVizoAfZn7UQDnOOcXOOcpAF8CcH/JczgA0fTrA6DdR9bhqJl7SUHVbpUw5nfp8nXfiKWQyfGymTtQXztkJJGB226BRSODrHeP6pISMM3QaibsIEZ9W2sbPpdN1TirITowSi+WLrsF9x4cwfdeu4Z0NocPvWVfXce7b8QDADh9rf1F1cfOrMDnsuG6CT8m+lywWRgudHhwF5+Tx2nFkSk/njfBOVpIpxZUxwFcKfh6XnmskP8G4BcYY/MAvgXg17VeiDH2IGPsOGPs+MpK5+iPgmgZWQaQPWb0bGRSe9w1WuWEBUE9RdVKU24icxcXKaOIi86LJjC6WthMwGZhGOjdehH1uWwGMvfyha63HxoDALzr1mlMK3WXWpke6IHdIuFUm++KOOd4/Owqbld8b6wWCTv6e3C+w4uqhbLGDTv6cGkt1pFTv+XIF1Q7K7jr4Z0APss5nwDwVgCfZ4xteW3O+Sc55zdyzm8cHBxs0I9uHLGCLUylTCntkNXQsh4QeJ1W9DqsdS3tCCfL+1PUuyR7SZmsNYOL4WIwjmGvU1MDF8FdT+tqpYzq2Owg/tcDh/CBe/bUfbw2i4TdQ704tdje4H52OYKlUAJ3zA6oj+0c6Km7Y+bxsyv4zJMX6z28soQLWoCPTPUBMFe/eziRgcMqwWFtzaIOQF9wvwpgsuDrCeWxQt4N4MsAwDn/EQAngAGYjFiFzH1HwI2NWLpqRiisB4Y0WiHF0o66gnsio9kpA9Qny8RTWWzG5DH8CytRBGOdPeK9uJnAmE/btMvnsiGb4+qdWCVE0CitswDyANL9h8Y1L/a1sG/E0/Z6htgMdceefHK1c7AXl9djdfnOf+GZy/jIN0827bwp7Gp63bgPNgszVb97q03DAH3B/TkAs4yxGcaYHXLB9OGS51wG8GYAYIzthxzcO093qYIoqGr9Me/oV9ohq0gzInPXKqgCsoFYPZp7ZVlGZO7Gg7vQsH/iwAgA4MR8Z//hLATjGPVrT4mKopUeaUZcLLVqGI1m36gHS6EENmPtazt87Owqdg32FLlZ7hrsQTrLcaUOuXA9KteafnD6WiMOcwvhRAY9Sq3JabPgdeM+U2XurV7UAegI7pzzDID3AvgOgJOQu2JeZYx9mDH2duVpHwDwK4yxEwC+COCXuN5xzg5CaNU9Do3MvV9uh6xmILYSTsLjsJYddBnvqzdzT1fV3Gux/RXF1HsPDoMxeYKxU8nlOK6FEhitkLkD0JVFVvp9Npq9I3LPQbt090Q6i2curOHOPcWS6E6lY6YeaWZDuWB955XmBPdIMl1U9D4y1YcT85tImWRoL1xmB0Mz0aW5c86/xTnfwznfxTn/iPLY73POH1b+/Rrn/DbO+fWc80Oc8+8286CbhVpQtWll7kpwr5K5r4STGPRqZ+2A3DGzGUvXbLMaSWbKbnOpJ3NfUIL7nmEPdg/24qUrnZsVrUaTSGe5aqNcigjuov2sEpFkeZmr0agdM00M7pVyqufm1pHM5HDnbHFw3yV63evomFmPyr/rR8+sNMUCozQ43rCjD8lMDicX2999pIdQPN1S0zCAJlSLEAVVrazbbbdi0OOoWlRdDic0i6mC8TrbIXXJMjVo7kuKLDPic+LQpB8vXdnU7aXTasTCk5Ey5l0+g7JMqzL3IY8DfW4bTi01JyD90Tdew0//1VNqZ0kpj59dhd0i4eadgaLH/W47Aj32mjtmcjmOjVgK1034EE9n8XgT7I0jyeLPSRRVnzeJNBNusd0vQMG9iFg6C5uFwW7V/rXIy7Kra+5axVSBCO61SDOZbA6xVFZzgAkoHGIyfqu6EEwg0GOH02bB4ak+bMTSuGzACVMvj59dwXde1b+EWgtRHxgrswXJWHBvXaGLMYa9I56myTLHL23gpSub+NXPP685CfvYmRXcON2nWVOSO2Zqy9zDiQyyOY63vn4UHqcV363z89UiVNJIMOJzYtzvMk2/e0dq7tuJWDJTsTNiKtBTsaDKOS9rPSBQe91rCO7RpPwHW70VspbMPaFmwocm/QAa2xKZyebwJ/98Cr/4qWfx/i+9qA6l1IKY8NUaYALyBVU9g0zhArvfVrBvxIvTS+Gm+P8sbsYxFXDjqfNr+MCXTxT9jOVQAqeWwqqJWSk7B3tqtiBYV/T2Ya8Db943hO+fvFZX540WEY3M98iOPrxoosy9IzX37ULp/tRSpvvdWAolyg4JRZIZxNPZot2ppQx5nLBKrCZZJlTFnyKvuRv/w1oMJtRguWe4V3Hfa0xwX40k8a5PP4u/fvQ8ju0ZRCKdqyu7WwzG4bBKCPTYNb/vcVjBmH5ZppVa6L4RD2KpLObrdActJZXJYSWSxE8fHsfvvGUfvvHyIv7om6+p0ppYzHHnHu0O5V2DvViNpGpy1lxXTMf63Hbcc3AEG7E0nptrbNDVagE+MuXHQjBR9wKcZpPK5JBI5+BpUW1HQMG9gFjJFqZS9o/K3Q7lxvOrtUEC8manEZ+zpilVdZCjzEnisNbuLbNY0FpotUh4/YSvIZOqz1/awNv+4gk8f2kDf/oz1+Gzv3wTJvpc+OqLlfeUVmJBuRAxpt2+KEkMXqe+KdVWZ1R7laLqyQbr7tdCCXAut9o+eOdO/PvbZvCZJ+fwicdkA7XHz65goNeO/SPaqwHr6ZgRjpKBHjuO7RmE3Srhu681VprRqo3csEMZZupwaSbcBl8ZgIJ7EdFUBj0Vrq637uqH3SLhkdPLmt9fDm1dr6fFmN9Vk3lY3l9D+ySRJLleYLSgKgaYClsLD0/6cXIhVLOLIeccn3tqDg988kewWyX80398A372xkkwxvCOQ+N48twqlsO1GagtbsbLtkEK9FgQpLNyRtWqbhlA7kZirPEdM4VGaowx/N5P7sdPXT+GP/nnU/iH41fw+NlV3DE7WNbVcmcdHTOFmXuPw4o7dg/gu69ea1hBPpPNIZ7eWmvaP+qF0ybhhUud27YLtMdXBqDgXkQslYXLVj5z73FYcfPOAB4p48u9EilvPVDIRI1TqnpsQ51WyXCfu5YJ1+EpP1LZHF6rcXvQh7/xGv7g4Vdx5+wgvv7e23FwzKd+7x2Hx5DjwNdPLNb02kvBRNkBJoHXZa0a3CNt+KPrcVgxFXA3IbiLIrP8e5Ekhv/5s9fhtt39+OA/voz1aKrIcqCUqYAbVonV1DEjNHchk917cARXN+MN2zxVLqmxWSRcN9H5JmKhMrbSzYaCewGxKpk7ANy1dwjnliO4otFJUsl6oJAxvwtLoYTholM5B8NCnDaLYeMwMcA0UhDcD03Kt7y16O65HMeXnr2Cn7xuFH/zrhvhcxef1LuHPHjduLdo05FesjmOa+Fk2WKqQE/mXuhX0kr2DnsaLsvki8z5OxqH1YK//oUbcGDUC4kBt1cI7jaLhKmAu6bMfSOagsMqqbYdb94/BImhYV0zlTLfG3b04bWFYEevl6TMvQPQWrFXyhv3yt0GP9SQZlbCSditUtWWp/E+F7I5rmr0egnpOEmcNothWabwll4w4nNixOusqWNmfiOOeDqLO3YPlJUB3nFoHC/PBw1nisvhBLI5rkuWqdYtE04q3vgtLnTtG/VibjXa0IC0GIzD67RuSU48Thu++OAt+Np7bquadOwc7K2pY2Y9mkKgx67WQPp7HbhxOoDvvtaYadVKtaYjU31IZzl+fDXYkJ/VDNqxYg+g4F5ELFk9uM8M9GBHv1tTmhHr9coV+gRjNfa6l9saVIjTJhkOGuW80cUwk1FOX5Mlhz1K8VCLt18/BokBDxksrIoMtdx0qkDO3Cu3W+ZXn7U4uI94kOPA2WuNs9ld2IxX7Pu/bsJf9TV2DfZgbi1meD/pejSFPndx59K9B0dwaimsy0m1GpVsmY9Mye+rk4eZ1POM+tzbRzRVuc8dkAdR3rh3CE+dX90SRJfDiYqdMoJap1QjiQysElO7YrRw2SyGWyEXgwn0uW1qK6Xg0JQfl9djWDPom31GCe6zQ71lnzPkdeK23QP42ksLhgpv+QtR5czdq2TulV67XbKMsCFo5KTqwmaibHDXy87BHqQyOcOdXOux1Ja21HsODANA3QNrQGU5sr/XgZmBno42EQtVuDg1EwruCpzLi5CrZe4AcNdeuVf76QtrRY9XG2ASiKzTaK+zaAerdGfgsFlqyNy1TbgOK8NMRh0iTy2FMe53VT2Z7z80jsvrMUPWrcJ6oJzdr8DnsiGldMOUI5Js/QIFQHYYddqkhhZVF4PxqnWIauxUV+4Zu6PYiG4N7pMBNw6MevHdV+uXZqpp1odrvMNsFaFEBoyVb2FuFhTcFVLZHDI5XrWgCgC37OyH0ybhhyXSzEqksvWAwG23ItBjN5y5hxPpqtOUsuZuPHPXCgyvn/DBIjHDRdUzS2E1O63EvQeH4bRJ+JoBaWYhGIfbbql6i6vHgkD1CG9xcLdIDLNDjbMhiKey2Iil68/clW1TRoP7ukZwB4B7Dg7j+csbWDFYWyolnKwc3KcHerAcTnZsUTWcSKPXbq15uXqtUHBXiFdY1FGK02bBG3YN4AenltXb/mRG7hXXk7kDsjRzbtnYH1ElR0j12KwSkgZP8qVgvKhTRuC2W7Fn2GMoK0plcji/Eqmotws8Thvu3j+Mb7y8gLTOzqHFzcoDTAIjwb3VmTsgSzONCu6V9skaIdBjh99tw4VV/Tp5OptDKJHZorkDsu7OOfD9k/Vl76rmXubcF+eumDPpNELxTMsHmAAK7iqV9qdq8ca9g7i8HsNF5Q9hRd2dqi+4371/GM9cXFf1aT2EdDgYOg3KMtWyPlFU1euFMrcWRSbHsXe4enAHgJ8+PI6NWFrdEFSNxQo+7oWIonOl4B5KpGFv8eozwd4RD1YjyYbsAdXqdqoFxpjhlXsbao/71uC1b8SDqYC7bt09rNSahDFeKeKiJi5ynUYrdwYUQsFdodL+VC3u2jsEAGrXTH53qr7s6V237oDLZsEnHr2g+xj12NPK3TL6ZRmxN7Wcfe7hKT/CiYzubE7oyHt0Bvc79wyiz23D115a0PV8eTq1+u9YT+YeSWRaroMKhJVFI3R3Ie9V6yDSw87BXkO97huKj3ufhizDGMM9B4bx1Lm1uoziIgnZ3K3c3Zo4H8S53GmEKLi3l0r7U7WYDLixe6hX7XcXt4R6umUA+Y/h52+axEMvXdWdcUQqLMcWGO1zV2/pywSGwwYdIs9cC8MiMXWcvRo2i4S3XTeG7722VDUACHOsUR3ask+HM2QrvdxL2at2zNQf3Bc1htBqZeegrF+X84QvZT1aPJ1aypv2DSGVzeGZkuYDI1TLfEeUOxbxe9DDejTVsn0FsjkdyTJto9L+1HK8ce8gnrmwjmgyg5WwmE7VF9wB4N23z4AD+NTj+rbG68vcjckyixqTjYXsGuyFx2Eta5ZWyumlMKb73VvaKivxjsNjSKRz+M4rlW/fVXOsBmXu7bBhFQz0OjDQa8epBmwSWgzGMdBrb4i8tHNAGIjpy943YpWD+5EdfXBYJTxxrvbXoWMYAAAgAElEQVQFHtVqTb0OKzwOqzppXY3FYBw3//fvN2WpiBaUubeZSvtTy/HGvXJW8tT5NSyHk5CY3Herl8mAGz913Si++Ozlqvs+Oefy7WkVGcFplWUZvVlJNVlGkhium/TpztxPXwurWalejkz1YTLgwteq2BGo2rKOzF3PkuxWrtjTYt+IVx34qoeFTX11CD3sHlIMxHROqqqZu0ZBFZCTjaMzATxRRyANJap77o/4nLrvgM9eiyCd5Q0ZsNJDOEEF1brI5jgeeumq4ek6gdGCKgDcOB1Aj92CR04vYyWcRH+vAxaD7U4P3rkL0VQWf/fMpYrPS6TlVs1qmaZDyZiTOtshFzbj6HPbKlodH5r049RSuKpnTSyVweX1GPYOa9vKlqPQKXJTyQS1UM2xdGTuFonB46hsHtZOWQaQpZnTS+Gaz1lBI3rcBVOBHlgkpjtzF8HdXya4A8DtuwdwdjmCazVq4no890d8Tt2Zu5gM39SxQL1eOOdtO8+6Jrg/e3Ed7//SS/jBKW073mrEa5Bl7FYJt88O4IenllXrAaMcGPPi2J5BfObJixXlFD2OkIDxJdlLwYSqWZbjyFQfsjleNXs/txwB58DekfKTqeW4c88gchwVlzyo5lg6+7m9Vfxl2rGRvpB9Ix4kM7m6M8hGTKcK7FYJk30uQ8Hd47CWXU0J5A3Las3eI8l01TusUZ9Tt+Y+vyGb/tWymMQosVQW2Rwnzb0ehPb36kJtBkJihZ2RzB2Qu2YWggk8N7deU3AHgF89thOrkRS+8sJ82edUG+QQGN2juhhMVM2Ej84EYJEYHj9buV3RaKdMIddN+GC3ShULb0vBODxOq24ppZozZLta1AT7Rsp3zFwLJfDRb5+qmu2GEmlEkpmGZe6A3DGjd5BpI5ZCoLd81g4A+0e86O+x48kadXc9F+ERnwsrkaSueQlhr9CK4N4uiwugi4K7+KBq9ZCO1ZC5A7IVASB/iHrbIEu5dWc/rp/w4W8eu1D2Fl3vwI3RPaqLZQaYCvE4bTgy5a9agDpzLQy7VcKOfn2dMoU4rBYcnvTj2bn1ss9ZKDNJW45KwZ1zLhfq2hjcZ4d7ITHgZElwf/jEAu752GP4Pz88j398vvwFHygoiDcocwfkSdWLq1Fdsw1apmGlSBLDG3YP4Ilzq4Y7VNRaU5XPadTnBOfQ5bSqyjItCO7VVmM2k64L7rUul4ilsrBZWMXbSy1GfS511F5vG2QpjDH86rFdmFuLlR34qOSMV4jQzvW0QybS8gCTnoB55+wgXlkIVjQRO30tgtmhXsN1B8HNO/vxytVg2ZZIWVvWH8QqLeyIprLI8fb80QmcNgumB3pwWjEQW4+m8J4vvID3ffFFzAz0YMjjwGtVumkWlDrEeAN63AU7B3uRzOR0uZaWsx4o5fbd/VgOJ3HW4FR2vtZUXXMH5Lu7arQ2c2/Pij2gC4P71c14xaJcOaptYarEG/fJA016p1O1uPfgCKb73fjEo+eLshvOOZ69uI6//ZFccK3aLWNAljEy2XjHnkFwjootbWeWwronU7W4eSaAHAeOl8neFzcThgZ1fC6bmjmVIrYw9Vaxc2g2wobgX05ewz0fewzffXUJH7x3L/7x127F4Sk/TlYJ7tVaWWthl1i5p2NwbUNH5g4At8/Kd7hG2w/DSX1JTX5KtbKMlc7m1A6xah1qjSAUb5/FRdcFdwBVsx0t9GxhKsfd+2V708mAu6b/HpC7O37lzp04MR/Ejy6s4fJaDH/+/TM49qc/xM994kd48twqfv7GSewarFysNCLLGPEkef24D363DY+d0f7jDMbSWAoldHnKlOPwlB9WieHZi1uDeyKdxVo0ZSiIVZJl9Baom82+ES8urcXw7s8dx0CvHQ+953a85427YbVIODDqw8XVqCoZarEYjENixuYrqmFkWbZs91v9Ajnud2FmoMew7l5tKbxg1CufF9U6ZpaCCeQ4YLOwlmTu7VqxBwDtPbMbSDCeRp/bho1YGq8thPCGXeVXimkRTWUrtgNW4oYdffjW++7Q5YRYiX99ZAIf+94Z/Nrnn1dtQt+wqx+/cfcs7j04ouvi4zDQLbNkoG/cIjHctnsAj59dAed8yyi46NeuJ3N32614/YRPM7irx2pQc0+kc0hmslsGfPRstWoFt+7qh/Vf5Av7b9w9W3ScB8a84FyeYj0y1af53y9sJjDsdcJqaVyeNtBrh8tmqWpJHU9lkUjnEOjRd2G5ffcAvvLCPFKZnG75U2+tyeuywm23VM3cxXvaM+ypaV+sUdq1EAboosw9FE9jeqAHw15HTbp7LJlBj8FiaiEHxrx1W3o6bRb85j17MRlw44P37sWTv/0m/P1/uAX/6siE7ruKWmSZcgNMpRybHcRyOKk5eKMG9zovcDfP9OPE/OaWi9OCugDaWOYOaGurEZ3dR83mpukATv/xW/Db9+3bcgHaPyr/Liudz43scRcwxjAZcGnuCS5kvYJpmBa37R5ALJU15DKal8+qL9HR0+su2iAPjnmRSOeabhPcrkUdQBcF92A8DZ/LhgOj3po6ZvTsT20FDxydwjffdwfe88bdNfUuO9UhJn2yjL/KAFMhd+yR74Ye15BmziyF4XFY6w40N88EkM5yvFBid5DXlvW/vreCv4zeAnUrKFeAHve74HVaK+rui8FEQztlBBN97qqZ+3pEDu56NHdAvkuRGPBElZbaQox8TqM6plSvbsbBWL4Ntdqe3XoJJzKwWco7WjaTrgvuB8d8OLcSMXxF7pTgXi9GhpgWDY6tj/pcmB3qxWMaf5ynr4WxZ8RT1We9GjdM94ExbJFmRBHMqOYOaGfu7fRy1wtjDAfGvGVrSJxzeXdqgzN3AJjoc+HKht7MXV9wF7tcjfjM6J3vAIARr6tq5n51I44hj0PtbGu27h6Ky/5F9f5d1ELXBfcDY15kc9yQTzogF1TdbfQZaRROqzFZxmimfeeeQTxzcb3o4sG5/PuuZXipFK9TvvsqDe4Lm8buMoAqsozO2/12c2DUh1OL2hYFG7E0kplcQztlBJN9boQTmYrBb0OxHtCy+y3HHbMDODEfLNvFVIqRi/Coz4lr4WRFO4erm3GM+13qudHsXnc91gnNoiuCey7HEVIzd/l2y6juHktl4a6xFbKTMJK5L4WMB/c7ZgeQyuTwTEHwXQknsRlLY++wcdsBLW6e6ccLlzeQKvDHKbfntRKVM/c0GENddZZWsH/Ug3g6izkNi4JG+riXMtEn/64r6e7CV6bfQHC/bfcAsjmOp8/rswAWsoyei/CIz4lsjldcgDK/EcdEnxt+t3Ju1NEOGUtl8E8vzOMLz1wu+5xQG51HuyK4R1IZ5Lj8xzzZ50avw2pYd48ma2+F7CREcI9XCe6JdBbr0ZTh4H7zTD/sVqloc5LwJK+nDbKQozMBJNI5/PhqvvBWi/yQ19y3thKGFIfNVu+1NMoBJVnR0t1FcG9K5q609VbS3TdiKUjMWJvfkak+uO0W3S2RkUQGLptFVzdQtV73XI5jMRjHeF/tmXsux/Gj82v4zX84gZv++Pv4z18+gd/96o/L7omVHSEpc68ZcfX1umyQJIYDo+V1ynLE092huVskBpuFVZVlltQFD8YCg8tuwdHpQJHPzJkGtEEWctO03PZXeHcgFw6NBfdq3TLt2sJkhNkhD2wWpnknmrdAbl7mPl9Bd19TBpiMXCDtVgk3zwTwuM7gbsRRsdqU6nI4iXSWY9zvgt8l323o1dyvhRL4s++dwZ1/+gje+TdP49uvLOFt143hD99+EED5wbtwIl1173Gz6I7grnxA4o/5wJgXJxdDuq1UU5kc0lneFcEdkAeZqskyCwbsc0u5c88AzlyLqJ0Jp5fCGOh1GPKyr0R/rwOzQ72q7h5Lydqv0QzVZpHgtlvKyjKd0ClTDbtVwq7BXs1kZSEYh83CMKCzz9wIPpcNHoe1cuYeTRnS2wW37R7AhZWoeudRCSP+P6NVNjKJC9V4nwsepxWMAUGd0+y//ZWX8b9/cBYzAz34858/hOf+y9346M9ch3cenYLTJhUlIoWE4u3zL+qK4B7SCO6xVFa3lWpc9XLv/ExODw6bpWor5FIdq9nuKBklP3MtXJPNbyVu3hnA8bkNZLI51eq3Fm253JRqu73cjXBgzKuduW8mMOJzNkVaYoxhvM9VMXPX6ytTijh/9HTNhBJp9Oq8CPe5bbBbpbLBXXjlTPa5IEkMXmdl19BCloIJvHnfMD7/7pvxjsPjamHfbpVwZKoPz1XI3NvhKwN0SXDfkrkry4f1SjP5FXvdkbm77NWXZBvxlSll34gHgx4HHjuzglyO48y1SEM6ZQo5OtOPSDKDk4vh/IXIa/xYywX3SLK602CncGDUi+VwckuhcDEYx1gT9HbBRJ8bV9Yra+7lNjBVYs9wLwY9Dl26u5FuE8ZYRV93cRci5kd8LptuzX01ksJAGWvjm6YDeG0xtKUDKJPNIZrKUuZeD6XBfXa4F1aJ6S6qqna/JtBg9aBHljE6wFQIYwx3zMoWrpfWY4insw3T2wVHpwMAgGcurhVMpxrP3L0VM/fOl2WA8kXVRi7p0GIyIGfu5Wx612uUZRhjuH33AJ48t1rVVtjoKsQRr7Os5n51M45Aj129Q/e79WXuuRzHRiyF/jLB/eaZADgHnr9UPHgnpqDb4SsD6AzujLH7GGOnGWPnGGMfKvOcn2OMvcYYe5Ux9oXGHmZlSoO7w2rB7LBHdztkTOxP7ZLMXc+S7KVgQrftgBbH9gxiM5bGPykLRhrVKSMY8Tmxo9+NZy6uq9OptUhIvjLbmNq9qMMI6p1owfmczXFcq6GV1QgTfW5EU1nNdXRywEvrth4o5bbdA1iNpNROq3IY/ZyqZe6iUAwombuOVshgPI1sjpf10Dk81adpeNdOR0hAR3BnjFkAfBzAWwAcAPBOxtiBkufMAvgdALdxzg8C+I0mHGtZgvE0rBIrklWM2BCILUy1God1Gk6bPlmmnqzv9t2yFcHfPS1bEc8ONVZzB+SM6Lm5dSxsxjHQa9/ivaKHcrJMKGGObhlA3k865nMWZe6rkSQyOd4U6wHBpOh119Ddw4kMsjmu23qglEOTPgDA2eXKwV1eCq//AjLic+FaKKF5R3B1I4Zxf3Fw12M/sKb085eTZVx2C1437sNzpcG9jV7ugL7M/SiAc5zzC5zzFIAvAbi/5Dm/AuDjnPMNAOCc17bItEbEdGrhiO/BMS9WI0ksh6vvVYyn5Stspw+06MVps1Rd1rEYTNSUCQv6ex143bgXG7E0xv2upkgcR2f6sRlL4/GzKzX3cmv9ASczWaQyOdNk7gC22BCoA0xNztwB7V53YT1QTqqohkgsRLFci2yOG9asR31OpLNcDcgCzrk6nSrQq7mLBTWVisc3zwTw8nyw6I653RYXeoL7OIArBV/PK48VsgfAHsbYk4yxpxlj9zXqAPUggnshQqfUk73Xuj+1U3FYLRUzd3WAqQ5ZBpC3MwGo2+q4HDfPyLq70fV6hXidNkRT2aLdmpE27rWslQOjXpxfiarBo56CuF4mAuWnVMV0aq2Zu9tuhc9lq9gOGakhOOZ73YsvGmvRFBLpHMYLZBmhuVdb/ZefxC3fcnrTdACpbK7I8bKdXu5A4wqqVgCzAO4C8E4Af8MY85c+iTH2IGPsOGPs+MqKfme4agTjW9uNDhiwIei6gqpNQrKC5m7Ex70SoqWt0Xq7YKLPpQb1WiUknzIdWJi9h03iK1PI/tFiz6RmWg8IvE4bfC6bduYeNWYapsWY31XRxTG/hclY5g5gy+uK1XribgSQM/dsjpdd6ygQdwGV7lJumg6AMRRJM3kv984N7lcBTBZ8PaE8Vsg8gIc552nO+UUAZyAH+yI455/knN/IOb9xcHCw1mPeQkgjc/c6bZgMuHQG9+1VUF2sYfGFFjdO9+FnbpjA264bret1ysEYU7P3WiUkn3vrlGqneLkbobRjZmEzAZfNsuW8bzQTZXrdN+rM3AFZUrpaQZYJ13CHpWbuoeLXFT3uhbKM3inVNR3Wxj63DXuHPUUL3kVC0cmyzHMAZhljM4wxO4AHADxc8pyvQc7awRgbgCzTXGjgcVZES5YBgIOjPl297iK4d1VBNVNellkK6V+vVwmbRcL//NnrcXDMV9frVOLoTD+A2o9Vy4KgnQsUakV4JolkZTEYx6jf2XQr2ck+N65U0NybmbmLi7CRO6yBHgesEtvSMSMy90JZRtztV+uYWY8m4XVaq26POjoTwPOX5ME7wASaO+c8A+C9AL4D4CSAL3POX2WMfZgx9nblad8BsMYYew3AIwA+yDnXZ/vWAMoF9wNjXlxcjVa97YqlMrBKDPYGriprJ06rRZ261WKhjtbCVnP3gSHctrsfR5UM3ihawb3df3S1IEkM+0c9arKyEEw0dYBJIDL3Ul16I5qCwyrVVaca9TuxGUuX3RFby55bSWIY9m7dyDS/EYPHaS2KE8IZslrHzGo0hQEd1ho3TQcQS2XVOl8okYbbrs/0rBno+qmc829xzvdwzndxzj+iPPb7nPOHlX9zzvl/5pwf4Jy/nnP+pWYedMmxIZTIaGfuyq3sqSrZezQp709th6F+MxDdMuUKRUvBBHwumynsFoY8Tvz9f7ilrm4ZoESWMWFwB2Td/eRiWHY33Gz8ej0tJvpcSKRzWI0Ud58I64F6/mbGq3TM1CLLANobmUo7ZYD8uVGtY2Y9os9mQSQgwoqg3bMUpk9VI0m537Zc5g5U75iJperbn9ppOG0SOAdSWW1pphl7NzsVrVV7nbRizwgHRr2IJDO4sBrFSiTZ1B53Qd76t1h3X1ccIeshb/SlLc3UeoeltUu1dIAJyGfuVTX3aFJXy+ewNz94BwjrhPadY6YP7qXTqYWMeJ0I9NirFlVjqSzcju7Q24HChR3awf3SWgxTAbfm97oNcV6EEvlbfzN2ywD5ZOWHp5fBeXN73AWiu6RUd1+P1WYaVohIMMq1Q9Ya3MWUauGd69WNCpl7Vc09VXY6tZSbpgM4PrcuLxCizL0+RHDXmgJjTPZ2f3UxWPE1umV/qkBdkq3RMZPLcVxaj2F6oKfVh9UWHFYLnDZpS7eMwypVLZB1GnuGPbBIDN8/eQ1A7e2hRijn616r3W8hIz4nGCsvy0SSaVgkBpfBDWkjPheSmZwatIPxNMLJTFEbJAC4bBbYLVLFzD2X41iPljcNK+XoTAAbsTTOr0SURR2UuddMPrhrXyEPjnlxZilSNMRSSiyVMYX+rJdKmftiKIFUJofp/u0R3AG5LbZwnVrIRKZhhThtFuwc6MFzc7JBVTN73AU9DisCPfYtve7r0ZSh9Xpa2CwShjyOipl7r8NqWNcv3cik1SkDyMmfbCxX3tN9M55GjuvvCsob3q2ry7HbhemDe6mXeykHxrxIZXM4txwp+xrdl7krS7I1LAgurcoe99P920OWAbb6y4QT6bYtLa4XsQAeaO50aiGTfa6iKdV0NodQIlO35g6IdsgymXuNnvv5Xnc5qGv1uAuqOUMK6wG9i2h29Lsx5HHgubn1ti7HBroguFfS3IF8x0ylomosle2ugqq1/JLsi8oCkx3bRJYBtgZ3M3m5lyIcIr1Oa8t2/k70udXsF5B93AHU7AhZyJjPpVo6lyL23BqlNHMv3MBUSjVnyDWDS8AZY7hpJoBnL6633Va664P7zEAvbBZWOXNPZroscy8vy1xai8Fuler2lTETWzN382xhKmW/EtxbobcL5F73uOq0uBGVf5f1au6AHIgXNuOabbvyHZbx4DjY64DE8jYbVzficNokzQDtL+MaKhDTqUYM0o5OB7AYTCCVba85XVcEd4vEyl7hLRLDZMCNudXyK/ei3SrLaGTuc6tR7Ai4m7KarVPRkmXatbS4XkRwb2Ur60TAjVQ2hxVFomiEr4xgzC/30Wtlz7XeYVktEoY8eV930eOupd1Xy9zXo9UdIUspHLijgmodBOOyflqp6DLT34O5CvtU46ls15iGAYWZu0ZwX4tum04ZgbfE9jeSMK8sM+hxYNdgjxrkW0Fpx8xGA6wHBKIofFWjqFrPHVZhr/v8Rhzjfdo1Jp+7sqf7qg5fmVL2DntUrZ009zoIxrWnUwuZHpCDu5aBfzqbQyqbg9tgu1Unky+oFssyuRzHpbXYtiqmAnJ2FlaG3QBzyzIA8PVfvx3/6Sf2tOznTYped2WfqtCha9mfWoqQl7SKqkZX7BVSOKV6dXPrAJNAnBuZMt1069EU/G4bbAYsBCSJ4Sala4aGmOqgnK9MIdP9biTSOSyHk1u+J0zDuilzd5QpqC6FEkhmctixjdoggXw9JpxII5fjiKTM2QopcNuthoJNvWzJ3JXg7m9AcBcdP6XtkJxzZXy/ts9pRBlkiqUyWI+mNDtlAFlzB4qH3ApZiyZrukO5SZFmSHOvAy0v91KEDHFRQ3dXvdy7SnPXDu5CmprZZrJMob9MNJUB5zDNir1OwGmzYKDXoWbu69EUPI7qLol66O+xw26VtnTMJDM5pLO85uA46nMilsri5KLsf182c3eLKVXtXve1SAoDOqdTC/mp68dw38ER7GuhfFaK6YO7lpd7KWJgR0t377YtTED5guqlNTnz2rHNZBlvQXA3oyNkJzAZcGF+M6+5B2pcr1eKJDFZQimZUq33cxpR7gievyT7vJTP3Ct7uq9Fa7NZGPe78Ne/eENbLS5MH9z1yDJjfhfsFkkzuMfVRR3d88derhVybjUKu1VqiVVsJ+HTDO7mlWXawUSfW51SbYRpWCGiHbKQWux+S18TAI4r07yl1gOCwgu/FuvRVM17YtuNqYM751xXcJfbIV2a7ZDRLpRlbBYJVolpyjJT26wNEigO7hFldZtZu2XaxWSfCwubcWQVr5VGdMoItKZU84s6atTclTmO5y9twGZhGPJoSytaltCCbI5jI1a/zUK7MHVwj6ayZe1+S5kZ6MHc6tZ1YfEuLKgCYtVeaea+/TplgOI/4BDJMjUx0edGOstxLZSQTcMamLmP+VxYCiXUbiagfllmWAnua9EURn2usglNJdvfjVgKnOu3Hug0TB3cq02nFjLdr90O2Y2ZOyBW7eUzd9kNMrqtDMMEWrKMWb1l2oUoSF5Zjyl2v42Ttcb8LmRzHMvhfPZeryxjt0rq9qRyejtQ2fZXTKc28i6llZg7uMf0B/cdAz1IZnK4Fi6+/Yt1YUEVkNshC2WZ5XASiXRuW3nKCJw2SbV2jSTqu93froilHWeXI0ikc7r9zfUw6t/q665m7nV8TkJ3L9cpA8gSZo/dopm5r0WFaRgF95ZjJHOf6dduhxStkN1UUAXkgJYskGXE+57Zhpm7sHYNxTN1Z4TblTG/7L3+8vwmgMaYhqmv7du6bq8RXU3CHVLLMKyQchYEqq9MAy9kraQrgrse/4bpATnzKNXdo4rm7uqyzF3W3POZ+yXhBrkNNXcA8LmsCCmyjMS6706t2TisFgx7nHh5Xl5801DN3S9cHPOZu1pQrSO4i8y9kiwDAD63XTNzFx46lLm3gWpe7oWM+VywWyU1yAniqSwsEoPDZFt5qiGWZAvm1mKwW6SWugl2El7FPCycSNe0AIKQ5Y0z1+ShoEbq0B6nDR6HtSRzT8Npk+qaxB1RZZnKCY3PZdVc2LEWSYKxxl7IWompI5oqy7irB3dJYpgKuLfIMtFUBm6bpev+2J02qahbZm41ismAC5Zt1gYpEM6Q4aS5rQfayWTADdGP0Ai730LG/K4izT3SgM/p4JgPDquE3UO9FZ/nd2ln7mtKV5BZ/2ZMH9wlBvTq1MunNdwhY8nuWo4tcJYUVOfWtmenjEAN7iY3DWsnhYXJRvd+j/qdRRYEoUSmbouIY3sG8dLv34PBMj3ugkqau1k7ZYAuCO5el033UM7MgBuX1mJF7ZCxdHdtYRIUau6cK26Q27BTRuArkGUouNeGCO4Sa7zb4ZjfVWRB0KiLsJ5aWrlVe43YE9tOTB/c9ejtgh39cjvkUih/EsWSma4rpgKAo0CWWQ4nEU9nt+UAk8DnsiGUSCMUJ1mmVoT1b5/b3vAp5zGfE2vRlJqQRBLplk0Re102JDO5LRPdq9GkaYupwDYL7sINsdCGoNv2pwoKM3fxfreb1W8hPpcNnMu2x5S514YoTDZabwfy1r/ChiCcyLRsW1a5KVU5czdnGySwzYK7av27VhjcuzNzL9Tct6vVbyGiXXY9mmqrU5+ZGfU7IbHmTGyqSzuUomorl5hrTalmsvLqP9Lc20RIh5d7IaNeJ+xWqShzj6ay6OnCgqrLLqmbmObWYrBZWEv3bnYahUkAyTK1YbNIGO9zYbAJXiul6/ZaWfjWsv1dV/zdB0wsy5g6hTGauUsSw46AG3Nr+UGmeCoLdzfKMlYLsjmOdDYnt0H2uWFt4faeTqM4uHff590q/vznD6syRiMRPemLQdlArBGtkHrJZ+75Xvf8EnDzyjKmPcv12v2WMj3QU5K5Z7pyWrFwG9PcNu+UAYq7Oyi4184NO/qa8roOq7ztaWEzrpr5tWpblpbmrloPmDhzN20qF0tlkdFp91vIzEAPLq3n2yFj3Zq5K9uY4uksLq1Ft63tgKBw0I2Ce2cy5ndiIZho+bYsrYUdYgk4tUK2ASOmYYXs6HcjlclhMZRAOptDKpPryszdoWTu8xtxxFLZbV1MBUpkGXKE7EjGfPKUqurc2aLg7nFYIbHSzF04QppXltl2wV24Is6tRhFLdafdL5CXZU4pC4K3cxskAPTYLeoYOW1h6kxG/U4sbsYLnDtbcxGWJNk1tLBbZj2agsQAv8H40klsu+CutkOuRvP7U7uwNc6pGKGdXgoBwLYeYAJk219xrpAs05mM+12IprJqx0wrPye/q3hKdVWxHjDzSsptF9xHvE44lHbIbt3CBOQz95NLYUTrMOcAAAleSURBVFglVtX2dDsgzpVGj84TjUEMMp1eku82W1VQBRR/mcJWyGjS1D3uwDYM7pLEsKNfbodU96d2ZUFVDu6nl8KYDGzvNkiBKJzREFNnInrdha1wK+cRSj3d1yLmnk4FTBzcQwYWdZQi3CGjyW7O3OWPNhhPb3tJRiASAdLcOxMxpXpaCe6t/Jx8LhuCJX3uARO3QQImDu7BeBqM1XbrNjPQg8trMXXbS3cG9/x72u7FVIHPZYPLZqlrAQTRPAZ6HbBKDFfW45CYXARvFVs19yQGSJZpD8F4Gl6nfrvfQnb09yCVzeH8SgRAtxZU838YlLnL7B7sxc5ButB1KhaJqZOqrd6WJSyhczmOVCaHUCJj6ulUQGdwZ4zdxxg7zRg7xxj7UIXn/WvGGGeM3di4Q9QmVMN0qkDsU31tQe4kcdm6MXPPf7TbfTpV8Otv2o2H3nNbuw+DqIBYlt1q/x+/24YcByKpDDZi5p9OBXQEd8aYBcDHAbwFwAEA72SMHdB4ngfA+wE80+iD1KIW6wGBGOh5VQnuXZm52wszdwrugFxMp8JyZyOKqq1uV1WnVGPpvPXANpBljgI4xzm/wDlPAfgSgPs1nvdHAD4KIKHxvYZTT3Af9jjhtEmqLNOVmrsiy1glVrQejSA6mVGlqNrqjiZ/gQXBWtT806mAvuA+DuBKwdfzymMqjLEjACY559+s9EKMsQcZY8cZY8dXVlYMH2wh9QR32R2yBzkurwxzWLsvm7NZGCQmr0ajbJUwC2O+9mTuhZ7ueUfI7s/cK8IYkwD8GYAPVHsu5/yTnPMbOec3Dg4O1vVzg/FMTW2QAqG799hbW7hpFYwxOG0W6pQhTIVoh+xtueae93RfjZjfyx3QF9yvApgs+HpCeUzgAfA6AD9kjM0BuAXAw80sqnLO6yqoAvkiYzduYRJMBdw4POVv92EQhG5G1YJqmzL3eArr0SQsEjP9JLOe3+BzAGYZYzOQg/oDAP6N+CbnPAhgQHzNGPshgN/knB9v7KHmSaRzSGVz9QV3JaPtxmKq4OH33q6aZRGEGRA2Ga0O7oWe7mtd4CsD6MjcOecZAO8F8B0AJwF8mXP+KmPsw4yxtzf7ALWo1XqgEBHcu7ENUmC3ShTcCVPhdVnxC7dM4c37hlv6c502C+xWSe6WiaZM3ykD6NzExDn/FoBvlTz2+2Wee1f9h1WZRgR30Q7ZjftTCcKsMMbwx+94fVt+tphSXYuY3zQMMOmEaiOC+7DXAadN6krTMIIgjONTPN3XoynTt0EC2zi4M8ZweLIPUwEazScIQtbdhea+bWSZTqMRwR0APvfvj4IkaYIgADmezK3FEE5muiK4b9vMHZALjjTgQxAEAPhcdlxaiwKA6e1+ARMHd8ZoXRpBEI3D57IhneUAYPpFHYBJg3sonpY3lpOmQhBEgxC97oD5HSEBkwb3YDwNn9vc02MEQXQWhTIvae5toh7TMIIgCC2KMneSZdoDBXeCIBqNMCK0Sgxel/nreRTcCYIgkPd0D/TYu8IploI7QRAE8pp7N0ynAiYO7vV4uRMEQZQiPN27oZgKmDC4J9JZpDI503stEwTRWXiVuZluaIMETBjcGzWdShAEUYjVImHc7+qa7WWmKwlTcCcIolk8/N7bumaBj+neBQV3giCaRbcUUwEzyjIxCu4EQRDVMF9wp8ydIAiiKhTcCYIguhDTBfeJPhfuPThMfe4EQRAVMF1B9Z6DI7jn4Ei7D4MgCKKjMV3mThAEQVSHgjtBEEQXQsGdIAiiC6HgThAE0YVQcCcIguhCKLgTBEF0IRTcCYIguhAK7gRBEF0I45y35wcztgLgUo3/+QCA1QYejlnYru8b2L7vnd739kLP+97BOR+s9kJtC+71wBg7zjm/sd3H0Wq26/sGtu97p/e9vWjk+yZZhiAIoguh4E4QBNGFmDW4f7LdB9Amtuv7Brbve6f3vb1o2Ps2peZOEARBVMasmTtBEARRAdMFd8bYfYyx04yxc4yxD7X7eJoFY+zTjLFlxtgrBY8FGGPfY4ydVf6/r53H2AwYY5OMsUcYY68xxl5ljL1febyr3ztjzMkYe5YxdkJ533+oPD7DGHtGOd//H2PM3u5jbQaMMQtj7EXG2DeUr7v+fTPG5hhjP2aMvcQYO6481rDz3FTBnTFmAfBxAG8BcADAOxljB9p7VE3jswDuK3nsQwD+hXM+C+BflK+7jQyAD3DODwC4BcB7lM+42997EsCbOOfXAzgE4D7G2C0APgrgY5zz3QA2ALy7jcfYTN4P4GTB19vlfb+Rc36ooP2xYee5qYI7gKMAznHOL3DOUwC+BOD+Nh9TU+CcPwZgveTh+wF8Tvn35wC8o6UH1QI454uc8xeUf4ch/8GPo8vfO5eJKF/alP9xAG8C8I/K4133vgGAMTYB4CcB/F/la4Zt8L7L0LDz3GzBfRzAlYKv55XHtgvDnPNF5d9LAIbbeTDNhjE2DeAwgGewDd67Ik28BGAZwPcAnAewyTnPKE/p1vP9zwH8FoCc8nU/tsf75gC+yxh7njH2oPJYw85z0+1QJWQ455wx1rWtToyxXgBfAfAbnPOQnMzJdOt755xnARxijPkBfBXAvjYfUtNhjL0NwDLn/HnG2F3tPp4Wczvn/CpjbAjA9xhjpwq/We95brbM/SqAyYKvJ5THtgvXGGOjAKD8/3Kbj6cpMMZskAP733PO/0l5eFu8dwDgnG8CeATArQD8jDGRhHXj+X4bgLczxuYgy6xvAvC/0P3vG5zzq8r/L0O+mB9FA89zswX35wDMKpV0O4AHADzc5mNqJQ8D+HfKv/8dgIfaeCxNQdFbPwXgJOf8zwq+1dXvnTE2qGTsYIy5APwE5HrDIwB+Rnla171vzvnvcM4nOOfTkP+ef8A5/7fo8vfNGOthjHnEvwHcA+AVNPA8N90QE2PsrZA1OguAT3POP9LmQ2oKjLEvArgLskvcNQB/AOBrAL4MYAqyo+bPcc5Li66mhjF2O4DHAfwYeQ32dyHr7l373hlj10EuoFkgJ11f5px/mDG2E3JGGwDwIoBf4Jwn23ekzUORZX6Tc/62bn/fyvv7qvKlFcAXOOcfYYz1o0HnuemCO0EQBFEds8kyBEEQhA4ouBMEQXQhFNwJgiC6EAruBEEQXQgFd4IgiC6EgjtBEEQXQsGdIAiiC6HgThAE0YX8f4mEuCfsgKAyAAAAAElFTkSuQmCC\n", "text/plain": "<Figure size 432x288 with 1 Axes>"}, "metadata": {"needs_background": "light"}, "output_type": "display_data"}], "source": "print(np.mean(Wape['Lstm']),np.median(Wape['Lstm']))\nplt.plot(Wape['Lstm'])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Running for the month of : October\ncluster id:  1\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.13780926167964935 wape is 0.1331251561641693 and pe is 0.08637341111898422 and total is 0.11910260717074077\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\nthe  smape is 0.11313740164041519 wape is 0.11245818436145782 and pe is 0.012121422216296196 and total is 0.07923900584379832\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  3\ntrial run:  4\ntrial run:  5\nthe  smape is 0.1146872490644455 wape is 0.11343083530664444 and pe is 0.00514252670109272 and total is 0.07775353888670604\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  743\nfor sanity check 0.11468725 0.113430835 0.0051425267 the mean is:  0.07775353888670604\nval wape is:  0.11343083530664444\ntotal time  -191.21117162704468\ntest Wsmape:  0.26921119269886423  test Wape:  0.2901822884032903  test Wpe:  0.2216277304862549\nSmape dis [0.5678879358408261, 0.8132123594382221, 0.9613998411970008] Wape dis [0.57276649224338, 0.8279701176126486, 1.0432575837151172] PE dis [0.29745142982800593, 0.6121785562312771, 0.8623007454592735]\nAgg SMAPE:  0.18431127 Agg WAPE:  0.19576837 Agg PE:  0.11419397\ntotal time  -191.21117162704468\n===============================================\ncluster id:  2\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.4363371431827545 wape is 0.43865036964416504 and pe is 0.43383607268333435 and total is 0.43627452850341797\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\nthe  smape is 0.40879085659980774 wape is 0.4215649366378784 and pe is 0.4147523045539856 and total is 0.41503604253133136\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  3\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\nEarly stopping!\nStart to test process. 34\ntrial run:  20\nEarly stopping!\nStart to test process. 34\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\nnumber of smape:  263\nfor sanity check 0.40879086 0.42156494 0.4147523 the mean is:  0.41503604253133136\nval wape is:  0.4215649366378784\ntotal time  -177.8138735294342\ntest Wsmape:  1.1366485800295953  test Wape:  0.7320840048635878  test Wpe:  0.727842232016198\nSmape dis [1.1058718824081621, 1.2078202453445162, 1.2564688609773804] Wape dis [0.7365824099240488, 0.7796220089925108, 0.8079402319342286] PE dis [0.7305937568941815, 0.7724496839616668, 0.7982993203752328]\nAgg SMAPE:  1.1375818 Agg WAPE:  0.7264858 Agg PE:  0.7264857\ntotal time  -177.8138735294342\n===============================================\ncluster id:  3\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.3074282705783844 wape is 0.2922130525112152 and pe is 0.29047924280166626 and total is 0.2967068552970886\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\ntrial run:  4\ntrial run:  5\ntrial run:  6\nthe  smape is 0.2880764901638031 wape is 0.27805304527282715 and pe is 0.2758944034576416 and total is 0.28067465623219806\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  7\ntrial run:  8\nthe  smape is 0.2821868360042572 wape is 0.273668110370636 and pe is 0.27109989523887634 and total is 0.27565161387125653\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\nnumber of smape:  504\nfor sanity check 0.28218684 0.2736681 0.2710999 the mean is:  0.27565161387125653\nval wape is:  0.273668110370636\ntotal time  -190.3651683330536\ntest Wsmape:  0.9327424365825592  test Wape:  0.6441754862841045  test Wpe:  0.6393950997098451\nSmape dis [0.8947835862469344, 1.0180323220062777, 1.0945154267048798] Wape dis [0.657548912409996, 0.7170744363862495, 0.7539253595843477] PE dis [0.647934108208591, 0.7000577654282478, 0.7352667341212266]\nAgg SMAPE:  0.9346476 Agg WAPE:  0.63931817 Agg PE:  0.63931817\ntotal time  -190.3651683330536\n===============================================\ncluster id:  4\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.1704799383878708 wape is 0.15781773626804352 and pe is 0.1315288096666336 and total is 0.1532754898071289\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\nEarly stopping!\nStart to test process. 186\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\nEarly stopping!\nStart to test process. 68\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  413\nfor sanity check 0.17047994 0.15781774 0.13152881 the mean is:  0.1532754898071289\nval wape is:  0.15781773626804352\ntotal time  -183.85787558555603\ntest Wsmape:  0.21093843866446127  test Wape:  0.20224578368494484  test Wpe:  0.11903427149807144\nSmape dis [0.43152213761376135, 0.6620959733136296, 0.8118932321438457] Wape dis [0.4081899217377538, 0.6086602671992332, 0.7295438789822382] PE dis [0.14477497016644486, 0.3053379353297866, 0.43461931904213846]\nAgg SMAPE:  0.10569592 Agg WAPE:  0.101221204 Agg PE:  0.061545413\ntotal time  -183.85787558555603\n===============================================\ncluster id:  5\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.7974628806114197 wape is 0.8698642253875732 and pe is 0.6278818845748901 and total is 0.7650696436564127\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\nthe  smape is 0.8262848854064941 wape is 0.9018217325210571 and pe is 0.5620511174201965 and total is 0.7633859316507975\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  4\ntrial run:  5\nthe  smape is 0.800150990486145 wape is 0.9176546335220337 and pe is 0.5030608177185059 and total is 0.7402888139088949\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\nthe  smape is 0.588944673538208 wape is 0.8155511617660522 and pe is 0.6533483862876892 and total is 0.6859480539957682\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  10\nthe  smape is 0.5604025721549988 wape is 0.8065719604492188 and pe is 0.6765983700752258 and total is 0.6811909675598145\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  11\nthe  smape is 0.5633425116539001 wape is 0.8076524138450623 and pe is 0.6701099276542664 and total is 0.6803682645161947\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\nthe  smape is 0.5561899542808533 wape is 0.805151104927063 and pe is 0.6776362061500549 and total is 0.6796590487162272\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  18\nthe  smape is 0.5349892973899841 wape is 0.7992169857025146 and pe is 0.6940977573394775 and total is 0.6761013666788737\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.001, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 2, 'relu': False}\nnumber of smape:  13\nfor sanity check 0.5349893 0.799217 0.69409776 the mean is:  0.6761013666788737\nval wape is:  0.7992169857025146\ntotal time  -191.49883770942688\ntest Wsmape:  1.6221924217580828  test Wape:  0.9046229401249162  test Wpe:  0.9046206383392085\nSmape dis [1.6769442535994292, 1.791174601634505, 1.8131127670945433] Wape dis [0.9405515475136818, 0.9556407051818833, 0.9621563974825181] PE dis [0.9405515475136819, 0.9556407051818833, 0.959276656043635]\nAgg SMAPE:  1.6240493 Agg WAPE:  0.90462065 Agg PE:  0.9046206\ntotal time  -191.49883770942688\n===============================================\ncluster id:  6\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.3185179531574249 wape is 0.40829336643218994 and pe is 0.2911427617073059 and total is 0.33931803703308105\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\nEarly stopping!\nStart to test process. 32\ntrial run:  19\nEarly stopping!\nStart to test process. 41\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  214\nfor sanity check 0.31851795 0.40829337 0.29114276 the mean is:  0.33931803703308105\nval wape is:  0.40829336643218994\ntotal time  -179.41535139083862\ntest Wsmape:  0.5112780657196998  test Wape:  0.41319979441551474  test Wpe:  0.40144143131505705\nSmape dis [0.4991999895052631, 0.590077360545348, 0.7113422334979965] Wape dis [0.4173619914976494, 0.5106734221877569, 0.5961198618681917] PE dis [0.4008097117713305, 0.488749817214029, 0.5388330968679311]\nAgg SMAPE:  0.5027688 Agg WAPE:  0.39829972 Agg PE:  0.39829966\ntotal time  -179.41535139083862\n===============================================\ncluster id:  7\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.09778327494859695 wape is 0.09820901602506638 and pe is 0.04841262474656105 and total is 0.08146830399831136\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\nthe  smape is 0.10255923867225647 wape is 0.10294650495052338 and pe is 0.03526787459850311 and total is 0.08025787274042766\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\ntrial run:  12\ntrial run:  13\ntrial run:  14\ntrial run:  15\nthe  smape is 0.10246539115905762 wape is 0.10249317437410355 and pe is 0.029155703261494637 and total is 0.07803808649381001\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  797\nfor sanity check 0.10246539 0.102493174 0.029155703 the mean is:  0.07803808649381001\nval wape is:  0.10249317437410355\ntotal time  -192.06852984428406\ntest Wsmape:  0.6328377349767776  test Wape:  0.655347992446738  test Wpe:  0.3355122136252179\nSmape dis [0.782277837120124, 1.0171599781019132, 1.147028741328264] Wape dis [0.7211014157021539, 1.004553121792218, 1.293452579891126] PE dis [0.28123636117990164, 0.6006261402129526, 0.9282065370599406]\nAgg SMAPE:  0.13906614 Agg WAPE:  0.14941582 Agg PE:  0.12893689\ntotal time  -192.06852984428406\n===============================================\ncluster id:  8\nBeginning the training...\nval shape is:  torch.Size([61, 90, 1]) torch.Size([61, 30])\ntrial run:  1\nthe  smape is 0.1761302947998047 wape is 0.18828587234020233 and pe is 0.18542295694351196 and total is 0.1832797129948934\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.1, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  2\ntrial run:  3\ntrial run:  4\ntrial run:  5\ntrial run:  6\ntrial run:  7\ntrial run:  8\ntrial run:  9\ntrial run:  10\ntrial run:  11\nthe  smape is 0.17453254759311676 wape is 0.1863042265176773 and pe is 0.18477891385555267 and total is 0.18187189102172852\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 10, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.1, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  12\ntrial run:  13\nthe  smape is 0.15414895117282867 wape is 0.16235432028770447 and pe is 0.15893056988716125 and total is 0.15847795208295187\nthe config is {} {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\ntrial run:  14\ntrial run:  15\ntrial run:  16\ntrial run:  17\ntrial run:  18\ntrial run:  19\ntrial run:  20\ntrial run:  21\ntrial run:  22\ntrial run:  23\ntrial run:  24\nthus best config is {'num_epochs': 200, 'learning_rate': 0.01, 'hidden_size': 20, 'window_size': 30, 'output_size': 30, 'batch_size': 32, 'dropout': 0.05, 'dropout1': 0, 'gamma': 0.9, 'optimizer_name': 'AdamW', 'num_layer': 1, 'relu': False}\nnumber of smape:  707\nfor sanity check 0.15414895 0.16235432 0.15893057 the mean is:  0.15847795208295187\nval wape is:  0.16235432028770447\ntotal time  -200.27052211761475\ntest Wsmape:  0.7101959301412871  test Wape:  0.9512769683165205  test Wpe:  0.7805692307069295\nSmape dis [0.8383941364706526, 1.1204310725597502, 1.2915226005968177] Wape dis [0.9785971092813089, 1.5345211307314501, 2.0385539360730625] PE dis [0.7127702838168654, 1.3591653308406844, 1.8289041221507576]\nAgg SMAPE:  0.5257588 Agg WAPE:  0.72846353 Agg PE:  0.72846365\ntotal time  -200.27052211761475\n===============================================\ncluster id:  9\n"}], "source": "################LSTM##########################\nname=['September','October','November','December','January','February','March','April','May','June','July','August']\n################LSTM##########################\nfrom torch.optim.lr_scheduler import ExponentialLR\nTimesL=[]\nSMAPEsL=[]\nPEsL=[]\nWAPEsL=[]\nAggSmapeL=[]\nAggWapeL=[]\nAggPeL=[]\ntimes_org=[]\nmedian_smape,median_wape,median_pe=[],[],[]\nEighty_per_smape,Eighty_per_wape,Eighty_per_pe=[],[],[]\nNinty_per_smape,Ninty_per_wape,Ninty_per_pe=[],[],[]\nwape_agg_lstm=np.zeros(50, dtype=np.float64)\ndf_lstm=np.zeros((30,50), dtype=np.float64)\n################LSTM##########################\nmonth=1\nprint('Running for the month of :',name[month])\n################LSTM##########################\n################LSTM##########################\n################LSTM##########################\nfor i in range(50):\n    print('cluster id: ',i+1)\n    current_date = '20220831'\n    test_group_id = test_group_id_list[i]#'/3944/1060825/447913'\n    #for top cluster:\n    path = f\"gs://adtech-artifacts/ds/forecast/month_eval/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_tft_data_{current_date}.csv\"\n    #path = f\"gs://adtech-artifacts/ds/forecast/month_eval/tail_clusters/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_data_20220831.csv\"\n    # read in data\n    df_panda = spark.read.option(\"header\", True).csv(path).toPandas() # The usage should be the same as previous\n    df_panda=df_panda[['ds', 'group_id', 'query_idx', 'SearchKeyValue',  'LocationKeyValue', 'daily_supply']]\n    #df_panda=spark.read.csv(\"gs://adtech-artifacts/ds/forecast/query_taxonomy_sample_Addedfeatures_cluster_\"+str(i)+\".csv\",header=True).toPandas()\n    Cols=['daily_supply']#, 'shifted_annual_daily_supply','Trend_Var','dayofyear_sin', 'dayofyear_cos', 'month_sin', 'month_cos']\n    for cols in Cols:  \n        df_panda[cols]=df_panda[cols].astype(float,errors='raise')\n    start=time.time()\n    minError=1e10\n    param = {\n        \"learning_rate\": [.1,.01,.001],\n        \"hidden_size\": [10,20],\n        'batch_size': [32],\n        'dropout': [0.05,.1],\n        'gamma': [0.9],\n        'optimizer_name': ['AdamW'],\n        'num_layer': [1,2],\n        'relu': [False]\n    }\n    prep = dataprep(df_panda, input_window=90, output_window=30, stride=1,start=start_month[month])\n    print('Beginning the training...')\n    train_set_in, train_set_out, val_set_in, val_set_out, test_in_set, test_out_set,df_test_org,  q,wq = prep._get_data()\n    #print('print len of q, value and wq value',len(q),q[0],wq[0])\n    train_set_in = torch.from_numpy(train_set_in.astype(np.float64)).float()\n    train_set_out = torch.from_numpy(train_set_out.astype(np.float64)).float()\n    val_set_in = torch.from_numpy(val_set_in.astype(np.float64)).float()\n    val_set_out = torch.from_numpy(val_set_out.astype(np.float64)).float()\n    test_in_set = torch.from_numpy(test_in_set.astype(np.float64)).float()\n    test_out_set = torch.from_numpy(test_out_set.astype(np.float64)).float()\n    print('val shape is: ',val_set_in.shape, val_set_out.shape)\n    end1=time.time()\n    if len(train_set_in.shape) < 3:\n        train_set_in = train_set_in.reshape(train_set_in.shape[0], train_set_in.shape[1], 1)\n        val_set_in = val_set_in.reshape(val_set_in.shape[0], val_set_in.shape[1], 1)\n        test_in_set = test_in_set.reshape(test_in_set.shape[0], test_in_set.shape[1], 1)\n        test_out_set=test_out_set.reshape(test_out_set.shape[0], test_out_set.shape[1])\n    counter=1\n    for lr in param['learning_rate']:\n        for h_s in param['hidden_size']:\n            for b_s in param['batch_size']:\n                for dp in param['dropout']:\n                    for opt in param['optimizer_name']:\n                        for n_l in param['num_layer']:\n                            for act in param['relu']:\n                                for gm in param['gamma']:\n                                    config = {\n                                        \"num_epochs\": 200,\n                                        \"learning_rate\": lr,\n                                        \"hidden_size\": h_s,\n                                        \"window_size\": 30,\n                                        \"output_size\": 30,\n                                        'batch_size': b_s,\n                                        'dropout': dp,\n                                        'dropout1': 0,\n                                        'gamma': gm,\n                                        'optimizer_name': opt,\n                                        'num_layer': n_l,\n                                        'relu': act\n                                    }\n                                    print('trial run: ',counter)\n                                    counter+=1\n                                    e1,e2,e3,model = running_lstm(config, save_model=True)\n                                    if (e1+e2+e3)/3<minError:\n                                        minError=(e1+e2+e3)/3\n                                        best_config=config\n                                        best_model=model\n                                        print('the  smape is {} wape is {} and pe is {} and total is {}'.format(e1,e2,e3,(e1+e2+e3)/3))\n                                        print('the config is {}',config)\n    if minError>=.7:\n        param = {\n        \"learning_rate\": [.001,.0001],\n        \"hidden_size\": [15,30],\n        'batch_size': [32],\n        'dropout': [.05,.5],\n        'gamma': [0.9],\n        'optimizer_name': ['Adam','AdamW'],\n        'num_layer': [3,5],\n        'relu': [True]}\n        for lr in param['learning_rate']:\n            for h_s in param['hidden_size']:\n                for b_s in param['batch_size']:\n                    for dp in param['dropout']:\n                        for opt in param['optimizer_name']:\n                            for n_l in param['num_layer']:\n                                for act in param['relu']:\n                                    for gm in param['gamma']:\n                                        config = {\n                                            \"num_epochs\": 200,\n                                            \"learning_rate\": lr,\n                                            \"hidden_size\": h_s,\n                                            \"window_size\": 30,\n                                            \"output_size\": 30,\n                                            'batch_size': b_s,\n                                            'dropout': dp,\n                                            'dropout1': 0,\n                                            'gamma': gm,\n                                            'optimizer_name': opt,\n                                            'num_layer': n_l,\n                                            'relu': act\n                                        }\n                                        print('trial run: ',counter)\n                                        counter+=1\n                                        e1,e2,e3,model = running_lstm(config, save_model=True)\n                                        \n                                        #print('the config is {}',config)\n                                        if (e1+e2+e3)/3<minError:\n                                            minError=(e1+e2+e3)/3\n                                            best_config=config\n                                            best_model=model\n                                            print(best_model)\n                                            print('the  smape is {} wape is {} and pe is {} and total is {}'.format(e1,e2,e3,(e1+e2+e3)/3))\n                                            print('the config is {}',config)\n\n    end = time.time()\n    print('thus best config is', best_config)\n    Wsmape, Wape, Wpe, smape_dis, wape_dis, pe_dis, smape_agg, wape_agg, pe_agg, model, df_lstm[:,i],wape_agg_lstm[i] = check_lstm(best_config, best_model)\n    print('val wape is: ',wape_agg_lstm[i])\n    print('total time ', end1 - end)\n    print('test Wsmape: ', Wsmape, ' test Wape: ', Wape, ' test Wpe: ', Wpe)\n    print('Smape dis',smape_dis,'Wape dis',wape_dis,'PE dis',pe_dis)\n    print('Agg SMAPE: ',smape_agg,'Agg WAPE: ',wape_agg,'Agg PE: ',pe_agg)\n    print('total time ', end1 - end)\n\n    #print('time needed to run the model: ',end-start)\n    AggSmapeL.append(smape_agg)\n    AggWapeL.append(wape_agg)\n    AggPeL.append(pe_agg)\n    SMAPEsL.append(Wsmape)\n    WAPEsL.append(Wape)\n    PEsL.append(Wpe)\n    median_smape.append(smape_dis[0])\n    Eighty_per_smape.append(smape_dis[1])\n    Ninty_per_smape.append(smape_dis[2])\n    median_wape.append(wape_dis[0])\n    Eighty_per_wape.append(wape_dis[1])\n    Ninty_per_wape.append(wape_dis[2])\n    median_pe.append(pe_dis[0])\n    Eighty_per_pe.append(pe_dis[1])\n    Ninty_per_pe.append(pe_dis[2])\n    TimesL.append(end1-end)\n    times_org.append(end1-start)\n    print('===============================================')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "print('TimesL=',TimesL)\nprint('SMAPEsL=',SMAPEsL)\nprint('PEsL=',PEsL)\nprint('WAPEsL=',WAPEsL)\nprint('AggSmapeL=',AggSmapeL)\nprint('AggWapeL=',AggWapeL)\nprint('AggPeL=',AggPeL)\nprint('times_org=',times_org)\nprint('median_smape=',median_smape)\nprint('median_wape=',median_wape)\nprint('median_pe=',median_pe)\nprint('Eighty_per_smape=',Eighty_per_smape)\nprint('Eighty_per_wape=',Eighty_per_wape)\nprint('Eighty_per_pe=',Eighty_per_pe)\nprint('Ninty_per_smape=',Ninty_per_smape)\nprint('Ninty_per_wape=',Ninty_per_wape)\nprint('Ninty_per_pe=',Ninty_per_pe)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "Times=[]\nSMAPEs=[]\nWAPEs=[]\nPEs=[]\nAggSmape=[]\nAggWape=[]\nAggPe=[]\nSMAPEs_dist=[]\nWAPEs_dist=[]\nPEs_dist=[]\ntraining_j=[]\n\n\nfor i in range(50):\n    st=time.time()\n    print('cluster id: ',i+1)\n    current_date = '20220831'\n    test_group_id = test_group_id_list[i]#'/3944/1060825/447913'\n    print('group id',test_group_id)\n    #for top cluster:\n    path = f\"gs://adtech-artifacts/ds/forecast/month_eval/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_tft_data_{current_date}.csv\"\n    #path = f\"gs://adtech-artifacts/ds/forecast/month_eval/tail_clusters/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_data_20220831.csv\"\n    # read in data\n    df = spark.read.option(\"header\", True).csv(path).toPandas() # The usage should be the same as previous\n    df=df[['ds', 'group_id', 'query_idx', 'SearchKeyValue',  'LocationKeyValue', 'daily_supply']]\n    #print('query id: ',df['query_idx'].unique())\n    #df_panda=spark.read.csv(\"gs://adtech-artifacts/ds/forecast/query_taxonomy_sample_Addedfeatures_cluster_\"+str(i)+\".csv\",header=True).toPandas()\n    Cols=['daily_supply']#, 'shifted_annual_daily_supply','Trend_Var','dayofyear_sin', 'dayofyear_cos', 'month_sin', 'month_cos']\n    for cols in Cols:df[cols]=df[cols].astype(float,errors='raise')\n    #print('time needed to preprocess the data is: ',time.time()-st)\n    df_train,df_test,train_date,test_date,q,wq=_generate_train_test_data(df,start=start_month[month])\n    df_trainW,df_valW,_,train_dateW,_,_,_=_generate_train_test_data(df,val_req=True,start=start_month[month])\n    df1_train,df1_test=df_train.sum(axis=1),df_test.sum(axis=1)\n    df1_trainW,df_valW=df_trainW.sum(axis=1),df_valW.sum(axis=1)\n    df1_train,df1_trainW,df_valW=df1_train.to_frame(),df1_trainW.to_frame(),df_valW.to_frame()\n    df1_train.index=train_date\n    df1_trainW.index=train_dateW\n    df1_train.index = df1_train.index.to_period(freq = 'D')\n    df1_trainW.index = df1_trainW.index.to_period(freq = 'D')\n    #df_test_log_temp.index = df_test_log_temp.index.to_period(freq = 'D')\n    df1_train=df1_train.rename(columns={0:'daily_supply'})\n    df1_trainW=df1_trainW.rename(columns={0:'daily_supply'})\n#     if i==0:\n#         print(q,wq)\n    start=time.time()\n    Models=statModels()\n\n    Models.clus_smape['Lstm']=AggSmapeL[i]\n    Models.clus_wape['Lstm']=AggWapeL[i]\n    Models.clus_pe['Lstm']=AggPeL[i]\n    #Models.models['Lstm'] = fit4\n    #self.stores_mape['Es'] = mape\n    Models.stores_smape['Lstm'] = SMAPEsL[i]\n    Models.store_wape['Lstm'] = WAPEsL[i]\n    Models.store_pe['Lstm'] = PEsL[i]\n    Models.smape_dis['Lstm']=[median_smape[i],Eighty_per_smape[i],Ninty_per_smape[i]]\n    Models.wape_dis['Lstm']=[median_wape[i],Eighty_per_wape[i],Ninty_per_wape[i]]\n    Models.pe_dis['Lstm']=[median_pe[i],Eighty_per_pe[i],Ninty_per_pe[i]]\n    Models.stores_times['Lstm'] = TimesL[i]\n    ##################\n    #Models.wape_Weighted['Lstm'] = wape_agg_lstm[i]\n\n    Models.training_infer(df1_train,df1_test,df_test,q.values,wq.values,df1_test.shape[0],df_lstm[:,i])#no_ts\n    Models.weighted_avg(df1_trainW, df1_test,df_valW.values, df_test,q.values,wq.values, forecast_period=60,lstm_pred=df_lstm[:,i])\n\n    end=time.time()\n    print('Wape is: ',Models.store_wape)\n    print('Smape is: ',Models.stores_smape)\n    print('Time is:' ,Models.stores_times)\n    #print('time needed to run the model: ',end-start)\n    SMAPEs.append(Models.stores_smape)\n    WAPEs.append(Models.store_wape)\n    PEs.append(Models.store_pe)\n    Times.append(Models.stores_times)\n    AggSmape.append(Models.clus_smape)\n    AggWape.append(Models.clus_wape)\n    AggPe.append(Models.clus_pe)\n    SMAPEs_dist.append(Models.smape_dis)\n    WAPEs_dist.append(Models.wape_dis)\n    PEs_dist.append(Models.pe_dis)\n    training_j.append(i)\n\n    print('===============================================')\n    "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "Smape={i:[] for i in SMAPEs[0]}\nWape={i:[] for i in WAPEs[0]}\nPe={i:[] for i in PEs[0]}\nTime={i:[] for i in Times[0]}\n\nAggSmapes={i:[] for i in AggSmape[0]}\nAggWapes={i:[] for i in AggWape[0]}\nAggPes={i:[] for i in AggPe[0]}\n\nSMAPE_dist_median={i:[] for i in SMAPEs_dist[0]}\nSMAPE_dist_80={i:[] for i in SMAPEs_dist[0]}\nSMAPE_dist_90={i:[] for i in SMAPEs_dist[0]}\n\nWAPE_dist_median={i:[] for i in WAPEs_dist[0]}\nWAPE_dist_80={i:[] for i in WAPEs_dist[0]}\nWAPE_dist_90={i:[] for i in WAPEs_dist[0]}\n\n\nPE_dist_median={i:[] for i in PEs_dist[0]}\nPE_dist_80={i:[] for i in PEs_dist[0]}\nPE_dist_90={i:[] for i in PEs_dist[0]}\nfor i in range(len(training_j)):\n    for keys,values in SMAPEs[i].items():\n        Smape[keys].append(values)\n    for keys,values in WAPEs[i].items():\n        Wape[keys].append(values)\n    for keys,values in PEs[i].items():\n        Pe[keys].append(values)\n\n    for keys,values in Times[i].items():\n        Time[keys].append(values)\n    for keys,values in AggSmape[i].items():\n        AggSmapes[keys].append(values)\n    for keys,values in AggWape[i].items():\n        AggWapes[keys].append(values)\n    for keys,values in AggPe[i].items():\n        AggPes[keys].append(values)\n    \n    for keys,values in SMAPEs_dist[i].items():\n        SMAPE_dist_median[keys].append(values[0])\n    for keys,values in SMAPEs_dist[i].items():\n        SMAPE_dist_80[keys].append(values[1])\n    for keys,values in SMAPEs_dist[i].items():\n        SMAPE_dist_90[keys].append(values[2])\n        \n    for keys,values in WAPEs_dist[i].items():\n        WAPE_dist_median[keys].append(values[0])\n    for keys,values in WAPEs_dist[i].items():\n        WAPE_dist_80[keys].append(values[1])\n    for keys,values in WAPEs_dist[i].items():\n        WAPE_dist_90[keys].append(values[2])\n        \n    for keys,values in PEs_dist[i].items():\n        PE_dist_median[keys].append(values[0])\n    for keys,values in PEs_dist[i].items():\n        PE_dist_80[keys].append(values[1])\n    for keys,values in PEs_dist[i].items():\n        PE_dist_90[keys].append(values[2])\nprint('Smape is: ',Smape)\nprint('====================')\nprint('Wape is: ',Wape)\nprint('====================')\nprint('Pe is: ',Pe)\nprint('====================')\nprint('Time is: ',Time)\nprint('====================')\nprint('AggSmapes is: ',AggSmapes)\nprint('====================')\nprint('AggWapes is: ',AggWapes)\nprint('====================')\nprint('AggPes is: ',AggPes)\nprint('====================')\nprint('SMAPE_dist_median is: ',SMAPE_dist_median)\nprint('====================')\nprint('SMAPE_dist_80 is: ',SMAPE_dist_80)\nprint('====================')\nprint('SMAPE_dist_90 is: ',SMAPE_dist_90)\nprint('====================')\nprint('WAPE_dist_median is: ',WAPE_dist_median)\nprint('====================')\nprint('WAPE_dist_80 is: ',WAPE_dist_80)\nprint('====================')\nprint('WAPE_dist_90 is: ',WAPE_dist_90)\nprint('====================')\nprint('PE_dist_median is: ',PE_dist_median)\nprint('====================')\nprint('PE_dist_80 is: ',PE_dist_80)\nprint('====================')\nprint('SmapePE_dist_90 is: ',PE_dist_90)\nprint('====================')\nprint('Original LSTM time is', times_org)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "################LSTM##########################\nname=['September','October','November','December','January','February','March','April','May','June','July','August']\n################LSTM##########################\nfrom torch.optim.lr_scheduler import ExponentialLR\nTimesL=[]\nSMAPEsL=[]\nPEsL=[]\nWAPEsL=[]\nAggSmapeL=[]\nAggWapeL=[]\nAggPeL=[]\ntimes_org=[]\nmedian_smape,median_wape,median_pe=[],[],[]\nEighty_per_smape,Eighty_per_wape,Eighty_per_pe=[],[],[]\nNinty_per_smape,Ninty_per_wape,Ninty_per_pe=[],[],[]\nwape_agg_lstm=np.zeros(50, dtype=np.float64)\ndf_lstm=np.zeros((30,50), dtype=np.float64)\n################LSTM##########################\nmonth=2\nprint('Running for the month of :',name[month])\n################LSTM##########################\n################LSTM##########################\n################LSTM##########################\nfor i in range(50):\n    print('cluster id: ',i+1)\n    current_date = '20220831'\n    test_group_id = test_group_id_list[i]#'/3944/1060825/447913'\n    #for top cluster:\n    path = f\"gs://adtech-artifacts/ds/forecast/month_eval/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_tft_data_{current_date}.csv\"\n    #path = f\"gs://adtech-artifacts/ds/forecast/month_eval/tail_clusters/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_data_20220831.csv\"\n    # read in data\n    df_panda = spark.read.option(\"header\", True).csv(path).toPandas() # The usage should be the same as previous\n    df_panda=df_panda[['ds', 'group_id', 'query_idx', 'SearchKeyValue',  'LocationKeyValue', 'daily_supply']]\n    #df_panda=spark.read.csv(\"gs://adtech-artifacts/ds/forecast/query_taxonomy_sample_Addedfeatures_cluster_\"+str(i)+\".csv\",header=True).toPandas()\n    Cols=['daily_supply']#, 'shifted_annual_daily_supply','Trend_Var','dayofyear_sin', 'dayofyear_cos', 'month_sin', 'month_cos']\n    for cols in Cols:  \n        df_panda[cols]=df_panda[cols].astype(float,errors='raise')\n    start=time.time()\n    minError=1e10\n    param = {\n        \"learning_rate\": [.1,.01,.001],\n        \"hidden_size\": [10,20],\n        'batch_size': [32],\n        'dropout': [0.05,.1],\n        'gamma': [0.9],\n        'optimizer_name': ['AdamW'],\n        'num_layer': [1,2],\n        'relu': [False]\n    }\n    prep = dataprep(df_panda, input_window=90, output_window=30, stride=1,start=start_month[month])\n    print('Beginning the training...')\n    train_set_in, train_set_out, val_set_in, val_set_out, test_in_set, test_out_set,df_test_org,  q,wq = prep._get_data()\n    #print('print len of q, value and wq value',len(q),q[0],wq[0])\n    train_set_in = torch.from_numpy(train_set_in.astype(np.float64)).float()\n    train_set_out = torch.from_numpy(train_set_out.astype(np.float64)).float()\n    val_set_in = torch.from_numpy(val_set_in.astype(np.float64)).float()\n    val_set_out = torch.from_numpy(val_set_out.astype(np.float64)).float()\n    test_in_set = torch.from_numpy(test_in_set.astype(np.float64)).float()\n    test_out_set = torch.from_numpy(test_out_set.astype(np.float64)).float()\n    print('val shape is: ',val_set_in.shape, val_set_out.shape)\n    end1=time.time()\n    if len(train_set_in.shape) < 3:\n        train_set_in = train_set_in.reshape(train_set_in.shape[0], train_set_in.shape[1], 1)\n        val_set_in = val_set_in.reshape(val_set_in.shape[0], val_set_in.shape[1], 1)\n        test_in_set = test_in_set.reshape(test_in_set.shape[0], test_in_set.shape[1], 1)\n        test_out_set=test_out_set.reshape(test_out_set.shape[0], test_out_set.shape[1])\n    counter=1\n    for lr in param['learning_rate']:\n        for h_s in param['hidden_size']:\n            for b_s in param['batch_size']:\n                for dp in param['dropout']:\n                    for opt in param['optimizer_name']:\n                        for n_l in param['num_layer']:\n                            for act in param['relu']:\n                                for gm in param['gamma']:\n                                    config = {\n                                        \"num_epochs\": 200,\n                                        \"learning_rate\": lr,\n                                        \"hidden_size\": h_s,\n                                        \"window_size\": 30,\n                                        \"output_size\": 30,\n                                        'batch_size': b_s,\n                                        'dropout': dp,\n                                        'dropout1': 0,\n                                        'gamma': gm,\n                                        'optimizer_name': opt,\n                                        'num_layer': n_l,\n                                        'relu': act\n                                    }\n                                    print('trial run: ',counter)\n                                    counter+=1\n                                    e1,e2,e3,model = running_lstm(config, save_model=True)\n                                    if (e1+e2+e3)/3<minError:\n                                        minError=(e1+e2+e3)/3\n                                        best_config=config\n                                        best_model=model\n                                        print('the  smape is {} wape is {} and pe is {} and total is {}'.format(e1,e2,e3,(e1+e2+e3)/3))\n                                        print('the config is {}',config)\n    if minError>=.7:\n        param = {\n        \"learning_rate\": [.001,.0001],\n        \"hidden_size\": [15,30],\n        'batch_size': [32],\n        'dropout': [.05,.5],\n        'gamma': [0.9],\n        'optimizer_name': ['Adam','AdamW'],\n        'num_layer': [3,5],\n        'relu': [True]}\n        for lr in param['learning_rate']:\n            for h_s in param['hidden_size']:\n                for b_s in param['batch_size']:\n                    for dp in param['dropout']:\n                        for opt in param['optimizer_name']:\n                            for n_l in param['num_layer']:\n                                for act in param['relu']:\n                                    for gm in param['gamma']:\n                                        config = {\n                                            \"num_epochs\": 200,\n                                            \"learning_rate\": lr,\n                                            \"hidden_size\": h_s,\n                                            \"window_size\": 30,\n                                            \"output_size\": 30,\n                                            'batch_size': b_s,\n                                            'dropout': dp,\n                                            'dropout1': 0,\n                                            'gamma': gm,\n                                            'optimizer_name': opt,\n                                            'num_layer': n_l,\n                                            'relu': act\n                                        }\n                                        print('trial run: ',counter)\n                                        counter+=1\n                                        e1,e2,e3,model = running_lstm(config, save_model=True)\n                                        \n                                        #print('the config is {}',config)\n                                        if (e1+e2+e3)/3<minError:\n                                            minError=(e1+e2+e3)/3\n                                            best_config=config\n                                            best_model=model\n                                            print(best_model)\n                                            print('the  smape is {} wape is {} and pe is {} and total is {}'.format(e1,e2,e3,(e1+e2+e3)/3))\n                                            print('the config is {}',config)\n\n    end = time.time()\n    print('thus best config is', best_config)\n    Wsmape, Wape, Wpe, smape_dis, wape_dis, pe_dis, smape_agg, wape_agg, pe_agg, model, df_lstm[:,i],wape_agg_lstm[i] = check_lstm(best_config, best_model)\n    print('val wape is: ',wape_agg_lstm[i])\n    print('total time ', end1 - end)\n    print('test Wsmape: ', Wsmape, ' test Wape: ', Wape, ' test Wpe: ', Wpe)\n    print('Smape dis',smape_dis,'Wape dis',wape_dis,'PE dis',pe_dis)\n    print('Agg SMAPE: ',smape_agg,'Agg WAPE: ',wape_agg,'Agg PE: ',pe_agg)\n    print('total time ', end1 - end)\n\n    #print('time needed to run the model: ',end-start)\n    AggSmapeL.append(smape_agg)\n    AggWapeL.append(wape_agg)\n    AggPeL.append(pe_agg)\n    SMAPEsL.append(Wsmape)\n    WAPEsL.append(Wape)\n    PEsL.append(Wpe)\n    median_smape.append(smape_dis[0])\n    Eighty_per_smape.append(smape_dis[1])\n    Ninty_per_smape.append(smape_dis[2])\n    median_wape.append(wape_dis[0])\n    Eighty_per_wape.append(wape_dis[1])\n    Ninty_per_wape.append(wape_dis[2])\n    median_pe.append(pe_dis[0])\n    Eighty_per_pe.append(pe_dis[1])\n    Ninty_per_pe.append(pe_dis[2])\n    TimesL.append(end1-end)\n    times_org.append(end1-start)\n    print('===============================================')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "print('TimesL=',TimesL)\nprint('SMAPEsL=',SMAPEsL)\nprint('PEsL=',PEsL)\nprint('WAPEsL=',WAPEsL)\nprint('AggSmapeL=',AggSmapeL)\nprint('AggWapeL=',AggWapeL)\nprint('AggPeL=',AggPeL)\nprint('times_org=',times_org)\nprint('median_smape=',median_smape)\nprint('median_wape=',median_wape)\nprint('median_pe=',median_pe)\nprint('Eighty_per_smape=',Eighty_per_smape)\nprint('Eighty_per_wape=',Eighty_per_wape)\nprint('Eighty_per_pe=',Eighty_per_pe)\nprint('Ninty_per_smape=',Ninty_per_smape)\nprint('Ninty_per_wape=',Ninty_per_wape)\nprint('Ninty_per_pe=',Ninty_per_pe)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "Times=[]\nSMAPEs=[]\nWAPEs=[]\nPEs=[]\nAggSmape=[]\nAggWape=[]\nAggPe=[]\nSMAPEs_dist=[]\nWAPEs_dist=[]\nPEs_dist=[]\ntraining_j=[]\n\n\nfor i in range(50):\n    st=time.time()\n    print('cluster id: ',i+1)\n    current_date = '20220831'\n    test_group_id = test_group_id_list[i]#'/3944/1060825/447913'\n    print('group id',test_group_id)\n    #for top cluster:\n    path = f\"gs://adtech-artifacts/ds/forecast/month_eval/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_tft_data_{current_date}.csv\"\n    #path = f\"gs://adtech-artifacts/ds/forecast/month_eval/tail_clusters/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_data_20220831.csv\"\n    # read in data\n    df = spark.read.option(\"header\", True).csv(path).toPandas() # The usage should be the same as previous\n    df=df[['ds', 'group_id', 'query_idx', 'SearchKeyValue',  'LocationKeyValue', 'daily_supply']]\n    #print('query id: ',df['query_idx'].unique())\n    #df_panda=spark.read.csv(\"gs://adtech-artifacts/ds/forecast/query_taxonomy_sample_Addedfeatures_cluster_\"+str(i)+\".csv\",header=True).toPandas()\n    Cols=['daily_supply']#, 'shifted_annual_daily_supply','Trend_Var','dayofyear_sin', 'dayofyear_cos', 'month_sin', 'month_cos']\n    for cols in Cols:df[cols]=df[cols].astype(float,errors='raise')\n    #print('time needed to preprocess the data is: ',time.time()-st)\n    df_train,df_test,train_date,test_date,q,wq=_generate_train_test_data(df,start=start_month[month])\n    df_trainW,df_valW,_,train_dateW,_,_,_=_generate_train_test_data(df,val_req=True,start=start_month[month])\n    df1_train,df1_test=df_train.sum(axis=1),df_test.sum(axis=1)\n    df1_trainW,df_valW=df_trainW.sum(axis=1),df_valW.sum(axis=1)\n    df1_train,df1_trainW,df_valW=df1_train.to_frame(),df1_trainW.to_frame(),df_valW.to_frame()\n    df1_train.index=train_date\n    df1_trainW.index=train_dateW\n    df1_train.index = df1_train.index.to_period(freq = 'D')\n    df1_trainW.index = df1_trainW.index.to_period(freq = 'D')\n    #df_test_log_temp.index = df_test_log_temp.index.to_period(freq = 'D')\n    df1_train=df1_train.rename(columns={0:'daily_supply'})\n    df1_trainW=df1_trainW.rename(columns={0:'daily_supply'})\n#     if i==0:\n#         print(q,wq)\n    start=time.time()\n    Models=statModels()\n\n    Models.clus_smape['Lstm']=AggSmapeL[i]\n    Models.clus_wape['Lstm']=AggWapeL[i]\n    Models.clus_pe['Lstm']=AggPeL[i]\n    #Models.models['Lstm'] = fit4\n    #self.stores_mape['Es'] = mape\n    Models.stores_smape['Lstm'] = SMAPEsL[i]\n    Models.store_wape['Lstm'] = WAPEsL[i]\n    Models.store_pe['Lstm'] = PEsL[i]\n    Models.smape_dis['Lstm']=[median_smape[i],Eighty_per_smape[i],Ninty_per_smape[i]]\n    Models.wape_dis['Lstm']=[median_wape[i],Eighty_per_wape[i],Ninty_per_wape[i]]\n    Models.pe_dis['Lstm']=[median_pe[i],Eighty_per_pe[i],Ninty_per_pe[i]]\n    Models.stores_times['Lstm'] = TimesL[i]\n    ##################\n    #Models.wape_Weighted['Lstm'] = wape_agg_lstm[i]\n\n    Models.training_infer(df1_train,df1_test,df_test,q.values,wq.values,df1_test.shape[0],df_lstm[:,i])#no_ts\n    Models.weighted_avg(df1_trainW, df1_test,df_valW.values, df_test,q.values,wq.values, forecast_period=60,lstm_pred=df_lstm[:,i])\n\n    end=time.time()\n    print('Wape is: ',Models.store_wape)\n    print('Smape is: ',Models.stores_smape)\n    print('Time is:' ,Models.stores_times)\n    #print('time needed to run the model: ',end-start)\n    SMAPEs.append(Models.stores_smape)\n    WAPEs.append(Models.store_wape)\n    PEs.append(Models.store_pe)\n    Times.append(Models.stores_times)\n    AggSmape.append(Models.clus_smape)\n    AggWape.append(Models.clus_wape)\n    AggPe.append(Models.clus_pe)\n    SMAPEs_dist.append(Models.smape_dis)\n    WAPEs_dist.append(Models.wape_dis)\n    PEs_dist.append(Models.pe_dis)\n    training_j.append(i)\n\n    print('===============================================')\n    "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "Smape={i:[] for i in SMAPEs[0]}\nWape={i:[] for i in WAPEs[0]}\nPe={i:[] for i in PEs[0]}\nTime={i:[] for i in Times[0]}\n\nAggSmapes={i:[] for i in AggSmape[0]}\nAggWapes={i:[] for i in AggWape[0]}\nAggPes={i:[] for i in AggPe[0]}\n\nSMAPE_dist_median={i:[] for i in SMAPEs_dist[0]}\nSMAPE_dist_80={i:[] for i in SMAPEs_dist[0]}\nSMAPE_dist_90={i:[] for i in SMAPEs_dist[0]}\n\nWAPE_dist_median={i:[] for i in WAPEs_dist[0]}\nWAPE_dist_80={i:[] for i in WAPEs_dist[0]}\nWAPE_dist_90={i:[] for i in WAPEs_dist[0]}\n\n\nPE_dist_median={i:[] for i in PEs_dist[0]}\nPE_dist_80={i:[] for i in PEs_dist[0]}\nPE_dist_90={i:[] for i in PEs_dist[0]}\nfor i in range(len(training_j)):\n    for keys,values in SMAPEs[i].items():\n        Smape[keys].append(values)\n    for keys,values in WAPEs[i].items():\n        Wape[keys].append(values)\n    for keys,values in PEs[i].items():\n        Pe[keys].append(values)\n\n    for keys,values in Times[i].items():\n        Time[keys].append(values)\n    for keys,values in AggSmape[i].items():\n        AggSmapes[keys].append(values)\n    for keys,values in AggWape[i].items():\n        AggWapes[keys].append(values)\n    for keys,values in AggPe[i].items():\n        AggPes[keys].append(values)\n    \n    for keys,values in SMAPEs_dist[i].items():\n        SMAPE_dist_median[keys].append(values[0])\n    for keys,values in SMAPEs_dist[i].items():\n        SMAPE_dist_80[keys].append(values[1])\n    for keys,values in SMAPEs_dist[i].items():\n        SMAPE_dist_90[keys].append(values[2])\n        \n    for keys,values in WAPEs_dist[i].items():\n        WAPE_dist_median[keys].append(values[0])\n    for keys,values in WAPEs_dist[i].items():\n        WAPE_dist_80[keys].append(values[1])\n    for keys,values in WAPEs_dist[i].items():\n        WAPE_dist_90[keys].append(values[2])\n        \n    for keys,values in PEs_dist[i].items():\n        PE_dist_median[keys].append(values[0])\n    for keys,values in PEs_dist[i].items():\n        PE_dist_80[keys].append(values[1])\n    for keys,values in PEs_dist[i].items():\n        PE_dist_90[keys].append(values[2])\nprint('Smape is: ',Smape)\nprint('====================')\nprint('Wape is: ',Wape)\nprint('====================')\nprint('Pe is: ',Pe)\nprint('====================')\nprint('Time is: ',Time)\nprint('====================')\nprint('AggSmapes is: ',AggSmapes)\nprint('====================')\nprint('AggWapes is: ',AggWapes)\nprint('====================')\nprint('AggPes is: ',AggPes)\nprint('====================')\nprint('SMAPE_dist_median is: ',SMAPE_dist_median)\nprint('====================')\nprint('SMAPE_dist_80 is: ',SMAPE_dist_80)\nprint('====================')\nprint('SMAPE_dist_90 is: ',SMAPE_dist_90)\nprint('====================')\nprint('WAPE_dist_median is: ',WAPE_dist_median)\nprint('====================')\nprint('WAPE_dist_80 is: ',WAPE_dist_80)\nprint('====================')\nprint('WAPE_dist_90 is: ',WAPE_dist_90)\nprint('====================')\nprint('PE_dist_median is: ',PE_dist_median)\nprint('====================')\nprint('PE_dist_80 is: ',PE_dist_80)\nprint('====================')\nprint('SmapePE_dist_90 is: ',PE_dist_90)\nprint('====================')\nprint('Original LSTM time is', times_org)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.13"}}, "nbformat": 4, "nbformat_minor": 2}