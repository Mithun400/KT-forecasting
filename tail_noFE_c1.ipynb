{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://repository.walmart.com/repository/pypi-proxy/simple/\n",
      "Requirement already satisfied: optuna in /opt/conda/anaconda/lib/python3.6/site-packages (2.10.1)\n",
      "Requirement already satisfied: cliff in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (3.10.1)\n",
      "Requirement already satisfied: colorlog in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (6.7.0)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (5.4.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (1.19.5)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (1.4.22)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (21.3)\n",
      "Requirement already satisfied: cmaes>=0.8.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (0.8.2)\n",
      "Requirement already satisfied: alembic in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (1.5.7)\n",
      "Requirement already satisfied: tqdm in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (4.64.0)\n",
      "Requirement already satisfied: scipy!=1.4.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from optuna) (1.2.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from packaging>=20.0->optuna) (3.0.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/anaconda/lib/python3.6/site-packages (from sqlalchemy>=1.1.0->optuna) (1.1.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/anaconda/lib/python3.6/site-packages (from sqlalchemy>=1.1.0->optuna) (3.10.1)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/anaconda/lib/python3.6/site-packages (from alembic->optuna) (2.8.2)\n",
      "Requirement already satisfied: Mako in /opt/conda/anaconda/lib/python3.6/site-packages (from alembic->optuna) (1.1.4)\n",
      "Requirement already satisfied: python-editor>=0.3 in /opt/conda/anaconda/lib/python3.6/site-packages (from alembic->optuna) (1.0.4)\n",
      "Requirement already satisfied: stevedore>=2.0.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from cliff->optuna) (3.5.0)\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from cliff->optuna) (2.5.0)\n",
      "Requirement already satisfied: cmd2>=1.0.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from cliff->optuna) (2.4.2)\n",
      "Requirement already satisfied: autopage>=0.4.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from cliff->optuna) (0.5.1)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from cliff->optuna) (5.10.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /opt/conda/anaconda/lib/python3.6/site-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
      "Requirement already satisfied: attrs>=16.3.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from cmd2>=1.0.0->cliff->optuna) (18.1.0)\n",
      "Requirement already satisfied: pyperclip>=1.6 in /opt/conda/anaconda/lib/python3.6/site-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/anaconda/lib/python3.6/site-packages (from cmd2>=1.0.0->cliff->optuna) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from Mako->alembic->optuna) (2.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from python-dateutil->alembic->optuna) (1.16.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/anaconda/lib/python3.6/site-packages (from tqdm->optuna) (5.4.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://repository.walmart.com/repository/pypi-proxy/simple/\n",
      "Requirement already satisfied: pmdarima in /opt/conda/anaconda/lib/python3.6/site-packages (1.2.1)\n",
      "Requirement already satisfied: scikit-learn>=0.19 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima) (0.24.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima) (1.1.1)\n",
      "Requirement already satisfied: Cython>=0.29 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima) (0.29.32)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima) (1.16.0)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima) (0.12.2)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima) (1.19.5)\n",
      "Requirement already satisfied: pandas>=0.19 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima) (1.1.5)\n",
      "Requirement already satisfied: scipy<1.3,>=1.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima) (1.2.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from pandas>=0.19->pmdarima) (2022.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/anaconda/lib/python3.6/site-packages (from pandas>=0.19->pmdarima) (2.8.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from scikit-learn>=0.19->pmdarima) (3.1.0)\n",
      "Requirement already satisfied: patsy>=0.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from statsmodels>=0.9.0->pmdarima) (0.5.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sktime 0.9.0 requires numpy<=1.19.3, but you have numpy 1.19.5 which is incompatible.\n",
      "sktime 0.9.0 requires statsmodels<=0.12.1, but you have statsmodels 0.12.2 which is incompatible.\u001b[0m\n",
      "Looking in indexes: https://repository.walmart.com/repository/pypi-proxy/simple/\n",
      "Requirement already satisfied: yellowbrick==0.9.1 in /opt/conda/anaconda/lib/python3.6/site-packages (0.9.1)\n",
      "Collecting scikit-learn==0.22.2\n",
      "  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/scikit-learn/scikit_learn-0.22.2-cp36-cp36m-manylinux1_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2UxLzdmLzM2NmRjYmExYmEwNzZhODhhNTBiZWE3MzJkYmMwMzNjMGM1YmJmNzg3NjAxMGU2ZWRjNjc5NDg1NzlkNS9zY2lraXRfbGVhcm4tMC4yMi4yLWNwMzYtY3AzNm0tbWFueWxpbnV4MV94ODZfNjQud2hsI3NoYTI1Nj1iYzhiMzdjYTBkZTc5NDgzODM3NmNmZmRkYWI2OWE0NmE5YTU0YzQzMzA4MDBlYTFmOTNjYWI2MWNhNjdkZmEz (7.1 MB)\n",
      "Requirement already satisfied: cycler>=0.10.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from yellowbrick==0.9.1) (0.10.0)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from yellowbrick==0.9.1) (1.2.0)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=1.5.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from yellowbrick==0.9.1) (2.2.4)\n",
      "Requirement already satisfied: numpy>=1.13.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from yellowbrick==0.9.1) (1.19.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/anaconda/lib/python3.6/site-packages (from scikit-learn==0.22.2) (1.1.1)\n",
      "Requirement already satisfied: six in /opt/conda/anaconda/lib/python3.6/site-packages (from cycler>=0.10.0->yellowbrick==0.9.1) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=1.5.1->yellowbrick==0.9.1) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=1.5.1->yellowbrick==0.9.1) (3.0.7)\n",
      "Requirement already satisfied: pytz in /opt/conda/anaconda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=1.5.1->yellowbrick==0.9.1) (2022.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=1.5.1->yellowbrick==0.9.1) (1.0.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/anaconda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=1.5.1->yellowbrick==0.9.1) (59.6.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.2\n",
      "    Uninstalling scikit-learn-0.24.2:\n",
      "      Successfully uninstalled scikit-learn-0.24.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sktime 0.9.0 requires numpy<=1.19.3, but you have numpy 1.19.5 which is incompatible.\n",
      "sktime 0.9.0 requires scikit-learn>=0.24.0, but you have scikit-learn 0.22.2 which is incompatible.\n",
      "sktime 0.9.0 requires statsmodels<=0.12.1, but you have statsmodels 0.12.2 which is incompatible.\u001b[0m\n",
      "Successfully installed scikit-learn-0.22.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://repository.walmart.com/repository/pypi-proxy/simple/\n",
      "Collecting sktime\n",
      "  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/sktime/sktime-0.9.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzczLzcyLzg0MGMzMmI0ZGJhMjM2Y2ZkYmQzZDA0NmJjNTUzZDNlY2ZlNDQwNzliY2VkZTNlYThmMDAwY2E0Mzk2ZC9za3RpbWUtMC45LjAtY3AzNi1jcDM2bS1tYW55bGludXhfMl8xN194ODZfNjQubWFueWxpbnV4MjAxNF94ODZfNjQud2hsI3NoYTI1Nj0zZDgwY2ZhZjgwY2NhNjA5M2UwYjg2MjQxMTA5ZDgzNmQ2ZWJlMGE1NzAyNDBkMTliNDE1MmY2NDc5MzVjMTdh (6.2 MB)\n",
      "Collecting numpy<=1.19.3\n",
      "  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/numpy/numpy-1.19.3-cp36-cp36m-manylinux2010_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzBlL2Y3L2E3ZDdlMGRlOTlhN2I0M2JkOTVhYWRkY2YyOWU2NWI1YTE4NWNhMzg5ZGQxMzI5YTUzY2M5ODZlZGMzOC9udW1weS0xLjE5LjMtY3AzNi1jcDM2bS1tYW55bGludXgyMDEwX3g4Nl82NC53aGwjc2hhMjU2PTcxOTdlZTBhMjU2MjllZDc4MmM3YmQwMTg3MWVlNDA3MDJmZmVlZjM1YmM0ODAwNGJjMmZkY2M3MWUyOWJhOWQ= (14.9 MB)\n",
      "Collecting deprecated>=1.2.13\n",
      "  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/deprecated/Deprecated-1.2.13-py2.py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzUxLzZhL2MzYTA0MDg2NDY0MDhmNzI4M2I3YmM1NTBjMzBhMzJjYzc5MTE4MWVjNDYxODU5MmVlYzEzZTA2NmNlMy9EZXByZWNhdGVkLTEuMi4xMy1weTIucHkzLW5vbmUtYW55LndobCNzaGEyNTY9NjQ3NTZlM2UxNGM4YzVlZWE5Nzk1ZDkzYzUyNDU1MTQzMmEwYmU3NTYyOWY4ZjI5ZTY3YWI4Y2FmMDc2Yzc2ZA== (9.6 kB)\n",
      "Collecting statsmodels<=0.12.1\n",
      "  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/statsmodels/statsmodels-0.12.1-cp36-cp36m-manylinux1_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2JlLzRjLzllMjQzNWNhNjY0NWQ2YmFmYTJiNTFiYjExZjBhMzY1YjI4OTM0YTJmZmU5ZDZlMzM5ZDY3MTMwOTI2ZC9zdGF0c21vZGVscy0wLjEyLjEtY3AzNi1jcDM2bS1tYW55bGludXgxX3g4Nl82NC53aGwjc2hhMjU2PTE0MmVhY2Q1YTFiZDg3MjgzNThmZjQ4MTAxZWUwZTUxY2EzZDQyYTkzZjZlNWNiNjFmY2ZhY2Y2MTM5NzdiY2Y= (9.5 MB)\n",
      "Collecting wheel\n",
      "  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/wheel/wheel-0.37.1-py2.py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzI3L2Q2LzAwM2U1OTMyOTZhODVmZDZlZDYxNmVkOTYyNzk1YjJmODc3MDljM2VlZTJiY2E0ZjZkMGZlNTVjNmQwMC93aGVlbC0wLjM3LjEtcHkyLnB5My1ub25lLWFueS53aGwjc2hhMjU2PTRiZGNkN2Q4NDAxMzgwODYxMjZjZDA5MjU0ZGM2MTk1ZmI0ZmM2ZjAxYzA1MGExZDcyMzZmMjYzMGRiMWQyMmE= (35 kB)\n",
      "Collecting scikit-learn>=0.24.0\n",
      "  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/scikit-learn/scikit_learn-0.24.2-cp36-cp36m-manylinux2010_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2QzL2ViL2QwZTY1ODQ2NWMwMjlmZWI3MDgzMTM5ZDllYWQ1MTAwMDc0MmU4OGIxZmI3ZjE1MDRlMTllMWI0Y2U2ZS9zY2lraXRfbGVhcm4tMC4yNC4yLWNwMzYtY3AzNm0tbWFueWxpbnV4MjAxMF94ODZfNjQud2hsI3NoYTI1Nj01ZmYzZTRlNGNmNzU5MmQzNjU0MWVkZWM0MzRlMDlmYjhhYjliYTZiNDc2MDhjNGZmZTMwYzkwMzhkMzAxODk3 (22.2 MB)\n",
      "Collecting numba>=0.53\n",
      "  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/numba/numba-0.53.1-cp36-cp36m-manylinux2014_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzRhL2MxL2U3ZmRiZmM4ODZhOWQ5YzExNzY3NTMzOTAzZGIwZDgxNmMwZjY1NmZkNjAyOWY0YTA2MTc0Mjg5MzY5NC9udW1iYS0wLjUzLjEtY3AzNi1jcDM2bS1tYW55bGludXgyMDE0X3g4Nl82NC53aGwjc2hhMjU2PThmYTVjOTYzYTQzODU1MDUwYTg2ODEwNmE4N2NkNjE0ZjNjM2Y0NTk5NTFjOGZjNDY4YWVjMjYzZWY4MGQwNjM= (3.4 MB)\n",
      "Collecting pandas>=1.1.0\n",
      "  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/pandas/pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2MzL2UyLzAwY2FjZWNhZmJhYjA3MWM3ODcwMTlmMDBhZDg0Y2EzMTg1OTUyZjZiYjliY2E5NTUwZWQ4Mzg3MGQ0ZC9wYW5kYXMtMS4xLjUtY3AzNi1jcDM2bS1tYW55bGludXgxX3g4Nl82NC53aGwjc2hhMjU2PWI2MTA4MDc1MGQxOWEwMTIyNDY5YWI1OWIwODczODA3MjFkNmI3MmE0ZTdkOTYyZTRkN2U2M2UwYzQ1MDQ4MTQ= (9.5 MB)\n",
      "Collecting wrapt<2,>=1.10\n",
      "  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/wrapt/wrapt-1.14.1-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2UwLzgwL2FmOWRhNzM3OWVlNmRmNTgzODc1ZDBhZWI4MGY5ZDVmMGJkNWYwODFkZDFlZTVjZTA2NTg3ZDhiZmVjNy93cmFwdC0xLjE0LjEtY3AzNi1jcDM2bS1tYW55bGludXhfMl81X3g4Nl82NC5tYW55bGludXgxX3g4Nl82NC5tYW55bGludXhfMl8xN194ODZfNjQubWFueWxpbnV4MjAxNF94ODZfNjQud2hsI3NoYTI1Nj0yMWFjMDE1NmM0YjA4OWIzMzBiNzY2NmRiNDBmZWVlMzBhNWQ1MjYzNGNjNDU2MGUxOTA1ZDY1MjlhMzg5N2Zm (74 kB)\n",
      "Collecting llvmlite<0.37,>=0.36.0rc1\n",
      "  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/llvmlite/llvmlite-0.36.0-cp36-cp36m-manylinux2010_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzRkLzVhLzcwN2NjN2UwNzJkNzFiYzE5ODY5ZDA5M2U1Y2Y5YjdiZTk4Y2I0MmQyMzk4NDg5NDY1NDc0ZDAwN2NlOC9sbHZtbGl0ZS0wLjM2LjAtY3AzNi1jcDM2bS1tYW55bGludXgyMDEwX3g4Nl82NC53aGwjc2hhMjU2PTc3Njg2NTg2NDZjNDE4YjliM2JlY2NiNzA0NDI3N2E2MDhiYzhjNjJiODJhODVlNzNjN2U1YzA2NWU0MTU3YzI= (25.3 MB)\n",
      "Collecting setuptools\n",
      "  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/setuptools/setuptools-59.6.0-py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2IwLzNhLzg4YjIxMGRiNjhlNTY4NTRkMGJjZjRiMzhlMTY1ZTAzYmUzNzdlMTM5MDc3NDZmODI1NzkwZjNkZjViZi9zZXR1cHRvb2xzLTU5LjYuMC1weTMtbm9uZS1hbnkud2hsI3NoYTI1Nj00Y2U5MmYxZTFmOGYwMTIzM2VlOTk1MmMwNGY2YjgxZDFlMDI5MzlkNmUxYjQ4ODQyODE1NDk3NGE0ZDA3ODNl (952 kB)\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/python-dateutil/python_dateutil-2.8.2-py2.py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzM2LzdhLzg3ODM3ZjM5ZDAyOTZlNzIzYmI5YjYyYmJiMjU3ZDAzNTVjN2Y2MTI4ODUzYzc4OTU1ZjU3MzQyYTU2ZC9weXRob25fZGF0ZXV0aWwtMi44LjItcHkyLnB5My1ub25lLWFueS53aGwjc2hhMjU2PTk2MWQwM2RjMzQ1M2ViYmM1OWRiZGVhOWU0ZTExYzU2NTE1MjBhODc2ZDBmNGRiMTYxZTg2NzRhYWU5MzVkYTk= (247 kB)\n",
      "Collecting pytz>=2017.2\n",
      "  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/pytz/pytz-2022.4-py2.py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2Q4LzY2LzMwOTU0NTQxMzE2MmJjODI3MWM3M2U2MjI0OTlhNDFjZGMzNzIxN2I0OTlmZWJlMjYxNTVmZjlmOTNlZC9weXR6LTIwMjIuNC1weTIucHkzLW5vbmUtYW55LndobCNzaGEyNTY9MmMwNzg0NzQ3MDcxNDAyYzZlOTlmMGJhZmRiN2RhMGZhMjI2NDVmMDY1NTRjN2FlMDZiZjYzNTg4OTdlOWM5MQ== (500 kB)\n",
      "Collecting six>=1.5\n",
      "  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/six/six-1.16.0-py2.py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2Q5LzVhL2U3YzMxYWRiZTg3NWYyYWJiYjkxYmQ4NGNmMmRjNTJkNzkyYjVhMDE1MDY3ODFkYmNmMjVjOTFkYWYxMS9zaXgtMS4xNi4wLXB5Mi5weTMtbm9uZS1hbnkud2hsI3NoYTI1Nj04YWJiMmYxZDg2ODkwYTJkZmI5ODlmOWE3N2NmY2ZkM2U0N2MyYTM1NGIwMTExMTc3MTMyNmY4YWEyNmUwMjU0 (11 kB)\n",
      "Collecting joblib>=0.11\n",
      "  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/joblib/joblib-1.1.1-py2.py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzdjLzkxL2QzYmEwNDAxZTYyZDdlNDI4MTZiYzdkOTdiODJkMTljOTVjMTY0YjNlMTQ5YTg3YzBhMWMwMjZhNzM1ZS9qb2JsaWItMS4xLjEtcHkyLnB5My1ub25lLWFueS53aGwjc2hhMjU2PWY5ZDZjM2NkZjJhNzc3OGU5MDU4ZTEwZTlkYmEwMjhlNDc3NzFhMWEzNTVlNTc2OGY0NjcwNGJmMDUzNDJlYmE= (309 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/threadpoolctl/threadpoolctl-3.1.0-py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzYxL2NmLzZlMzU0MzA0YmNiOWM2NDEzYzRlMDJhNzQ3YjYwMDA2MWMyMWQzOGJhNTFlN2U1NDRhYzdiYzY2YWVjYy90aHJlYWRwb29sY3RsLTMuMS4wLXB5My1ub25lLWFueS53aGwjc2hhMjU2PThiOTlhZGRhMjY1ZmViNjc3MzI4MGRmNDFlZWNlN2IyZTY1NjFiNzcyZDIxZmZkNTJlMzcyZjk5OTAyNDkwN2I= (14 kB)\n",
      "Collecting scipy>=0.19.1\n",
      "  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/scipy/scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2M4Lzg5LzYzMTcxMjI4ZDVjZWQxNDhmNWNlZDUwMzA1Yzg5ZTg1NzZmZmM2OTVhOTBiNThmZTViYjYwMmI5MTBjMi9zY2lweS0xLjUuNC1jcDM2LWNwMzZtLW1hbnlsaW51eDFfeDg2XzY0LndobCNzaGEyNTY9MzY4YzBmNjlmOTMxODYzMDllMWI0YmViOGUyNmQ1MWRkNmY1MDEwYjc5MjY0YzBmMWU5Y2EwMGNkOTJlYThjOQ== (25.9 MB)\n",
      "Collecting patsy>=0.5\n",
      "  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/patsy/patsy-0.5.3-py2.py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzJhL2U0L2IzMjYzYjBlMzUzZjJiZTdiMTRmMDQ0ZDU3ODc0NDkwYzljZWYxNzk4YTQzNWYwMzg2ODNhY2VhNWM5OC9wYXRzeS0wLjUuMy1weTIucHkzLW5vbmUtYW55LndobCNzaGEyNTY9N2ViNTM0OTc1NGVkNmFhOTgyYWY4MWY2MzY0NzliMWI4ZGI5ZDViMWE2ZTk1N2E2MDE2ZWMwNTM0YjVjODZiNw== (233 kB)\n",
      "Installing collected packages: six, pytz, python-dateutil, numpy, wrapt, threadpoolctl, setuptools, scipy, patsy, pandas, llvmlite, joblib, wheel, statsmodels, scikit-learn, numba, deprecated, sktime\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "distributed 1.21.8 requires msgpack, which is not installed.\n",
      "pmdarima 1.2.1 requires scipy<1.3,>=1.2, but you have scipy 1.5.4 which is incompatible.\u001b[0m\n",
      "Successfully installed deprecated-1.2.13 joblib-1.1.1 llvmlite-0.36.0 numba-0.53.1 numpy-1.19.5 pandas-1.1.5 patsy-0.5.3 python-dateutil-2.8.2 pytz-2022.4 scikit-learn-0.24.2 scipy-1.5.4 setuptools-59.6.0 six-1.16.0 sktime-0.9.0 statsmodels-0.12.2 threadpoolctl-3.1.0 wheel-0.37.1 wrapt-1.14.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://repository.walmart.com/repository/pypi-proxy/simple/\n",
      "Requirement already satisfied: delayed in /opt/conda/anaconda/lib/python3.6/site-packages (0.11.0b1)\n",
      "Requirement already satisfied: hiredis in /opt/conda/anaconda/lib/python3.6/site-packages (from delayed) (2.0.0)\n",
      "Requirement already satisfied: redis in /opt/conda/anaconda/lib/python3.6/site-packages (from delayed) (4.3.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/anaconda/lib/python3.6/site-packages (from redis->delayed) (3.10.0.2)\n",
      "Requirement already satisfied: packaging>=20.4 in /opt/conda/anaconda/lib/python3.6/site-packages (from redis->delayed) (21.3)\n",
      "Requirement already satisfied: importlib-metadata>=1.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from redis->delayed) (3.10.1)\n",
      "Requirement already satisfied: async-timeout>=4.0.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from redis->delayed) (4.0.2)\n",
      "Requirement already satisfied: deprecated>=1.2.3 in /opt/conda/anaconda/lib/python3.6/site-packages (from redis->delayed) (1.2.13)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/anaconda/lib/python3.6/site-packages (from deprecated>=1.2.3->redis->delayed) (1.14.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from importlib-metadata>=1.0->redis->delayed) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from packaging>=20.4->redis->delayed) (3.0.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://repository.walmart.com/repository/pypi-proxy/simple/\n",
      "Requirement already satisfied: fbprophet in /opt/conda/anaconda/lib/python3.6/site-packages (0.7.1)\n",
      "Requirement already satisfied: Cython>=0.22 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (0.29.32)\n",
      "Requirement already satisfied: cmdstanpy==0.9.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (0.9.5)\n",
      "Requirement already satisfied: pystan>=2.14 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (2.19.1.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (1.19.5)\n",
      "Requirement already satisfied: pandas>=1.0.4 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (1.1.5)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (2.2.4)\n",
      "Requirement already satisfied: LunarCalendar>=0.0.9 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (0.0.9)\n",
      "Requirement already satisfied: convertdate>=2.1.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (2.3.2)\n",
      "Requirement already satisfied: holidays>=0.10.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (0.13)\n",
      "Requirement already satisfied: setuptools-git>=1.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (2.8.2)\n",
      "Requirement already satisfied: tqdm>=4.36.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from fbprophet) (4.64.0)\n",
      "Requirement already satisfied: pytz>=2014.10 in /opt/conda/anaconda/lib/python3.6/site-packages (from convertdate>=2.1.2->fbprophet) (2022.4)\n",
      "Requirement already satisfied: pymeeus<=1,>=0.3.13 in /opt/conda/anaconda/lib/python3.6/site-packages (from convertdate>=2.1.2->fbprophet) (0.5.11)\n",
      "Requirement already satisfied: hijri-converter in /opt/conda/anaconda/lib/python3.6/site-packages (from holidays>=0.10.2->fbprophet) (2.2.4)\n",
      "Requirement already satisfied: korean-lunar-calendar in /opt/conda/anaconda/lib/python3.6/site-packages (from holidays>=0.10.2->fbprophet) (0.3.1)\n",
      "Requirement already satisfied: ephem>=3.7.5.3 in /opt/conda/anaconda/lib/python3.6/site-packages (from LunarCalendar>=0.0.9->fbprophet) (4.1.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from matplotlib>=2.0.0->fbprophet) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/anaconda/lib/python3.6/site-packages (from matplotlib>=2.0.0->fbprophet) (3.0.7)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/anaconda/lib/python3.6/site-packages (from matplotlib>=2.0.0->fbprophet) (1.16.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/anaconda/lib/python3.6/site-packages (from matplotlib>=2.0.0->fbprophet) (0.10.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/anaconda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->fbprophet) (59.6.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/anaconda/lib/python3.6/site-packages (from tqdm>=4.36.1->fbprophet) (5.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from importlib-resources->tqdm>=4.36.1->fbprophet) (3.6.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://repository.walmart.com/repository/pypi-proxy/simple/\n",
      "Requirement already satisfied: tbats in /opt/conda/anaconda/lib/python3.6/site-packages (1.1.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/anaconda/lib/python3.6/site-packages (from tbats) (1.5.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/anaconda/lib/python3.6/site-packages (from tbats) (0.24.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/anaconda/lib/python3.6/site-packages (from tbats) (1.19.5)\n",
      "Requirement already satisfied: pmdarima in /opt/conda/anaconda/lib/python3.6/site-packages (from tbats) (1.2.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima->tbats) (1.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima->tbats) (1.16.0)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima->tbats) (0.12.2)\n",
      "Requirement already satisfied: Cython>=0.29 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima->tbats) (0.29.32)\n",
      "Collecting scipy\n",
      "  Using cached https://repository.walmart.com/repository/pypi-proxy/packages/scipy/scipy-1.2.3-cp36-cp36m-manylinux1_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2M3L2Q2LzY2MWNmNGZiMzJlY2RhNDBmYzAzNjg1ZWYwOTk1ZGIzZDMzMDBjNjE5ZjQ2MTc1MTYyMWY2NDZhYjNmNi9zY2lweS0xLjIuMy1jcDM2LWNwMzZtLW1hbnlsaW51eDFfeDg2XzY0LndobCNzaGEyNTY9YzA4MDEzZTBmZTE1NTQzNzJkYTkzMTJkNWJhZDU4ODQwMmY3MWMwNjM2ZjBmODZhOWI5YjYxZDUwN2M1OWJhYw== (24.8 MB)\n",
      "Requirement already satisfied: pandas>=0.19 in /opt/conda/anaconda/lib/python3.6/site-packages (from pmdarima->tbats) (1.1.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/anaconda/lib/python3.6/site-packages (from pandas>=0.19->pmdarima->tbats) (2022.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/anaconda/lib/python3.6/site-packages (from pandas>=0.19->pmdarima->tbats) (2.8.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/anaconda/lib/python3.6/site-packages (from scikit-learn->tbats) (3.1.0)\n",
      "Requirement already satisfied: patsy>=0.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from statsmodels>=0.9.0->pmdarima->tbats) (0.5.3)\n",
      "Installing collected packages: scipy\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.5.4\n",
      "    Uninstalling scipy-1.5.4:\n",
      "      Successfully uninstalled scipy-1.5.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sktime 0.9.0 requires numpy<=1.19.3, but you have numpy 1.19.5 which is incompatible.\n",
      "sktime 0.9.0 requires statsmodels<=0.12.1, but you have statsmodels 0.12.2 which is incompatible.\u001b[0m\n",
      "Successfully installed scipy-1.2.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://repository.walmart.com/repository/pypi-proxy/simple/\n",
      "Requirement already satisfied: workalendar in /opt/conda/anaconda/lib/python3.6/site-packages (16.4.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/anaconda/lib/python3.6/site-packages (from workalendar) (3.10.1)\n",
      "Requirement already satisfied: lunardate in /opt/conda/anaconda/lib/python3.6/site-packages (from workalendar) (0.2.0)\n",
      "Requirement already satisfied: convertdate in /opt/conda/anaconda/lib/python3.6/site-packages (from workalendar) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/anaconda/lib/python3.6/site-packages (from workalendar) (2.8.2)\n",
      "Requirement already satisfied: backports.zoneinfo in /opt/conda/anaconda/lib/python3.6/site-packages (from workalendar) (0.2.1)\n",
      "Requirement already satisfied: pyluach in /opt/conda/anaconda/lib/python3.6/site-packages (from workalendar) (1.4.2)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/anaconda/lib/python3.6/site-packages (from backports.zoneinfo->workalendar) (5.4.0)\n",
      "Requirement already satisfied: pytz>=2014.10 in /opt/conda/anaconda/lib/python3.6/site-packages (from convertdate->workalendar) (2022.4)\n",
      "Requirement already satisfied: pymeeus<=1,>=0.3.13 in /opt/conda/anaconda/lib/python3.6/site-packages (from convertdate->workalendar) (0.5.11)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from importlib-metadata->workalendar) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/anaconda/lib/python3.6/site-packages (from importlib-metadata->workalendar) (3.10.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/anaconda/lib/python3.6/site-packages (from python-dateutil->workalendar) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna\n",
    "!pip install pmdarima\n",
    "!pip install pmdarima scipy==1.2 -Uqq\n",
    "!pip install yellowbrick==0.9.1 scikit-learn==0.22.2\n",
    "!pip install sktime --ignore-installed\n",
    "!pip install delayed\n",
    "!pip install fbprophet\n",
    "!pip install tbats\n",
    "!pip install workalendar\n",
    "\n",
    "import optuna\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
    "# os.chdir(\"../../..\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import sys, os, time\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from workalendar.core import Calendar\n",
    "from workalendar.registry import registry\n",
    "from pandas.tseries.offsets import BDay\n",
    "import numpy as np\n",
    "from pandas.tseries.holiday import (\n",
    "    AbstractHolidayCalendar, Holiday, DateOffset, \\\n",
    "    SU, MO, TU, WE, TH, FR, SA, \\\n",
    "    next_monday, nearest_workday, sunday_to_monday,\n",
    "    EasterMonday, GoodFriday, Easter\n",
    ")\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error#, mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import statistics\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import re\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"WMX - impr forecast - KT\") \\\n",
    "    .enableHiveSupport()\\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\")\\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\\\n",
    "    .config(\"spark.sql.catalogImplementation\",\"hive\")\\\n",
    "    .config(\"spark.sql.hive.convertMetastoreOrc\", \"false\")\\\n",
    "    .config(\"spark.executor.memory\", \"3G\")\\\n",
    "    .config(\"spark.driver.memory\", \"3G\")\\\n",
    "    .config(\"spark.network.timeout\", \"43200\")\\\n",
    "    .config(\"spark.rdd.compress\", \"true\")\\\n",
    "    .config(\"spark.cores.max\", \"512\")\\\n",
    "    .config(\"spark.default.parallelism\", \"256\")\\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"64\")\\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"2048\")\\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"1G\")\\\n",
    "    .config(\"spark.sql.hive.convertMetastoreOrc\", \"false\")\\\n",
    "    .config(\"spark.sql.execution.arrow.enabled\",\"true\")\\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "    .config(\"spark.driver.maxResultSize\",\"0\")\\\n",
    "    .config(\"spark.memory.offHeap.enabled\",\"true\") \\\n",
    "    .config(\"spark.memory.offHeap.size\",\"1g\")\\\n",
    "    .config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest_2.11.jar')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "1\n",
      "Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
    "# os.chdir(\"../../..\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import sys, os, time\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error#, mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import statistics\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import re\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
    "# os.chdir(\"../../..\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import os,sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import sys, os, time\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error#, mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import statistics\n",
    "\n",
    "import random\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import time\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "import sys, os, time\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from datetime import datetime\n",
    "import statistics\n",
    "\n",
    "import random\n",
    "import time\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os,sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import re\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "from dateutil.relativedelta import relativedelta\n",
    "# from ray import tune\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#le = preprocessing.LabelEncoder()\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "# from ray import tune\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pmdarima as pmd\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n",
    "#import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import sktime\n",
    "import delayed\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.forecasting.ets import AutoETS\n",
    "from warnings import simplefilter\n",
    "\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sktime.forecasting.compose import (\n",
    "    EnsembleForecaster,\n",
    "    MultiplexForecaster,\n",
    "    TransformedTargetForecaster,\n",
    "    make_reduction,\n",
    ")\n",
    "from sktime.forecasting.exp_smoothing import ExponentialSmoothing\n",
    "from sktime.forecasting.model_evaluation import evaluate\n",
    "from sktime.forecasting.model_selection import (\n",
    "    ExpandingWindowSplitter,\n",
    "    ForecastingGridSearchCV,\n",
    "    SlidingWindowSplitter,\n",
    "    temporal_train_test_split,\n",
    ")\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from sktime.forecasting.theta import ThetaForecaster\n",
    "from sktime.forecasting.trend import PolynomialTrendForecaster\n",
    "from sktime.transformations.series.detrend import Deseasonalizer, Detrender\n",
    "from sktime.forecasting.theta import ThetaForecaster\n",
    "from statsmodels.tsa.api import STLForecast\n",
    "from sktime.forecasting.bats import BATS\n",
    "from sktime.forecasting.tbats import TBATS\n",
    "from sktime.forecasting.croston import Croston\n",
    "from statsmodels.tsa.exponential_smoothing.ets import ETSModel\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import sys, os, time\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error#, mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import statistics\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# import darts\n",
    "# from darts import TimeSeries\n",
    "# from darts.metrics import *\n",
    "# from darts.utils.missing_values import fill_missing_values\n",
    "\n",
    "import warnings\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "import os,sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, TimeDistributed, Conv1D, MaxPooling1D, Flatten, Bidirectional, Input, Flatten, Activation, Reshape, RepeatVector, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import torch\n",
    "from sktime.forecasting.fbprophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import sys, os, time\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error#, mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import statistics\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# import darts\n",
    "# from darts import TimeSeries\n",
    "# from darts.metrics import *\n",
    "# from darts.utils.missing_values import fill_missing_values\n",
    "\n",
    "import warnings\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "import os,sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, TimeDistributed, Conv1D, MaxPooling1D, Flatten, Bidirectional, Input, Flatten, Activation, Reshape, RepeatVector, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataprep:\n",
    "    def __init__(self, df, input_window=90, output_window=30, stride=1, start='2020-11-01'):\n",
    "        # le = preprocessing.LabelEncoder()\n",
    "        df['ds'] = pd.to_datetime(df['ds'], format='%Y-%m-%d')\n",
    "        start = pd.to_datetime(start, format='%Y-%m-%d') #+ relativedelta(months=months)\n",
    "        end = start + relativedelta(days=365 + 30) #+ relativedelta(months=months)\n",
    "        # start1=df.ds.sort_values()[0]\n",
    "        # end1=df.ds.sort_values()[len(df_panda)-1]\n",
    "        # if end1<end:end=end1\n",
    "        # if start1>start:start=start1\n",
    "        # print(start,end)\n",
    "        df = df[(df.ds >= start) & (df.ds <= end)]\n",
    "\n",
    "\n",
    "        df['query_idx'] = df['query_idx'].map(str)\n",
    "        df.sort_values('ds', inplace=True)\n",
    "        if df['ds'].iloc[-1]!=end:\n",
    "            last_date=df.iloc[-1,:]\n",
    "            last_date['ds']=end\n",
    "            last_date['daily_supply'] = 0\n",
    "            df = df.append(last_date)\n",
    "        if df['ds'].iloc[0]!=start:\n",
    "            first_date=df.iloc[0,:]\n",
    "            first_date['ds']=end\n",
    "            first_date['daily_supply'] = 0\n",
    "            df = df.append(first_date)\n",
    "        df.sort_values('ds', inplace=True)\n",
    "        df['grp'] = df.apply(\n",
    "            lambda x: '%s_%s_%s_%s' % (x['group_id'], x['query_idx'], x['SearchKeyValue'], x['LocationKeyValue']),\n",
    "            axis=1)\n",
    "        dts = (end - start).days + 1\n",
    "\n",
    "        #qr = df.groupby(['grp'])['daily_supply'].count()\n",
    "        # self.df_test=df[['ds','daily_supply','grp']]\n",
    "###############################\n",
    "        #qrs = list(qr[(dts - qr) <= 50].index)\n",
    "\n",
    "        #df = df.loc[df['grp'].isin(qrs)]\n",
    "###############################\n",
    "\n",
    "        df.sort_values('ds', inplace=True)\n",
    "        # df.set_index(['ds', 'grp'], append=True,inplace=True)\n",
    "        # df= df.unstack(fill_value=0)\\\n",
    "        # df=df.asfreq('D', fill_value=0).stack(). \\\n",
    "        #     sort_index(level=1).reset_index()\n",
    "\n",
    "\n",
    "        try:\n",
    "            df = df.set_index(['ds', 'grp']).unstack(fill_value=0). \\\n",
    "            asfreq('D', fill_value=0).stack(). \\\n",
    "            sort_index(level=1).reset_index()\n",
    "        except:\n",
    "            df = df.groupby(['ds', 'grp']).agg({'daily_supply':['sum'],'group_id':['first'],'query_idx':['first'], 'SearchKeyValue':['first'], 'LocationKeyValue':['first']})\n",
    "            df = df.unstack(fill_value=0). \\\n",
    "                asfreq('D', fill_value=0).stack(). \\\n",
    "                sort_index(level=1).reset_index()\n",
    "            df.columns = df.columns.get_level_values(0)\n",
    "        #cls = ['year', 'month', 'week', 'quarter', 'dayofweek', 'isWeekend', 'Trend', 'weeklyChange', 'priceMov',\n",
    "        #       'Season', 'isHoliday', 'isPromotion', 'dayofyear_sin', 'dayofyear_cos', 'month_sin', 'month_cos']\n",
    "        df.sort_values('ds', inplace=True)\n",
    "        #df[cls] = df[cls].replace(to_replace=0, method='ffill')  # **********\n",
    "        #df[cls] = df[cls].replace(to_replace=0, method='bfill')\n",
    "        self.df = df\n",
    "        self.test_split_date = pd.to_datetime((end + relativedelta(days=-30)).strftime('%Y-%m-%d'))\n",
    "        ####################################################################################\n",
    "        self.val_split_date = pd.to_datetime((self.test_split_date + relativedelta(days=-30)).strftime('%Y-%m-%d'))\n",
    "\n",
    "        self.df = self.df.drop(['group_id', 'query_idx', 'SearchKeyValue', 'LocationKeyValue'], axis=1)\n",
    "\n",
    "        self.df_trains = self.df[self.df.ds <= self.val_split_date]\n",
    "        self.df_tests = self.df[self.df.ds > self.test_split_date]\n",
    "        df_train_temp = self.df[self.df.ds <= self.test_split_date]\n",
    "\n",
    "        self.df_test_org = pd.pivot_table(self.df_tests, values='daily_supply', index=['ds'], columns=['grp'])\n",
    "        self.df_train_org = pd.pivot_table(df_train_temp, values='daily_supply', index=['ds'],columns=['grp'])  # ,aggfunc=np.sum\n",
    "\n",
    "        self.df_train_org.sort_index(inplace=True)\n",
    "        # print(self.df_train_org.iloc[:,0])\n",
    "        # print(self.df_train_org.iloc[-14:,0])\n",
    "\n",
    "        # $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "        self.q_train = self.df_train_org.iloc[-14:, :].sum(axis=0) / np.sum(np.sum(self.df_train_org.iloc[-14:, :]))\n",
    "        self.q_test = self.df_test_org.sum(axis=0) / np.sum(np.sum(self.df_test_org))\n",
    "        del self.df_train_org\n",
    "        del self.df_trains\n",
    "        del self.df_tests\n",
    "        del df_train_temp\n",
    "        self.df = self.df.groupby('ds').agg(\n",
    "            {'daily_supply': 'sum'\n",
    "             #,'shifted_annual_daily_supply': 'sum', 'year': lambda x: x.iloc[0],\n",
    "             #'month': lambda x: x.iloc[0],\n",
    "             #'week': lambda x: x.iloc[0], 'quarter': lambda x: x.iloc[0], 'dayofweek': lambda x: x.iloc[0],\n",
    "             #'isWeekend': lambda x: x.iloc[0], 'Trend': lambda x: x.value_counts().index[0], 'Trend_Var': 'sum',\n",
    "             #'weeklyChange': lambda x: x.value_counts().index[0], 'priceMov': lambda x: x.value_counts().index[0],\n",
    "             #'Season': lambda x: x.iloc[0],\n",
    "             #'isHoliday': lambda x: x.iloc[0], 'isPromotion': lambda x: x.iloc[0], 'dayofyear_sin': lambda x: x.iloc[0],\n",
    "             #'dayofyear_cos': lambda x: x.iloc[0], 'month_sin': lambda x: x.iloc[0], 'month_cos': lambda x: x.iloc[0]\n",
    "             })\n",
    "\n",
    "        self.inp = input_window\n",
    "        self.out = output_window\n",
    "        self.stride = stride\n",
    "\n",
    "    def _get_data(self):\n",
    "        # strCol = ['year', 'month', 'week', 'quarter', 'dayofweek', 'isWeekend', 'Trend', 'weeklyChange', 'priceMov',\n",
    "        #           'Season', 'isHoliday',\n",
    "        #           'isPromotion']  # *******************************************************************\n",
    "        # for cols in strCol:\n",
    "        #     encoder = OneHotEncoder()\n",
    "        #     encoder_df = pd.DataFrame(encoder.fit_transform(self.df[[cols]]).toarray())\n",
    "        #     column_name = encoder.get_feature_names([cols])\n",
    "        #     encoder_df.columns = column_name\n",
    "        #     encoder_df.index = self.df.index\n",
    "        #     self.df = self.df.join(encoder_df)\n",
    "        #     self.df = self.df.drop(cols, axis=1)\n",
    "\n",
    "        Cols_minmax = ['daily_supply',\n",
    "                       # 'shifted_annual_daily_supply', 'Trend_Var', 'dayofyear_sin', 'dayofyear_cos',\n",
    "                       # 'month_sin', 'month_cos'\n",
    "                       ]\n",
    "        for cols in Cols_minmax:\n",
    "            if cols != 'shifted_annual_daily_supply':\n",
    "                self.scaler_fz = MinMaxScaler(feature_range=(-1, 1))\n",
    "                self.scaler_fz = self.scaler_fz.fit(self.df[cols].values.reshape(-1, 1))\n",
    "            if cols == 'daily_supply':\n",
    "                self.scaler_f = self.scaler_fz\n",
    "            self.df[cols] = self.scaler_fz.transform(self.df[cols].values.reshape(-1, 1))\n",
    "\n",
    "        self.df_train = self.df[self.df.index <= self.val_split_date]\n",
    "        self.df_val = self.df[(self.df.index > self.val_split_date) & (self.df.index <= self.test_split_date)]\n",
    "        self.df_test = self.df[self.df.index > self.test_split_date]\n",
    "\n",
    "        train_date, test_date = list(self.df_train.index), list(self.df_test.index)\n",
    "\n",
    "        average = 0\n",
    "\n",
    "        try: val_shape = self.df_val['daily_supply'].shape[0]\n",
    "        except:val_shape = self.df_val.shape[0]\n",
    "        self.df_train.reset_index(inplace=True, drop=True)\n",
    "        self.df_val.reset_index(inplace=True, drop=True)\n",
    "        self.df_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "        if self.inp > val_shape:\n",
    "            df_test = np.concatenate([self.df_train.iloc[-(self.inp - val_shape):], self.df_val, self.df_test], axis=0)\n",
    "            df_val = np.concatenate([self.df_train.iloc[-self.inp:], self.df_val], axis=0)\n",
    "        else:\n",
    "            df_test = np.concatenate([self.df_val.iloc[:, :][-self.inp:], self.df_test], axis=0)#$$$$$$need to change for more features\n",
    "            df_val = np.concatenate([self.df_train.iloc[-self.inp:], self.df_val], axis=0)#$$$$$$need to change for more features\n",
    "        self.df_test, self.df_val = df_test, df_val\n",
    "        df_test = pd.DataFrame(df_test)\n",
    "        try:df_test.columns = self.df_train.columns\n",
    "        except:df_test.name = self.df_train.name\n",
    "\n",
    "        df_val = pd.DataFrame(df_val)\n",
    "        try:df_val.columns = self.df_train.columns\n",
    "        except:df_val.name = self.df_train.name\n",
    "\n",
    "        self.df_val, self.df_test = df_val, df_test\n",
    "\n",
    "        X_train, Y_train = self.sliding_windows(self.df_train, seq_length_x=self.inp, seq_length_y=self.out,\n",
    "                                                time_step=self.stride)\n",
    "        X_val, Y_val = self.sliding_windows(self.df_val, seq_length_x=self.inp, seq_length_y=self.out,\n",
    "                                            time_step=self.stride)\n",
    "        X_test, Y_test = self.sliding_windows(self.df_test, seq_length_x=self.inp, seq_length_y=self.out,\n",
    "                                              time_step=self.stride)\n",
    "        return X_train, Y_train, X_val, Y_val, X_test, Y_test, self.df_test_org.values, self.q_train, self.q_test\n",
    "\n",
    "    def sliding_windows(self, data, seq_length_x, seq_length_y, time_step):\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(0, len(data) - seq_length_x - seq_length_y + 1, time_step):\n",
    "            try:\n",
    "                _x = data.iloc[i:(i + seq_length_x), :].values\n",
    "                _y = data.loc[i + seq_length_x:i + seq_length_x + seq_length_y - 1,'daily_supply'].values\n",
    "            except:\n",
    "                _x = data.iloc[i:(i + seq_length_x)].values\n",
    "                _y = data.loc[i + seq_length_x:i + seq_length_x + seq_length_y - 1].values\n",
    "              # data.loc[i + seq_length_x:i + seq_length_x + seq_length_y,'daily_supply'].values\n",
    "            x.append(_x)\n",
    "            y.append(_y)\n",
    "        return np.array(x), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        np.random.seed(int(43))\n",
    "        super(Data, self).__init__()\n",
    "        # store the raw tensors\n",
    "        self._x = x\n",
    "        self._y = y\n",
    "\n",
    "    def __len__(self):\n",
    "    # a DataSet must know it size\n",
    "        return self._x.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self._x[index, :]\n",
    "        y = self._y[index, :]\n",
    "        return x, y\n",
    "    \n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, output_size, input_size=1, hidden_size=4, num_layers=1, dropout=.3,relu='False'):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.relu=relu\n",
    "        # self.dp = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True, dropout=dropout)  # .25\n",
    "        # self.batchnorm=nn.BatchNorm1d(hidden_size)\n",
    "        self.rel=nn.LeakyReLU(hidden_size)\n",
    "        # self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "        # self.dp = nn.Dropout(.5)\n",
    "        self.fc=nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        for it in self.lstm.parameters():\n",
    "            w = it\n",
    "            break\n",
    "\n",
    "        h_0 = w.new_zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size, requires_grad=w.requires_grad).to(self.device)\n",
    "\n",
    "        c_0 = w.new_zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size, requires_grad=w.requires_grad).to(self.device)\n",
    "\n",
    "        # Propagate input through LSTM\n",
    "\n",
    "        h_out, _ = self.lstm(x, (h_0, c_0))\n",
    "        #h_out.requires_grad_()\n",
    "        h_out = h_out[:, -1, :]\n",
    "        h_out = h_out.reshape(-1, self.hidden_size)\n",
    "        #out = self.dp(h_out)\n",
    "\n",
    "        #out = self.batchnorm(h_out)\n",
    "        if self.relu=='True':\n",
    "            h_out=  self.rel(h_out)\n",
    "        # out=  self.fc(h_out)\n",
    "        # out = self.dp(out)\n",
    "        out = self.fc(h_out)\n",
    "        #out.requires_grad_()\n",
    "        return out\n",
    "\n",
    "class LSTM1(nn.Module):\n",
    "\n",
    "    def __init__(self, output_size, input_size=1, hidden_size=4, num_layers=1,dropout=.1):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True,dropout = .2)\n",
    "        self.batchnorm=nn.BatchNorm1d(hidden_size)\n",
    "        self.dp=nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for it in self.lstm.parameters():\n",
    "            w = it\n",
    "            break\n",
    "\n",
    "        h_0 = w.new_zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size, requires_grad=w.requires_grad).to(self.device)\n",
    "\n",
    "        c_0 = w.new_zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size, requires_grad=w.requires_grad).to(self.device)\n",
    "\n",
    "        # Propagate input through LSTM\n",
    "\n",
    "        h_out, _ = self.lstm(x, (h_0, c_0))\n",
    "        h_out.requires_grad_()\n",
    "        h_out = h_out[:, -1, :]\n",
    "        h_out = h_out.reshape(-1, self.hidden_size)\n",
    "        out=self.batchnorm(h_out)\n",
    "        out=self.dp(out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        out.requires_grad_()\n",
    "        return out\n",
    "    \n",
    "    \n",
    "\n",
    "class LSTM3(nn.Module):\n",
    "\n",
    "    def __init__(self, output_size, input_size=1, hidden_size=4, num_layers=1,dropout=.1):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True,dropout = dropout)\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True,dropout = dropout)\n",
    "        self.batchnorm=nn.BatchNorm1d(hidden_size)\n",
    "        self.dp=nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for it in self.lstm1.parameters():\n",
    "            w = it\n",
    "            break\n",
    "\n",
    "        h_0 = w.new_zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size, requires_grad=w.requires_grad).to(self.device)\n",
    "\n",
    "        c_0 = w.new_zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size, requires_grad=w.requires_grad).to(self.device)\n",
    "        for it in self.lstm2.parameters():\n",
    "            w1 = it\n",
    "            break\n",
    "        h_1 = w1.new_zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size, requires_grad=w1.requires_grad).to(self.device)\n",
    "\n",
    "        c_1 = w1.new_zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size, requires_grad=w1.requires_grad).to(self.device)\n",
    "\n",
    "        torch.nn.init.xavier_normal_(h_0)\n",
    "        torch.nn.init.xavier_normal_(c_0)\n",
    "        torch.nn.init.xavier_normal_(h_1)\n",
    "        torch.nn.init.xavier_normal_(c_1)\n",
    "        # Propagate input through LSTM\n",
    "\n",
    "        h_out, _ = self.lstm1(x, (h_0, c_0))\n",
    "        h_out.requires_grad_()\n",
    "        #h_out = h_out[:, -1, :]\n",
    "        #h_out = h_out.reshape(-1, self.hidden_size)\n",
    "        \n",
    "        h_out1, _ = self.lstm2(h_out, (h_0, c_0))\n",
    "        h_out1.requires_grad_()\n",
    "        h_out1 = h_out1[:, -1, :]\n",
    "        h_out1 = h_out1.reshape(-1, self.hidden_size)\n",
    "        \n",
    "        out=self.batchnorm(h_out1)\n",
    "        out=self.dp(out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        out.requires_grad_()\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMAPE(A, F):\n",
    "    smape_mean = 0\n",
    "    for i in range(A.shape[0]):\n",
    "        if A[i] == F[i]:\n",
    "            continue\n",
    "        smape_mean += 2 * np.abs(F[i] - A[i]) / (np.abs(A[i]) + np.abs(F[i]))\n",
    "    smape_mean = 1 / A.shape[0] * smape_mean\n",
    "    return smape_mean\n",
    "def SMAPE2(A, F):\n",
    "    smape_mean = 0\n",
    "    for i in range(A.shape[0]):\n",
    "        if A[i] == F[i]:\n",
    "            continue\n",
    "        smape_mean += 2 * np.abs(F[i] - A[i]) / (np.abs(A[i]) + np.abs(F[i]))\n",
    "    smape_mean = 1 / A.shape[0] * smape_mean\n",
    "    return smape_mean\n",
    "def WAPE(A, F):\n",
    "    return np.sum((np.abs(F - A))/np.sum(A))\n",
    "def weightedSMAPE(A,F,wq):\n",
    "    res=0\n",
    "    print('number of smape: ',len(wq))\n",
    "    for i in range(len(wq)):\n",
    "        res+=wq[i]*SMAPE(A[:,i],F[:,i])\n",
    "    return res\n",
    "def WAPE_fun(actual,predicted,w):\n",
    "    #w=query percentage in the cluster\n",
    "    wape_tot=0\n",
    "    #print('number of smape: ', len(w))\n",
    "    for i in range(len(w)):\n",
    "        if np.sum(actual[:,i].reshape(-1,1))<=0:\n",
    "            continue\n",
    "        wape_tot+=w[i]*WAPE(actual[:,i].reshape(-1,1),predicted[:,i].reshape(-1,1))\n",
    "    return wape_tot\n",
    "def PE(A, F):\n",
    "    return np.abs(np.sum(F)-np.sum(A))/np.sum(A)\n",
    "def PE_fun(actual,predicted,w):\n",
    "    wape_tot = 0\n",
    "    for i in range(len(w)):\n",
    "        if np.sum(actual[:, i].reshape(-1, 1)) <= 0:\n",
    "            continue\n",
    "        wape_tot += w[i] * PE(actual[:, i].reshape(-1, 1), predicted[:, i].reshape(-1, 1))\n",
    "    return wape_tot\n",
    "def estimate(actual,pred,q):\n",
    "    if type(q)==pd.core.series.Series:q=q.values\n",
    "    #pred=pred * q.reshape(-1, )\n",
    "    smape_error = weightedSMAPE(actual, pred, q)\n",
    "    wape=WAPE_fun(actual,pred,q)\n",
    "    pe=PE_fun(actual,pred,q)\n",
    "    return smape_error,wape,pe\n",
    "def metric_dist(A,F,q,metric='wape'):\n",
    "    res=[]\n",
    "    if metric=='wape':\n",
    "        for i in range(len(q)):\n",
    "            if np.sum(A[:, i].reshape(-1, 1)) <= 0:\n",
    "                continue\n",
    "            res.append(WAPE(A[:, i].reshape(-1, 1), F[:, i].reshape(-1, 1)))\n",
    "    elif metric=='smape':\n",
    "        for i in range(len(q)):\n",
    "            if np.sum(A[:, i].reshape(-1, 1)) <= 0:\n",
    "                continue\n",
    "            res.append(SMAPE(A[:, i].reshape(-1, 1), F[:, i].reshape(-1, 1)))\n",
    "    else:\n",
    "        for i in range(len(q)):\n",
    "            if np.sum(A[:, i].reshape(-1, 1)) <= 0:\n",
    "                continue\n",
    "            res.append(PE(A[:, i].reshape(-1, 1), F[:, i].reshape(-1, 1)))\n",
    "    return [np.percentile(res,50),np.percentile(res,80),np.percentile(res,90)]\n",
    "def Estimate(actual,pred,metric='wape'):\n",
    "    if metric=='pe':return PE(actual,pred)\n",
    "    elif metric=='wape':return WAPE(actual.reshape(-1,1),pred.reshape(-1,1))\n",
    "    elif metric=='smape':return SMAPE(actual.reshape(-1,1),pred.reshape(-1,1))\n",
    "    else:\n",
    "        raise ValueError('incorrect metric is defined')\n",
    "def myloss(A,F):\n",
    "    #loss=torch.mean(np.abs((actual.detach().numpy() - predict.detach().numpy()) / actual))\n",
    "    loss=torch.mean(1 / A.size(dim=0)* torch.sum(2 * torch.abs(F - A) / (torch.abs(A) + torch.abs(F))))\n",
    "    return loss#torch.from_numpy(np.asarray(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(config,save_model=False):\n",
    "    num_epochs = config['num_epochs']\n",
    "    learning_rate = config['learning_rate']\n",
    "    window_size = 30\n",
    "    output_size = 30\n",
    "    relu=config['relu']\n",
    "    input_size = train_set_in.shape[2]\n",
    "    hidden_size = config['hidden_size']\n",
    "    num_layers = 1\n",
    "    drop_out = config['dropout']\n",
    "    batch_size=config['batch_size']\n",
    "    gamma=config['gamma']\n",
    "    optimizer_name=config['optimizer_name']\n",
    "    num_layers=config['num_layer']\n",
    "    #dropout1=config['dropout1']\n",
    "    lstm = trainLSTM(num_epochs, learning_rate, output_size, input_size, hidden_size, num_layers, drop_out,gamma,optimizer_name,relu)\n",
    "    lstm.train(train_set_in, train_set_out,val_set_in,val_set_out,test_in_set,test_out_set,batch_size=batch_size)\n",
    "    val_pred,_, valid_smape, valid_wape,valid_pe= lstm.valid(val_set_in, val_set_out)\n",
    "    testing_predict, testing_truth, Wsmape, Wape, Wpe, smape_dis, wape_dis, pe_dis, smape_agg, wape_agg,pe_agg = lstm.valid(test_in_set, test_out_set,not_test=False,pred_is_req=True,df_test_org=df_test_org)\n",
    "    print('for sanity check', valid_smape[0], valid_wape,valid_pe, 'the mean is: ',(valid_smape[0]+valid_wape+valid_pe)/3)\n",
    "    return [Wsmape, Wape, Wpe, smape_dis, wape_dis, pe_dis, smape_agg, wape_agg,pe_agg,lstm,val_pred.reshape(-1,),valid_wape]#[training_smape*100,valid_smape*100, testing_smape*100,training_wape*100,valid_wape*100, testing_wape*100,lstm]\n",
    "def running_lstm(config,save_model=False,Device='cpu'):\n",
    "    num_epochs = config['num_epochs']\n",
    "    learning_rate = config['learning_rate']\n",
    "    window_size = 30\n",
    "    output_size = 30\n",
    "    batch_size=config['batch_size']\n",
    "    input_size = train_set_in.shape[2]\n",
    "    hidden_size = config['hidden_size']\n",
    "    num_layers = 1\n",
    "    drop_out=config['dropout']\n",
    "    gamma=config['gamma']\n",
    "    optimizer_name=config['optimizer_name']\n",
    "    num_layers=config['num_layer']\n",
    "    relu=config['relu']\n",
    "    #dropout1=config['dropout1']\n",
    "    lstm = trainLSTM(num_epochs, learning_rate, output_size, input_size, hidden_size, num_layers,drop_out,gamma,optimizer_name,relu)\n",
    "    #lstm.to(Device)\n",
    "    loss=lstm.train(train_set_in, train_set_out,val_set_in,val_set_out,test_in_set,test_out_set,batch_size=batch_size)\n",
    "    _,_, Wsmape, Wwape,Wpe = lstm.valid(val_set_in, val_set_out)\n",
    "    #testing_predict, testing_truth, testing_smape,testing_wape,smape_agg,wape_agg = lstm.valid(val_set_in, val_set_out)\n",
    "    return Wsmape[0],Wwape,Wpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Tail clusters\n",
    "test_group_id_list = [\n",
    " '/1005862/1007220/1044020/3392907',\n",
    " '/3944/77622/8375901/1230415/1230417',\n",
    " '/4171/645779/2165088/3723286',\n",
    " '/1334134/5899871/7445675/5456260',\n",
    " '/5428/1102183/8617474/1564368',\n",
    " '/2637/5249422/5222147',\n",
    " '/1334134/6355365/1285843/3783885',\n",
    " '/976759/976787/1001390/2523039/2119183',\n",
    " '/91083/2962826/8564995/2827548',\n",
    " '/4125/4134/1026285/9642310/1251682',\n",
    " '/1005862/1007220/1007399/6461098',\n",
    " '/5427/133283/9095197',\n",
    " '/1115193/1071967/1149380/5135111',\n",
    " '/976759/976794/5403011/9731488',\n",
    " '/4125/4134/2350251/1078395',\n",
    " '/5428/8018905/5374562/7304919/6954191',\n",
    " '/6197502/5702707/9458810/9028847',\n",
    " '/91083/1074769/6350088',\n",
    " '/5440/1001299/6148116',\n",
    " '/1334134/6172404/7659444/3836217',\n",
    " '/3920/1987289/582374/8324310/1974555',\n",
    " '/4125/4161/4165/1075707',\n",
    " '/976760/1414629/6578740/1310637',\n",
    " '/91083/1074765/9183778/3753296',\n",
    " '/3920/582053/584177/585913',\n",
    " '/976760/1005863/8121112',\n",
    " '/1105910/133161/1072306/5297453/5115891',\n",
    " '/976760/1005860/4157476/8398146/6630246',#\n",
    " '/2637/6749799/2053705',\n",
    " '/1072864/1067612/6801317/3181011/4259773',\n",
    " '/1085666/5859906/3311357',\n",
    " '/976760/9575239/2981737',\n",
    " '/2637/9489599/2275107',\n",
    " '/4125/4134/1078384/1078388',\n",
    " '/5440/202073/1749780/3053202/2610736',\n",
    " '/1105910/133161/1103334/3123349/9119436',\n",
    " '/1334134/6172404/4854691/2291474',\n",
    " '/4171/7357538/6711451/4147327',\n",
    " '/1334134/8495017/8541719',\n",
    " '/4125/546956/4128/4547850/2751930',\n",
    " '/1072864/1067618/1230774/8250798/6489704',\n",
    " '/1072864/1808984/4037404/9537015',#\n",
    " '/1085666/3316357/6852893/3607949',\n",
    " '/1229749/6562796/6709176',\n",
    " '/91083/1077064/8595272/2238009',#\n",
    " '/5440/9672181',\n",
    " '/1072864/1067618/1230774/8250798/6489704',\n",
    " '/1085666/3147628/8028415/6121950',#\n",
    " '/4044/103150/4038/7389568/2455597',#\n",
    " '/976760/1876667/7219872']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_month=['2020-09-01','2020-10-01','2020-11-01','2020-12-01','2021-01-01','2021-02-01','2021-03-01',\n",
    "            '2021-04-01','2021-05-01','2021-06-01','2021-07-01','2021-08-01']\n",
    "name=['September','October','November','December','January','February','March','April','May','June','July','August']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "September  :  2020-09-01\n",
      "October  :  2020-10-01\n",
      "November  :  2020-11-01\n",
      "December  :  2020-12-01\n",
      "January  :  2021-01-01\n",
      "February  :  2021-02-01\n",
      "March  :  2021-03-01\n",
      "April  :  2021-04-01\n",
      "May  :  2021-05-01\n",
      "June  :  2021-06-01\n",
      "July  :  2021-07-01\n",
      "August  :  2021-08-01\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(name)):\n",
    "    print(name[i],' : ',start_month[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_date = '20220831'\n",
    "# test_group_id = test_group_id_list[0]#'/3944/1060825/447913'\n",
    "# # Path to the new data\n",
    "# path = f\"gs://adtech-artifacts/ds/forecast/month_eval/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_tft_data_{current_date}.csv\"\n",
    "# # read in data\n",
    "# tf_data_pd = spark.read.option(\"header\", True).csv(path).toPandas() # The usage should be the same as previous\n",
    "\n",
    "# # The new train+valid test split logic becomes (if you use the same logic - past year years of data as train and test and predict the next 30 days)\n",
    "# # Month | train+valid | test\n",
    "# # Sep | 2020.09 - 2021.08 | 2021.09\n",
    "# # Start date is 2020-09-01 and next 365 days is the train+valid and the following 30 days is the test. For the next \"month\"'s eval model, we will move the entire train+valid+test for the next 30 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_data_pd.to_csv('df_1_noF.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params={\n",
    "        #\"num_layers\":trial.suggest_int(\"num_layer\",1,2),\n",
    "        'optimizer_name':trial.suggest_categorical(\"optimizer_name\", [\"Adam\",'RMSprop','AdamW','SGD']),#\"Adam\",'RMSprop',\n",
    "        \"hidden_size\":trial.suggest_int('hidden_size',8,20),\n",
    "        'dropout':trial.suggest_uniform(\"dropout\",0,0.10),\n",
    "        #'dropout1':trial.suggest_uniform(\"dropout1\",0,0.3),\n",
    "        'learning_rate':trial.suggest_loguniform('learning_rate',1e-4,1e-0),\n",
    "        'num_epochs':trial.suggest_int('num_epochs',100,100),\n",
    "        'batch_size':trial.suggest_int('batch_size',32,64),\n",
    "        'gamma':trial.suggest_uniform(\"gamma\",0.8,1),\n",
    "        'num_layer':trial.suggest_int('num_layer',1,3),\n",
    "        'relu':trial.suggest_categorical('relu',['True','False'])}\n",
    "    temp_loss=running_lstm(params,save_model=False)\n",
    "    return temp_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainLSTM:\n",
    "    def __init__(self, num_epochs, learning_rate, output_size, input_size, hidden_size, num_layers, drop_out, gamma,\n",
    "                 optimizer_name, relu=False):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.num_epochs = num_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.output_size = output_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.drop_out = drop_out\n",
    "        self.lstm = LSTM(output_size, input_size, hidden_size, num_layers, drop_out, relu)\n",
    "        self.lstm.to(self.device)\n",
    "        self.criterion = torch.nn.MSELoss()  # mean-squared error for regression: MSELoss  #L1Loss\n",
    "        self.gamma = gamma\n",
    "        self.optimizer_name = optimizer_name\n",
    "        self.optimizer = getattr(torch.optim, optimizer_name)(self.lstm.parameters(), lr=learning_rate)\n",
    "        self.val_loss_ep = []\n",
    "        self.train_loss_ep = []\n",
    "        self.test_loss_ep = []\n",
    "        # torch.optim.Adam(self.lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "    def train(self, trainX, trainY, valX, valY, testX, testY, batch_size=32, n_epochs_stop=10):\n",
    "        # Train the model\n",
    "        # self.lstm.train()\n",
    "        trainX, trainY = trainX.to(self.device), trainY.to(self.device)\n",
    "        valX, valY = valX.to(self.device), valY.to(self.device)\n",
    "        DF = Data(trainX, trainY)\n",
    "        trainloader = DataLoader(dataset=DF, batch_size=batch_size, shuffle=True, num_workers=0)  ####\n",
    "        # Early stopping\n",
    "        last_loss = 1e10\n",
    "        patience = n_epochs_stop\n",
    "        trigger_times = 0\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', patience=75, factor=self.gamma)  #ExponentialLR(self.optimizer, gamma=self.gamma)#\n",
    "        clip = 5\n",
    "        PATH = \"best_mod_agg.pt\"\n",
    "        for epoch in range(self.num_epochs):\n",
    "            train_losses = 0\n",
    "            self.lstm.train()\n",
    "            start1 = time.time()\n",
    "            for x, y in trainloader:\n",
    "                # trainX.requires_grad_()\n",
    "                # print(x[0,:5,:])\n",
    "                outputs = self.lstm(x)\n",
    "\n",
    "                # obtain the loss function\n",
    "                loss = self.criterion(outputs.reshape(-1, 1),\n",
    "                                      y.reshape(-1, 1))  # myloss(outputs.reshape(-1, 1), y.reshape(-1, 1))#.\n",
    "                # loss.requires_grad_()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()  # retain_graph=True\n",
    "                # nn.utils.clip_grad_norm_(self.lstm.parameters(), clip)\n",
    "                self.optimizer.step()\n",
    "                train_losses += loss.item()  # *x.size(0)\n",
    "            train_losses = train_losses / len(DF)\n",
    "            # if epoch % 5 == 0:\n",
    "            #    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item() / batch_size))\n",
    "            self.train_loss_ep.append(train_losses)\n",
    "\n",
    "            current_loss = self.eval(valX, valY)  # Estimate(valY.data.cpu().numpy(), output.data.cpu().numpy())\n",
    "            scheduler.step(current_loss)\n",
    "            self.val_loss_ep.append(current_loss)\n",
    "            test_loss = self.eval(testX, testY)\n",
    "            self.test_loss_ep.append(test_loss)\n",
    "            if current_loss > last_loss and epoch >= 10:\n",
    "                trigger_times += 1\n",
    "                # print('Trigger Times:', trigger_times)\n",
    "                if trigger_times >= patience:\n",
    "                    print('Early stopping!\\nStart to test process.', epoch)\n",
    "                    break\n",
    "            else:\n",
    "                trigger_times = 0\n",
    "                # torch.save({\n",
    "                #     'epoch': epoch,\n",
    "                #     'model_state_dict': self.lstm.state_dict(),\n",
    "                #     'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                #     'loss': train_losses,\n",
    "                #     'val_loss':current_loss\n",
    "                # }, PATH)\n",
    "                # print(self.lstm.state_dict()['lstm.bias_hh_l4'][:5])\n",
    "                torch.save({'model': self.lstm, 'val_loss': current_loss}, PATH)\n",
    "                # print(self.lstm.state_dict())\n",
    "            last_loss = current_loss\n",
    "            # print('Epoach : ', epoch, ' train_loss: ', train_losses, ' valid_loss: ', current_loss, 'test loss',\n",
    "            #      test_loss, 'time taken', time.time() - start1)\n",
    "        checkpoint = torch.load(PATH)\n",
    "        self.lstm = checkpoint['model']\n",
    "        # self.lstm.load_state_dict(checkpoint['model_state_dict'])\n",
    "        # self.lstm=LSTM(self.output_size, self.input_size, self.hidden_size, self.num_layers, self.drop_out)\n",
    "        # self.lstm.load_state_dict(checkpoint['model_state_dict'])\n",
    "        # self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        # print('lets check the model poaram')\n",
    "        # print(self.lstm.state_dict()['lstm.bias_hh_l4'][:5])\n",
    "        current_loss = checkpoint['val_loss']\n",
    "        return current_loss\n",
    "\n",
    "    def eval(self, valX, valY, batch_size=32):\n",
    "        valX, valY = valX.to(self.device), valY.to(self.device)\n",
    "        DFv = Data(valX, valY)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset=DFv, batch_size=batch_size, shuffle=False)\n",
    "        val_loss = 0\n",
    "        self.lstm.eval()  # prep model for evaluation\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                outputs = self.lstm(x)\n",
    "                loss = self.criterion(y, outputs)  # Estimate(valY.data.cpu().numpy(), outputs.data.cpu().numpy())\n",
    "                val_loss += loss.item()  # *batch_size#*x.size(0)\n",
    "            current_loss = val_loss / len(DFv)\n",
    "        return current_loss\n",
    "\n",
    "    def valid(self, testX, testY, not_test=True, pred_is_req=False, df_test_org=None):\n",
    "        # evaluate the model\n",
    "        self.lstm.eval()\n",
    "        testX, testY = testX.to(self.device), testY.to(self.device)\n",
    "        test_predict = self.lstm(testX)\n",
    "        data_predict = test_predict.data.cpu().numpy()\n",
    "        dataY_plot = testY.data.cpu().numpy()  # to(self.device)\n",
    "        data_predict1 = prep.scaler_f.inverse_transform(data_predict.T)\n",
    "        try:\n",
    "            dataY_plot = prep.scaler_f.inverse_transform(dataY_plot.T)\n",
    "        except:\n",
    "            dataY_plot = dataY_plot.reshape(dataY_plot.shape[0], dataY_plot.shape[1])\n",
    "            dataY_plot = prep.scaler_f.inverse_transform(dataY_plot.T)\n",
    "        data_predict1[data_predict1 < 0] = 0\n",
    "        if not_test:\n",
    "            smape, wape, pe = Estimate(dataY_plot, data_predict1, metric='smape'), Estimate(dataY_plot, data_predict1, metric='wape'), Estimate(dataY_plot, data_predict1, metric='pe')\n",
    "            #print('wape is :', smape, wape, pe)\n",
    "            return data_predict1[:,-1], dataY_plot, smape, wape, pe\n",
    "        else:\n",
    "            smape_agg, wape_agg, pe_agg = Estimate(dataY_plot, data_predict1, metric='smape'), Estimate(dataY_plot,data_predict1, metric='wape'), Estimate(\n",
    "                dataY_plot, data_predict1, metric='pe')\n",
    "            #print('smape is: ', smape_agg, 'wape is: ', wape_agg, 'pe is:', pe_agg)\n",
    "            data_predict11 = np.tile(data_predict1.T, (q.shape[0], 1)).T\n",
    "            data_predict2 = data_predict11 * q.values.reshape(-1, 1).T\n",
    "            Wsmape, Wwape, Wpe = estimate(df_test_org, data_predict2, wq)\n",
    "            smape_dis, wape_dis, pe_dis = metric_dist(df_test_org, data_predict2, wq, metric='smape'), metric_dist( df_test_org, data_predict2, wq, metric='wape'), metric_dist(df_test_org, data_predict2, wq, metric='pe')\n",
    "#         if pred_is_req:\n",
    "#             data_predict_2 = data_predict.T\n",
    "#             data_predict_2 = prep.scaler_f.inverse_transform(data_predict_2)\n",
    "#             data_predict_2[data_predict_2 < 0] = 0\n",
    "        return data_predict2, dataY_plot, Wsmape, Wwape, Wpe, smape_dis, wape_dis, pe_dis, smape_agg[\n",
    "            0], wape_agg, pe_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "AggSmapeL=[]\n",
    "AggWapeL=[]\n",
    "AggPeL=[]\n",
    "times_org=[]\n",
    "TimesL=[]\n",
    "wape_agg_lstm=np.zeros(50, dtype=np.float64)\n",
    "df_lstm=np.zeros((30,50), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "AggSmapeL=[0]*50\n",
    "AggWapeL=[0]*50\n",
    "AggPeL=[0]*50\n",
    "times_org=[0]*50\n",
    "TimesL=[0]*50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_grp=[39,38,30,20,33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(49,50):\n",
    "#     print('cluster id: ',i+1)\n",
    "#     current_date = '20220831'\n",
    "#     test_group_id = test_group_id_list[i]#'/3944/1060825/447913'\n",
    "#     #for top cluster:\n",
    "#     #path = f\"gs://adtech-artifacts/ds/forecast/month_eval/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_tft_data_{current_date}.csv\"\n",
    "#     path = f\"gs://adtech-artifacts/ds/forecast/month_eval/tail_clusters/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_data_20220831.csv\"\n",
    "#     # read in data\n",
    "#     df_panda = spark.read.option(\"header\", True).csv(path).toPandas() # The usage should be the same as previous\n",
    "#     df_panda=df_panda[['ds', 'group_id', 'query_idx', 'SearchKeyValue',  'LocationKeyValue', 'daily_supply']]\n",
    "#     #df_panda=spark.read.csv(\"gs://adtech-artifacts/ds/forecast/query_taxonomy_sample_Addedfeatures_cluster_\"+str(i)+\".csv\",header=True).toPandas()\n",
    "#     Cols=['daily_supply']#, 'shifted_annual_daily_supply','Trend_Var','dayofyear_sin', 'dayofyear_cos', 'month_sin', 'month_cos']\n",
    "#     for cols in Cols:  \n",
    "#         df_panda[cols]=df_panda[cols].astype(float,errors='raise')\n",
    "\n",
    "#df_panda.to_csv('df_20_noF.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=43\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for the month of : September\n",
      "cluster id:  1\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: gs://adtech-artifacts/ds/forecast/month_eval/query_taxonomy_clusters_sample_1005862_1007220_1044020_3392907_tft_data_20220831.csv;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o165.csv.\n: org.apache.spark.sql.AnalysisException: Path does not exist: gs://adtech-artifacts/ds/forecast/month_eval/query_taxonomy_clusters_sample_1005862_1007220_1044020_3392907_tft_data_20220831.csv;\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:576)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:559)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:559)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:373)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:242)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:230)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:641)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-83412d0fd2cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#path = f\"gs://adtech-artifacts/ds/forecast/month_eval/tail_clusters/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_data_20220831.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# read in data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdf_panda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# The usage should be the same as previous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mdf_panda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_panda\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'group_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'query_idx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SearchKeyValue'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m'LocationKeyValue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'daily_supply'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#df_panda=spark.read.csv(\"gs://adtech-artifacts/ds/forecast/query_taxonomy_sample_Addedfeatures_cluster_\"+str(i)+\".csv\",header=True).toPandas()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue)\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: gs://adtech-artifacts/ds/forecast/month_eval/query_taxonomy_clusters_sample_1005862_1007220_1044020_3392907_tft_data_20220831.csv;'"
     ]
    }
   ],
   "source": [
    "month=0\n",
    "print('Running for the month of :',name[month])\n",
    "################LSTM##########################\n",
    "################LSTM##########################\n",
    "for i in range(50):\n",
    "    print('cluster id: ',i+1)\n",
    "    current_date = '20220831'\n",
    "    test_group_id = test_group_id_list[i]#'/3944/1060825/447913'\n",
    "    #for top cluster:\n",
    "    path = f\"gs://adtech-artifacts/ds/forecast/month_eval/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_tft_data_{current_date}.csv\"\n",
    "    #path = f\"gs://adtech-artifacts/ds/forecast/month_eval/tail_clusters/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_data_20220831.csv\"\n",
    "    # read in data\n",
    "    df_panda = spark.read.option(\"header\", True).csv(path).toPandas() # The usage should be the same as previous\n",
    "    df_panda=df_panda[['ds', 'group_id', 'query_idx', 'SearchKeyValue',  'LocationKeyValue', 'daily_supply']]\n",
    "    #df_panda=spark.read.csv(\"gs://adtech-artifacts/ds/forecast/query_taxonomy_sample_Addedfeatures_cluster_\"+str(i)+\".csv\",header=True).toPandas()\n",
    "    Cols=['daily_supply']#, 'shifted_annual_daily_supply','Trend_Var','dayofyear_sin', 'dayofyear_cos', 'month_sin', 'month_cos']\n",
    "    for cols in Cols:  \n",
    "        df_panda[cols]=df_panda[cols].astype(float,errors='raise')\n",
    "    start=time.time()\n",
    "    start=time.time()\n",
    "    minError=1e10\n",
    "    param = {\n",
    "        \"learning_rate\": [.1,.01,.001],\n",
    "        \"hidden_size\": [10,20],\n",
    "        'batch_size': [32],\n",
    "        'dropout': [0.05,.1],\n",
    "        'gamma': [0.9],\n",
    "        'optimizer_name': ['AdamW'],\n",
    "        'num_layer': [1,2,3],\n",
    "        'relu': [False]\n",
    "    }\n",
    "    prep = dataprep(df_panda, input_window=90, output_window=30, stride=1,start=start_month[month])\n",
    "    print('Beginning the training...')\n",
    "    train_set_in, train_set_out, val_set_in, val_set_out, test_in_set, test_out_set,df_test_org,  q,wq = prep._get_data()\n",
    "    #print('print len of q, value and wq value',len(q),q[0],wq[0])\n",
    "    train_set_in = torch.from_numpy(train_set_in.astype(np.float64)).float()\n",
    "    train_set_out = torch.from_numpy(train_set_out.astype(np.float64)).float()\n",
    "    val_set_in = torch.from_numpy(val_set_in.astype(np.float64)).float()\n",
    "    val_set_out = torch.from_numpy(val_set_out.astype(np.float64)).float()\n",
    "    test_in_set = torch.from_numpy(test_in_set.astype(np.float64)).float()\n",
    "    test_out_set = torch.from_numpy(test_out_set.astype(np.float64)).float()\n",
    "    print('val shape is: ',val_set_in.shape, val_set_out.shape)\n",
    "    end1=time.time()\n",
    "    if len(train_set_in.shape) < 3:\n",
    "        train_set_in = train_set_in.reshape(train_set_in.shape[0], train_set_in.shape[1], 1)\n",
    "        val_set_in = val_set_in.reshape(val_set_in.shape[0], val_set_in.shape[1], 1)\n",
    "        test_in_set = test_in_set.reshape(test_in_set.shape[0], test_in_set.shape[1], 1)\n",
    "        test_out_set=test_out_set.reshape(test_out_set.shape[0], test_out_set.shape[1])\n",
    "    counter=1\n",
    "    for lr in param['learning_rate']:\n",
    "        for h_s in param['hidden_size']:\n",
    "            for b_s in param['batch_size']:\n",
    "                for dp in param['dropout']:\n",
    "                    for opt in param['optimizer_name']:\n",
    "                        for n_l in param['num_layer']:\n",
    "                            for act in param['relu']:\n",
    "                                for gm in param['gamma']:\n",
    "                                    config = {\n",
    "                                        \"num_epochs\": 200,\n",
    "                                        \"learning_rate\": lr,\n",
    "                                        \"hidden_size\": h_s,\n",
    "                                        \"window_size\": 30,\n",
    "                                        \"output_size\": 30,\n",
    "                                        'batch_size': b_s,\n",
    "                                        'dropout': dp,\n",
    "                                        'dropout1': 0,\n",
    "                                        'gamma': gm,\n",
    "                                        'optimizer_name': opt,\n",
    "                                        'num_layer': n_l,\n",
    "                                        'relu': act\n",
    "                                    }\n",
    "                                    #torch.manual_seed(seed)\n",
    "                                    print('trial run: ',counter)\n",
    "                                    counter+=1\n",
    "                                    e1,e2,e3 = running_lstm(config, save_model=True)\n",
    "                                    if (e1+e2+e3)/3<minError:\n",
    "                                        minError=(e1+e2+e3)/3\n",
    "                                        best_config=config\n",
    "                                        print('the  smape is {} wape is {} and pe is {} and total is {}'.format(e1,e2,e3,(e1+e2+e3)/3))\n",
    "                                        print('the config is {}',config)\n",
    "    if minError>=.7:\n",
    "        param = {\n",
    "        \"learning_rate\": [.005,.1,.0001],\n",
    "        \"hidden_size\": [15,30],\n",
    "        'batch_size': [64],\n",
    "        'dropout': [0.3,.5],\n",
    "        'gamma': [0.9],\n",
    "        'optimizer_name': ['Adam','AdamW'],\n",
    "        'num_layer': [3,5],\n",
    "        'relu': [True]}\n",
    "        for lr in param['learning_rate']:\n",
    "            for h_s in param['hidden_size']:\n",
    "                for b_s in param['batch_size']:\n",
    "                    for dp in param['dropout']:\n",
    "                        for opt in param['optimizer_name']:\n",
    "                            for n_l in param['num_layer']:\n",
    "                                for act in param['relu']:\n",
    "                                    for gm in param['gamma']:\n",
    "                                        config = {\n",
    "                                            \"num_epochs\": 200,\n",
    "                                            \"learning_rate\": lr,\n",
    "                                            \"hidden_size\": h_s,\n",
    "                                            \"window_size\": 30,\n",
    "                                            \"output_size\": 30,\n",
    "                                            'batch_size': b_s,\n",
    "                                            'dropout': dp,\n",
    "                                            'dropout1': 0,\n",
    "                                            'gamma': gm,\n",
    "                                            'optimizer_name': opt,\n",
    "                                            'num_layer': n_l,\n",
    "                                            'relu': act\n",
    "                                        }\n",
    "                                        print('trial run: ',counter)\n",
    "                                        counter+=1\n",
    "                                        e1,e2,e3 = running_lstm(config, save_model=True)\n",
    "                                        \n",
    "                                        #print('the config is {}',config)\n",
    "                                        if (e1+e2+e3)/3<minError:\n",
    "                                            minError=(e1+e2+e3)/3\n",
    "                                            best_config=config\n",
    "                                            print('the  smape is {} wape is {} and pe is {} and total is {}'.format(e1,e2,e3,(e1+e2+e3)/3))\n",
    "                                            print('the config is {}',config)\n",
    "\n",
    "    end = time.time()\n",
    "    print('thus best config is', best_config)\n",
    "    Wsmape, Wape, Wpe, smape_dis, wape_dis, pe_dis, smape_agg, wape_agg, pe_agg, model, df_lstm[:,i],wape_agg_lstm[i] = train_lstm(best_config, save_model=True)\n",
    "    print('val wape is: ',wape_agg_lstm[i])\n",
    "    print('total time ', end1 - end)\n",
    "    print('test Wsmape: ', Wsmape, ' test Wape: ', Wape, ' test Wpe: ', Wpe)\n",
    "    print('Smape dis',smape_dis,'Wape dis',wape_dis,'PE dis',pe_dis)\n",
    "    print('Agg SMAPE: ',smape_agg,'Agg WAPE: ',wape_agg,'Agg PE: ',pe_agg)\n",
    "    print('total time ', end1 - end)\n",
    "\n",
    "    #print('time needed to run the model: ',end-start)\n",
    "    AggSmapeL.append(smape_agg)\n",
    "    AggWapeL.append(wape_agg)\n",
    "    AggPeL.append(pe_agg)\n",
    "    SMAPEsL.append(Wsmape)\n",
    "    WAPEsL.append(Wape)\n",
    "    PEsL.append(Wpe)\n",
    "    median_smape.append(smape_dis[0])\n",
    "    Eighty_per_smape.append(smape_dis[1])\n",
    "    Ninty_per_smape.append(smape_dis[2])\n",
    "    median_wape.append(wape_dis[0])\n",
    "    Eighty_per_wape.append(wape_dis[1])\n",
    "    Ninty_per_wape.append(wape_dis[2])\n",
    "    median_pe.append(pe_dis[0])\n",
    "    Eighty_per_pe.append(pe_dis[1])\n",
    "    Ninty_per_pe.append(pe_dis[2])\n",
    "    TimesL.append(end1-end)\n",
    "    times_org.append(end1-start)\n",
    "    print('===============================================')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_panda.to_csv('df_36_noF.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TimesL=',TimesL)\n",
    "print('AggSmapeL=',AggSmapeL)\n",
    "print('AggWapeL=',AggWapeL)\n",
    "print('AggPeL=',AggPeL)\n",
    "print('times_org=',times_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Running for the month of :',name[month])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_panda.to_csv('df_36.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class statModels:\n",
    "    def __init__(self):\n",
    "        # self.df_train=df\n",
    "        self.models = {}\n",
    "        self.sc = MinMaxScaler(feature_range=(0, 1))\n",
    "        self.clus_wape={}\n",
    "        self.clus_smape={}\n",
    "        self.clus_pe={}\n",
    "        self.wape_Weighted={}\n",
    "        self.stores_times={}\n",
    "        self.dfStore=None\n",
    "    def arimamodel(self, timeseriesarray):\n",
    "        autoarima_model = pmd.auto_arima(timeseriesarray, start_p=0, d=0, trace=1,\n",
    "                                         suppress_warnings=True, seasonal=True, m=12, D=1,\n",
    "                                         start_q=0, error_action='ignore', stationary=True, njob=-1,\n",
    "                                         test=\"adf\", stepwise=True)\n",
    "        return autoarima_model\n",
    "    def preprocessing(self, df_train):\n",
    "        temp = df_train  # np.log(df_train)\n",
    "        temp = temp.reshape((-1, 1))\n",
    "        return self.sc.fit_transform(temp)\n",
    "\n",
    "    def postprocess(self, prediction, df_test_org,dfx_test, q, wq, ensemble=False):\n",
    "        # prediction=np.exp(prediction)\n",
    "        if type(prediction)==pd.core.frame.DataFrame:\n",
    "            prediction = prediction.values.reshape(-1, 1)\n",
    "        else:prediction = prediction.reshape(-1, 1)\n",
    "        if not ensemble:\n",
    "            prediction = self.sc.inverse_transform(prediction)\n",
    "        re_elements = wq.shape[0]  # testY.shape[-1]\n",
    "        testing_predict = prediction.reshape(-1, )\n",
    "        #else:testing_predict = prediction\n",
    "        clus_wape,clus_smape,clus_pe=WAPE(dfx_test,testing_predict),SMAPE(dfx_test,testing_predict),PE(dfx_test,testing_predict)\n",
    "        \n",
    "        testing_predict = np.tile(testing_predict, (re_elements, 1))\n",
    "        testing_predict = testing_predict.T     \n",
    "        testing_predict *= q\n",
    "\n",
    "#         Wsmape, Wwape,Wpe = estimate(df_test_org.values, testing_predict, wq)\n",
    "#         smape_dis,wape_dis,pe_dis=metric_dist(df_test_org.values, testing_predict, wq,metric='smape'),metric_dist(df_test_org.values, testing_predict, wq,metric='wape') ,metric_dist(df_test_org.values, testing_predict, wq,metric='pe')\n",
    "        return clus_smape,clus_wape,clus_pe, prediction  # ,testing_predict\n",
    "\n",
    "    def training_infer(self, df_train, df_test, df_test_org,q, wq, forecast_period=30,lstm_pred=None):\n",
    "        '''\n",
    "        df_test: pandas (shape:(:,1))\n",
    "        df_test_org: pandas (shape(:,no_ts))\n",
    "        '''\n",
    "\n",
    "        # self.df_train_log,self.df_test_log=test_train_spl(self.df,ratio=.8)\n",
    "        ###\n",
    "        fh = np.arange(forecast_period) + 1\n",
    "        ###\n",
    "        df_train['daily_supply'] = self.preprocessing(df_train.values)\n",
    "        self.df_train = df_train\n",
    "        # print(df_train['daily_supply'])\n",
    "        df_store = df_test.to_frame()\n",
    "        df_store = df_store.rename(columns={0: 'daily_supply'})\n",
    "        ###################################################\n",
    "        #df_store['lstm']=lstm_pred\n",
    "        #print('naive')\n",
    "        # naive\n",
    "        start = time.time()\n",
    "        forecaster=NaiveForecaster(strategy=\"last\")\n",
    "        forecaster.fit(self.df_train)\n",
    "        y_pred = forecaster.predict(fh)\n",
    "        df_store['naive'] = y_pred.values\n",
    "        # print(df_store['snaive'])\n",
    "        # print(df_store)\n",
    "        clus_smape,clus_wape,clus_pe,df_store['naive'] = self.postprocess(df_store['naive'].values, df_test_org,df_test,q, wq)\n",
    "        \n",
    "        end = time.time()\n",
    "        self.clus_smape['naive']=clus_smape\n",
    "        self.clus_wape['naive']=clus_wape\n",
    "        self.clus_pe['naive']=clus_pe\n",
    "        self.stores_times['naive'] = end - start\n",
    "        self.models['naive'] = forecaster\n",
    "        \n",
    "\n",
    "        #print('snaive')\n",
    "        # snaive\n",
    "        start = time.time()\n",
    "#         y = self.pysnaive(self.df_train, 52, forecast_period)[1].iloc[:forecast_period]\n",
    "#         df_store['snaive'] = y.values\n",
    "        forecaster1=NaiveForecaster(strategy=\"last\",sp=12)\n",
    "        forecaster1.fit(self.df_train)\n",
    "        y_pred = forecaster1.predict(fh)\n",
    "        clus_smape,clus_wape,clus_pe,df_store['snaive'] = self.postprocess(y_pred.values, df_test_org,df_test,q, wq)\n",
    "        \n",
    "        end = time.time()\n",
    "        self.clus_smape['snaive']=clus_smape\n",
    "        self.clus_wape['snaive']=clus_wape\n",
    "        self.clus_pe['snaive']=clus_pe\n",
    "        \n",
    "        self.stores_times['snaive'] = end - start\n",
    "        self.models['snaive'] = forecaster1\n",
    "        \n",
    "        #print('arima')\n",
    "        start = time.time()\n",
    "        try: \n",
    "            model_fit = ARIMA(self.df_train, order=(1, 1, 1)).fit()\n",
    "            df_store['arima'], _, _ = model_fit.forecast(forecast_period)  # 95% conf\n",
    "            print('done')\n",
    "        except (np.linalg.linalg.LinAlgError,ValueError):\n",
    "            model_fit = self.arimamodel(self.df_train)\n",
    "            df_store['arima']=model_fit.predict(forecast_period)\n",
    "            print('done3')\n",
    "#         else:\n",
    "#             model_fit = ARIMA(self.df_train, order=(0, 0, 0)).fit()\n",
    "#             df_store['arima']=model_fit.predict(forecast_period)\n",
    "#             print('done3')\n",
    "            \n",
    "        \n",
    "        #df_store['arima']= model_fit.forecast(forecast_period).values  # 95% conf\n",
    "        clus_smape,clus_wape,clus_pe,df_store['arima'] = self.postprocess(df_store['arima'].values, df_test_org,df_test,q, wq)\n",
    "        \n",
    "        end = time.time()\n",
    "\n",
    "        self.models['arima'] = model_fit\n",
    "        # self.stores_mape['arima'] = mape\n",
    "        self.clus_smape['arima']=clus_smape\n",
    "        self.clus_wape['arima']=clus_wape\n",
    "        self.clus_pe['arima']=clus_pe\n",
    "        \n",
    "        self.stores_times['arima'] = end - start\n",
    "        self.models['arima'] = model_fit\n",
    "\n",
    "        #print('ses')\n",
    "        # ses\n",
    "        start = time.time()\n",
    "        fit3 = SimpleExpSmoothing(self.df_train, initialization_method=\"estimated\").fit()\n",
    "        df_store['ses'] = fit3.forecast(forecast_period).values\n",
    "        clus_smape,clus_wape,clus_pe,df_store['ses'] = self.postprocess(df_store['ses'].values, df_test_org,df_test,q, wq)\n",
    "        \n",
    "        end = time.time()\n",
    "        self.models['ses'] = fit3\n",
    "\n",
    "        #self.stores_mape['ses'] = mape\n",
    "        self.clus_smape['ses']=clus_smape\n",
    "        self.clus_wape['ses']=clus_wape\n",
    "        self.clus_pe['ses']=clus_pe\n",
    "        self.stores_times['ses'] = end - start\n",
    "        #print('holts')\n",
    "        # holts\n",
    "        start = time.time()\n",
    "        try:\n",
    "            fit3 = Holt(self.df_train, damped_trend=True, exponential=True, initialization_method=\"estimated\").fit(\n",
    "                smoothing_level=0.8, smoothing_trend=0.1)\n",
    "        except (np.linalg.linalg.LinAlgError,ValueError):\n",
    "            fit3 = Holt(self.df_train, damped_trend=True, exponential=False, initialization_method=\"estimated\").fit(\n",
    "                smoothing_level=0.8, smoothing_trend=0.1)\n",
    "        df_store['holts'] = fit3.forecast(forecast_period).values\n",
    "        clus_smape,clus_wape,clus_pe,df_store['holts'] = self.postprocess(df_store['holts'].values, df_test_org,df_test,q, wq)\n",
    "        \n",
    "        end = time.time()\n",
    "\n",
    "        self.models['holts'] = fit3\n",
    "        #self.stores_mape['holts'] = mape\n",
    "        self.clus_smape['holts']=clus_smape\n",
    "        self.clus_wape['holts']=clus_wape\n",
    "        self.clus_pe['holts']=clus_pe\n",
    "        self.stores_times['holts'] = end - start\n",
    "        #print('Exp_smoothing')\n",
    "        # Exp_smoothing\n",
    "        start = time.time()\n",
    "        try:\n",
    "            fit4 = ExponentialSmoothing(self.df_train, seasonal_periods=52, trend=\"add\", seasonal=\"add\",\n",
    "                                        damped_trend=True, use_boxcox=True, initialization_method=\"estimated\", ).fit()\n",
    "        except:\n",
    "            fit4 = ExponentialSmoothing(self.df_train, seasonal_periods=52, trend=\"add\", seasonal=\"add\",\n",
    "                                        damped_trend=True, use_boxcox=False, initialization_method=\"estimated\", ).fit()\n",
    "        df_store['Es'] = fit4.forecast(forecast_period).values\n",
    "        clus_smape,clus_wape,clus_pe,df_store['Es'] = self.postprocess(df_store['Es'].values, df_test_org,df_test,q, wq)\n",
    "        \n",
    "        end = time.time()\n",
    "        self.models['Es'] = fit4\n",
    "        #self.stores_mape['Es'] = mape\n",
    "        self.clus_smape['Es']=clus_smape\n",
    "        self.clus_wape['Es']=clus_wape\n",
    "        self.clus_pe['Es']=clus_pe\n",
    "        self.stores_times['Es'] = end - start\n",
    "\n",
    "        \n",
    "        #print('Auto_ETS')\n",
    "        # Auto_ETS\n",
    "        start = time.time()\n",
    "        try: \n",
    "            forecaster = AutoETS(auto=True, sp=12, n_jobs=-1, error='add', trend='True', damped_trend='True', seasonal='add')\n",
    "            forecaster.fit(self.df_train)\n",
    "        except (ValueError):\n",
    "            forecaster = AutoETS(auto=False, sp=7, n_jobs=-1, error='add')\n",
    "            forecaster.fit(self.df_train)\n",
    "        df_store['auto_ets'] = forecaster.predict(fh).values\n",
    "        clus_smape,clus_wape,clus_pe,df_store['auto_ets'] = self.postprocess(df_store['auto_ets'].values, df_test_org,df_test,q, wq)\n",
    "        \n",
    "        end = time.time()\n",
    "        self.models['auto_ets'] = forecaster\n",
    "        #self.stores_mape['auto_ets'] = mape\n",
    "        self.clus_smape['auto_ets']=clus_smape\n",
    "        self.clus_wape['auto_ets']=clus_wape\n",
    "        self.clus_pe['auto_ets']=clus_pe\n",
    "        self.stores_times['auto_ets'] = end - start\n",
    "        \n",
    "        #theta\n",
    "        start = time.time()\n",
    "        forecaster4 = ThetaForecaster(sp=52, deseasonalize=False)\n",
    "        forecaster4.fit(self.df_train)\n",
    "        df_store['theta'] = forecaster4.predict(fh).values\n",
    "        clus_smape,clus_wape,clus_pe,df_store['theta'] = self.postprocess(df_store['theta'].values, df_test_org,df_test,q, wq)\n",
    "        \n",
    "        end = time.time()\n",
    "\n",
    "        self.models['theta'] = forecaster4\n",
    "        #self.stores_mape['theta'] = mape\n",
    "        self.clus_smape['theta']=clus_smape\n",
    "        self.clus_wape['theta']=clus_wape\n",
    "        self.clus_pe['theta']=clus_pe\n",
    "\n",
    "        self.stores_times['theta'] = end - start\n",
    "\n",
    "        #print('Croston')\n",
    "        # Croston\n",
    "        start = time.time()\n",
    "        forecaster5 = Croston(smoothing=.2)\n",
    "        forecaster5.fit(self.df_train)\n",
    "        df_store['croston'] = forecaster5.predict(fh).values\n",
    "        clus_smape,clus_wape,clus_pe,df_store['croston'] = self.postprocess(df_store['croston'].values, df_test_org,df_test,q, wq)\n",
    "        \n",
    "        end = time.time()\n",
    "\n",
    "        self.models['croston'] = forecaster5\n",
    "        #self.stores_mape['croston'] = mape\n",
    "        self.clus_smape['croston']=clus_smape\n",
    "        self.clus_wape['croston']=clus_wape\n",
    "        self.clus_pe['croston']=clus_pe\n",
    "        self.stores_times['croston'] = end - start\n",
    "\n",
    "        #prophet\n",
    "        start = time. time()\n",
    "        df_train=self.df_train.squeeze()\n",
    "        df_train.index.name='Period'\n",
    "        df_train.index=df_train.index.to_timestamp()\n",
    "        forecaster6 = Prophet(seasonality_mode='additive',n_changepoints=int(len(df_train) / 12),\n",
    "            add_country_holidays={'country_name': 'US'},yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=True)\n",
    "        forecaster6.fit(df_train)\n",
    "        df_store['prophet'] = forecaster6.predict(fh).values\n",
    "        clus_smape,clus_wape,clus_pe,df_store['prophet'] = self.postprocess(df_store['prophet'].values, df_test_org,df_test,q, wq)\n",
    "        \n",
    "        end = time.time()\n",
    "\n",
    "        self.models['prophet'] = forecaster6\n",
    "        # self.stores_mape['croston'] = mape\n",
    "        self.clus_smape['prophet']=clus_smape\n",
    "        self.clus_wape['prophet']=clus_wape\n",
    "        self.clus_pe['prophet']=clus_pe\n",
    "        self.stores_times['prophet'] = end - start\n",
    "\n",
    "        #print('Average')\n",
    "        # Average of stats models\n",
    "        tog_time = np.sum(list(self.stores_times.values()))\n",
    "        start = time.time()\n",
    "        df_temp = df_store.drop(['daily_supply'], axis=1)  # df_store.loc[:, df_store.columns != 'daily_supply']\n",
    "        df_store['Avg_ensemble'] = df_temp.mean(axis=1).values\n",
    "\n",
    "        clus_smape,clus_wape,clus_pe,df_store['Avg_ensemble'] = self.postprocess(df_store['Avg_ensemble'].values, df_test_org,df_test,q, wq,ensemble=True)\n",
    "        \n",
    "        end = time.time()\n",
    "        #self.stores_mape['Avg_ensemble'] = mape\n",
    "        self.clus_smape['Avg_ensemble']=clus_smape\n",
    "        self.clus_wape['Avg_ensemble']=clus_wape\n",
    "        self.clus_pe['Avg_ensemble']=clus_pe\n",
    "        self.stores_times['Avg_ensemble'] = (end - start) + tog_time-self.stores_times['Lstm']#%%%%%%%%%%%%\n",
    "        self.dfStore=df_store\n",
    "\n",
    "    def weighted_avg(self,df_train, df_test,df_val_org, df_test_org,q, wq, forecast_period=60,lstm_pred=None):\n",
    "        \n",
    "        fh = np.arange(forecast_period) + 1\n",
    "        ###\n",
    "        df_train['daily_supply'] = self.preprocessing(df_train.values)\n",
    "        self.df_trainW = df_train\n",
    "        # print(df_train['daily_supply'])\n",
    "        df_store = df_test.to_frame()\n",
    "        df_store = df_store.rename(columns={0: 'daily_supply'})\n",
    "        #############################################\n",
    "        start = time.time()\n",
    "        forecaster=NaiveForecaster(strategy=\"last\")\n",
    "        forecaster.fit(self.df_trainW)\n",
    "        y_pred = forecaster.predict(fh)\n",
    "\n",
    "        smape,wape,pe,_= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq)\n",
    "        self.wape_Weighted['naive'] = (smape+wape+pe)/3\n",
    "\n",
    "        \n",
    "\n",
    "        #print('snaive')\n",
    "        # snaive\n",
    "       # start = time.time()\n",
    "#         y = self.pysnaive(self.df_train, 52, forecast_period)[1].iloc[:forecast_period]\n",
    "#         df_store['snaive'] = y.values\n",
    "        forecaster1=NaiveForecaster(strategy=\"last\",sp=12)\n",
    "        forecaster1.fit(self.df_trainW)\n",
    "        y_pred = forecaster1.predict(fh)\n",
    "        \n",
    "        smape,wape,pe,_= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq)\n",
    "        self.wape_Weighted['snaive'] = (smape+wape+pe)/3\n",
    "        \n",
    "        #print('arima')\n",
    "        #start = time.time()\n",
    "        try: \n",
    "            model_fit = ARIMA(self.df_trainW, order=(1, 1, 1)).fit()\n",
    "            y_pred, _, _ = model_fit.forecast(forecast_period)  # 95% conf\n",
    "        except (np.linalg.linalg.LinAlgError,ValueError):\n",
    "            model_fit = self.arimamodel(self.df_trainW)\n",
    "            y_pred=model_fit.predict(forecast_period)\n",
    "        \n",
    "        smape,wape,pe,_= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq)\n",
    "        self.wape_Weighted['arima'] = (smape+wape+pe)/3\n",
    "    \n",
    "\n",
    "        fit3 = SimpleExpSmoothing(self.df_trainW, initialization_method=\"estimated\").fit()\n",
    "        y_pred = fit3.forecast(forecast_period).values\n",
    "        \n",
    "        smape,wape,pe,_= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq)\n",
    "        self.wape_Weighted['ses'] = (smape+wape+pe)/3\n",
    "        \n",
    "        #print('holts')\n",
    "        # holts\n",
    "        start = time.time()\n",
    "        try:\n",
    "            fit3 = Holt(self.df_trainW, damped_trend=True, exponential=True, initialization_method=\"estimated\").fit(\n",
    "                smoothing_level=0.8, smoothing_trend=0.1)\n",
    "        except (np.linalg.linalg.LinAlgError,ValueError):\n",
    "            fit3 = Holt(self.df_trainW, damped_trend=True, exponential=False, initialization_method=\"estimated\").fit(\n",
    "                smoothing_level=0.8, smoothing_trend=0.1)\n",
    "        y_pred = fit3.forecast(forecast_period).values\n",
    "        \n",
    "        smape,wape,pe,_= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq)\n",
    "        self.wape_Weighted['holts'] = (smape+wape+pe)/3\n",
    "        # Exp_smoothing\n",
    "        #start = time.time()\n",
    "        try:\n",
    "            fit4 = ExponentialSmoothing(self.df_trainW, seasonal_periods=52, trend=\"add\", seasonal=\"add\",\n",
    "                                        damped_trend=True, use_boxcox=True, initialization_method=\"estimated\", ).fit()\n",
    "        except:\n",
    "            fit4 = ExponentialSmoothing(self.df_trainW, seasonal_periods=52, trend=\"add\", seasonal=\"add\",\n",
    "                                        damped_trend=True, use_boxcox=False, initialization_method=\"estimated\", ).fit()\n",
    "        y_pred = fit4.forecast(forecast_period).values\n",
    "        \n",
    "        smape,wape,pe,_= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq)\n",
    "        self.wape_Weighted['Es'] = (smape+wape+pe)/3\n",
    "\n",
    "        \n",
    "        #print('Auto_ETS')\n",
    "        # Auto_ETS\n",
    "        #start = time.time()\n",
    "        try:\n",
    "            forecaster = AutoETS(auto=True, sp=12, n_jobs=-1, error='add', trend='True', damped_trend='True', seasonal='add')\n",
    "            forecaster.fit(self.df_trainW)\n",
    "        except (ValueError):\n",
    "            forecaster = AutoETS(auto=False, sp=7, n_jobs=-1, error='add')\n",
    "            forecaster.fit(self.df_trainW)\n",
    "        y_pred = forecaster.predict(fh).values\n",
    "        \n",
    "        smape,wape,pe,_= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq)\n",
    "        self.wape_Weighted['auto_ets'] = (smape+wape+pe)/3\n",
    "        \n",
    "        #theta\n",
    "        #start = time.time()\n",
    "        forecaster4 = ThetaForecaster(sp=52, deseasonalize=False)\n",
    "        forecaster4.fit(self.df_trainW)\n",
    "        y_pred = forecaster4.predict(fh).values\n",
    "        \n",
    "        smape,wape,pe,_= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq)\n",
    "        self.wape_Weighted['theta'] = (smape+wape+pe)/3\n",
    "\n",
    "        #print('Croston')\n",
    "        # Croston\n",
    "        #start = time.time()\n",
    "        forecaster5 = Croston(smoothing=.2)\n",
    "        forecaster5.fit(self.df_trainW)\n",
    "        y_pred = forecaster5.predict(fh).values\n",
    "        \n",
    "        smape,wape,pe,_= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq)\n",
    "        self.wape_Weighted['croston'] = (smape+wape+pe)/3\n",
    "\n",
    "        #prophet\n",
    "        #start = time. time()\n",
    "        df_train=self.df_trainW.squeeze()\n",
    "        df_train.index.name='Period'\n",
    "        df_train.index=df_train.index.to_timestamp()\n",
    "        forecaster6 = Prophet(seasonality_mode='additive',n_changepoints=int(len(df_train) / 12),\n",
    "            add_country_holidays={'country_name': 'US'},yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=True)\n",
    "        forecaster6.fit(df_train)\n",
    "        y_pred = forecaster6.predict(fh).values\n",
    "        \n",
    "        smape,wape,pe,_= self.postprocess(y_pred[:30], df_val_org,df_val_org,q, wq)\n",
    "        self.wape_Weighted['prophet'] = (smape+wape+pe)/3\n",
    "        \n",
    "        store_error = self.wape_Weighted.copy()\n",
    "\n",
    "        for k, v in self.wape_Weighted.items():\n",
    "            store_error[k] = 1 / v\n",
    "        norms = list(store_error.values())\n",
    "        norm = 0\n",
    "        for i in norms: norm += i\n",
    "        for k, v in store_error.items():\n",
    "            store_error[k] = v / norm\n",
    "        self.store_error = store_error\n",
    "        #print('weighted')\n",
    "        # weighted average\n",
    "        #del self.dfStore['daily_supply']\n",
    "        print(self.store_error)\n",
    "        subset = self.dfStore.drop(['daily_supply', 'Avg_ensemble'], axis=1)#.columns\n",
    "        self.dfStore['Weighted_ensemble'] = (subset * store_error).sum(1).values\n",
    "        clus_smape,clus_wape,clus_pe,self.dfStore['Weighted_ensemble'] = self.postprocess(self.dfStore['Weighted_ensemble'].values, df_test_org,df_test,q, wq,ensemble=True)\n",
    "        \n",
    "        end = time.time()\n",
    "        self.clus_smape['Weighted_ensemble']=clus_smape\n",
    "        self.clus_wape['Weighted_ensemble']=clus_wape\n",
    "        self.clus_pe['Weighted_ensemble']=clus_pe\n",
    "        self.stores_times['Weighted_ensemble'] = (end - start)#+self.stores_times['Lstm']\n",
    "        #self.dfStore['Weighted_ensemble']=df_store['Weighted_ensemble'].values\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_train_test_data(df, window_size=30,val_req=False,start='2020-11-01'):\n",
    "    start = pd.to_datetime(start, format='%Y-%m-%d') #+ relativedelta(months=months)\n",
    "    end = start + relativedelta(days=365 + 30) #+ relativedelta(months=months)    \n",
    "    df3 = df.astype({'query_idx': 'string'})\n",
    "    df3.sort_values('ds', inplace=True)\n",
    "    df1 = pd.pivot_table(df3, values='daily_supply', index=['ds'],\n",
    "                         columns=['group_id', 'query_idx', 'SearchKeyValue', 'LocationKeyValue'])\n",
    "\n",
    "    df1.columns = ['_'.join(col) for col in df1.columns.values]\n",
    "    print('Total TS before filter: ', df1.shape[1])\n",
    "\n",
    "    df1.index = pd.to_datetime(df1.index)\n",
    "\n",
    "#     start = df1.index[0]#****\n",
    "#     end = df1.index[-1]#*****\n",
    "            \n",
    "    \n",
    "    #start,end=pd.to_datetime('2020-05-02',format='%Y-%m-%d'),pd.to_datetime('2022-05-02',format='%Y-%m-%d')\n",
    "#     start1=df1.index.sort_values()[0]\n",
    "#     end1=df1.index.sort_values()[len(df1)-1]\n",
    "#     #print(df1.index.sort_values())\n",
    "#     if end1<end:end=end1\n",
    "#     if start1>start:start=start1\n",
    "    df1=df1[(df1.index>=start) & (df1.index<=end)]\n",
    "    df1.sort_index(inplace=True)\n",
    "\n",
    "    train_test_split = end - pd.to_timedelta(30, unit='d')  ############30 needs to be a variable\n",
    "    # start_temp2 = pd.to_datetime((train_test_split + relativedelta(days=-4 * window_size)).strftime('%Y-%m-%d'))\n",
    "\n",
    "    end_train = pd.to_datetime((train_test_split).strftime('%Y-%m-%d'))\n",
    "    start_test = pd.to_datetime((train_test_split + relativedelta(days=1)).strftime('%Y-%m-%d'))\n",
    "    #print(df1.index.unique())\n",
    "    dates = pd.date_range(start=start, end=end, freq='D').format()\n",
    "    s = pd.Series(np.nan, index=pd.date_range(start, end, freq='D'))\n",
    "    #df1['ds']=pd.to_datetime(df_['ds'])\n",
    "    #df_.set_index('ds',inplace=True)\n",
    "    df1=pd.concat([df1,s[~s.index.isin(df1.index)]]).sort_index()\n",
    "    \n",
    "    #print(df1)\n",
    "    #del df1['0']\n",
    "    #dates = pd.date_range(start=start, end=end, freq='D').format()\n",
    "    #df1=df1.reindex(dates, fill_value=0)\n",
    "    df1.index = pd.to_datetime(df1.index)\n",
    "    df1 = df1.fillna(0)\n",
    "#########################    \n",
    "    #df1 = df1.loc[:, df1.isin([0]).sum(axis=0) <= 50]\n",
    "########################\n",
    "    #     df1=df1.loc[:,df1.notna().sum()>90]\n",
    "    #     df1 = df1.fillna(0)\n",
    "    del df1[0]\n",
    "    # impute_missing_time_modify(temp, train_test_split, end, 4, train=True)\n",
    "    df_train, df_test = df1[(df1.index <= end_train)], df1[df1.index >= start_test]\n",
    "    # temp_train, temp_test = temp[temp['ds'] < train_test_split], temp[temp['ds'] >= train_test_split]\n",
    "    print('Total TS after filter: ', df1.shape[1])\n",
    "    df_train.sort_index(inplace=True)\n",
    "    #print(df_train.iloc[-14:,:])\n",
    "    \n",
    "    #df_train_temp=df_train_temp[(df_train_temp.index <= end_train)], df_train_temp[df_train_temp.index >= start_test]\n",
    "    \n",
    "    qtrain = df_train.iloc[-14:,:].sum(axis=0) / np.sum(df_train.iloc[-14:,:].sum(axis=0))\n",
    "    qtest = df_test.sum(axis=0) / np.sum(df_test.sum(axis=0))  # df_train.iloc[-30:,].sum(axis=0)/np.sum(df_train.iloc[-30:,].sum(axis=0))*100\n",
    "    # average=df_train.mean(axis=0).values\n",
    "    # df_train+=average\n",
    "    # df_test+= average\n",
    "    train_date, test_date = df_train.index, df_test.index\n",
    "    df_train.reset_index(inplace=True, drop=True)\n",
    "    df_test.reset_index(inplace=True, drop=True)\n",
    "    # df.values.sum()\n",
    "    # q=df.sum(axis=1)/np.sum(np.sum(df))\n",
    "    # print(q)\n",
    "    if val_req:\n",
    "        df_train,df_val=df_train.iloc[:-30,:],df_train.iloc[-30:,:]\n",
    "        return df_train, df_val,df_test, train_date.values[:-30], test_date.values, qtrain, qtest\n",
    "    return df_train, df_test, train_date.values, test_date.values, qtrain, qtest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Times=[]\n",
    "AggSmape=[]\n",
    "AggWape=[]\n",
    "AggPe=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# month=0\n",
    "# start_month[month]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "counter=0\n",
    "for i in range(50):\n",
    "    if i not in [44,47,48]:\n",
    "        st=time.time()\n",
    "        print('cluster id: ',i+1)\n",
    "        current_date = '20220831'\n",
    "        test_group_id = test_group_id_list[i]#'/3944/1060825/447913'\n",
    "        print('group id',test_group_id)\n",
    "        #for top cluster:\n",
    "        #path = f\"gs://adtech-artifacts/ds/forecast/month_eval/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_tft_data_{current_date}.csv\"\n",
    "        path = f\"gs://adtech-artifacts/ds/forecast/month_eval/tail_clusters/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_data_20220831.csv\"\n",
    "        # read in data\n",
    "        df = spark.read.option(\"header\", True).csv(path).toPandas() # The usage should be the same as previous\n",
    "        df=df[['ds', 'group_id', 'query_idx', 'SearchKeyValue',  'LocationKeyValue', 'daily_supply']]\n",
    "        #print('query id: ',df['query_idx'].unique())\n",
    "        #df_panda=spark.read.csv(\"gs://adtech-artifacts/ds/forecast/query_taxonomy_sample_Addedfeatures_cluster_\"+str(i)+\".csv\",header=True).toPandas()\n",
    "        Cols=['daily_supply']#, 'shifted_annual_daily_supply','Trend_Var','dayofyear_sin', 'dayofyear_cos', 'month_sin', 'month_cos']\n",
    "        for cols in Cols:df[cols]=df[cols].astype(float,errors='raise')\n",
    "        #print('time needed to preprocess the data is: ',time.time()-st)\n",
    "        df_train,df_test,train_date,test_date,q,wq=_generate_train_test_data(df,start=start_month[month])\n",
    "        df_trainW,df_valW,_,train_dateW,_,_,_=_generate_train_test_data(df,val_req=True,start=start_month[month])\n",
    "        df1_train,df1_test=df_train.sum(axis=1),df_test.sum(axis=1)\n",
    "        df1_trainW,df_valW=df_trainW.sum(axis=1),df_valW.sum(axis=1)\n",
    "        df1_train,df1_trainW,df_valW=df1_train.to_frame(),df1_trainW.to_frame(),df_valW.to_frame()\n",
    "        df1_train.index=train_date\n",
    "        df1_trainW.index=train_dateW\n",
    "        df1_train.index = df1_train.index.to_period(freq = 'D')\n",
    "        df1_trainW.index = df1_trainW.index.to_period(freq = 'D')\n",
    "        #df_test_log_temp.index = df_test_log_temp.index.to_period(freq = 'D')\n",
    "        df1_train=df1_train.rename(columns={0:'daily_supply'})\n",
    "        df1_trainW=df1_trainW.rename(columns={0:'daily_supply'})\n",
    "    #     if i==0:\n",
    "    #         print(q,wq)\n",
    "        start=time.time()\n",
    "        Models=statModels()\n",
    "\n",
    "        Models.clus_smape['Lstm']=AggSmapeL[counter]\n",
    "        Models.clus_wape['Lstm']=AggWapeL[counter]\n",
    "        Models.clus_pe['Lstm']=AggPeL[counter]\n",
    "        #Models.models['Lstm'] = fit4\n",
    "        #self.stores_mape['Es'] = mape\n",
    "        Models.stores_times['Lstm'] = TimesL[counter]\n",
    "        ##################\n",
    "        #Models.wape_Weighted['Lstm'] = wape_agg_lstm[i]\n",
    "\n",
    "        Models.training_infer(df1_train,df1_test,df_test,q.values,wq.values,df1_test.shape[0],df_lstm[:,i])#no_ts\n",
    "                             #(df_train, df_test, df_test_org,q, wq, forecast_period=30,lstm_pred=None)\n",
    "        Models.weighted_avg(df1_trainW, df1_test,df_valW.values, df_test,q.values,wq.values, forecast_period=60,lstm_pred=df_lstm[:,i])\n",
    "\n",
    "        end=time.time()\n",
    "        print('Wape is: ',Models.clus_wape)\n",
    "        print('Smape is: ',Models.clus_smape)\n",
    "        print('Pe is: ',Models.clus_smape)\n",
    "        print('Time is:' ,Models.clus_pe)\n",
    "        #print('time needed to run the model: ',end-start)\n",
    "        Times.append(Models.stores_times)\n",
    "        AggSmape.append(Models.clus_smape)\n",
    "        AggWape.append(Models.clus_wape)\n",
    "        AggPe.append(Models.clus_pe)\n",
    "        #training_j.append(i)\n",
    "        counter+=1\n",
    "    print('===============================================')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counter,len(AggPe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ls=[AggWape[i]['Avg_ensemble'] for i in range(len(AggWape))]\n",
    "# res=[]\n",
    "# for i in range(6):\n",
    "#     res.append(np.argmax(Ls))\n",
    "#     print(np.argmax(Ls),':',Ls[np.argmax(Ls)])\n",
    "#     Ls[np.argmax(Ls)]=0\n",
    "# Ls=[AggWape[i]['Avg_ensemble'] for i in range(len(AggWape))]\n",
    "# for i in range(3):\n",
    "#     res.append(np.argmin(Ls))\n",
    "#     print(np.argmin(Ls),':',Ls[np.argmin(Ls)])\n",
    "#     Ls[np.argmin(Ls)]=1\n",
    "\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res[4]=45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(nrows=3, ncols=3)\n",
    "# sh=[[i,j] for i in range(3) for j in range(3)]\n",
    "# counter=0\n",
    "# matplotlib.rcParams['figure.figsize'] = (30, 20)\n",
    "\n",
    "# for i in res:\n",
    "#     if i not in [44,47,48]:\n",
    "#         st=time.time()\n",
    "#         print('cluster id: ',i+1)\n",
    "#         current_date = '20220831'\n",
    "#         test_group_id = test_group_id_list[i]#'/3944/1060825/447913'\n",
    "#         print('group id',test_group_id)\n",
    "#         #for top cluster:\n",
    "#         #path = f\"gs://adtech-artifacts/ds/forecast/month_eval/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_tft_data_{current_date}.csv\"\n",
    "#         path = f\"gs://adtech-artifacts/ds/forecast/month_eval/tail_clusters/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_data_20220831.csv\"\n",
    "#         # read in data\n",
    "#         df = spark.read.option(\"header\", True).csv(path).toPandas() # The usage should be the same as previous\n",
    "#         df=df[['ds', 'group_id', 'query_idx', 'SearchKeyValue',  'LocationKeyValue', 'daily_supply']]\n",
    "#         #print('query id: ',df['query_idx'].unique())\n",
    "#         #df_panda=spark.read.csv(\"gs://adtech-artifacts/ds/forecast/query_taxonomy_sample_Addedfeatures_cluster_\"+str(i)+\".csv\",header=True).toPandas()\n",
    "#         Cols=['daily_supply']#, 'shifted_annual_daily_supply','Trend_Var','dayofyear_sin', 'dayofyear_cos', 'month_sin', 'month_cos']\n",
    "#         for cols in Cols:df[cols]=df[cols].astype(float,errors='raise')\n",
    "#         #print('time needed to preprocess the data is: ',time.time()-st)\n",
    "#         df_train,df_test,train_date,test_date,q,wq=_generate_train_test_data(df,start=start_month[month])\n",
    "#         df_trainW,df_valW,_,train_dateW,_,_,_=_generate_train_test_data(df,val_req=True,start=start_month[month])\n",
    "#         df1_train,df1_test=df_train.sum(axis=1),df_test.sum(axis=1)\n",
    "#         df1_trainW,df_valW=df_trainW.sum(axis=1),df_valW.sum(axis=1)\n",
    "#         df1_train,df1_trainW,df_valW=df1_train.to_frame(),df1_trainW.to_frame(),df_valW.to_frame()\n",
    "#         df1_train.index=train_date\n",
    "#         df1_trainW.index=train_dateW\n",
    "        \n",
    "#         df1_train.index = df1_train.index.to_period(freq = 'D')\n",
    "#         df1_trainW.index = df1_trainW.index.to_period(freq = 'D')\n",
    "#         #df_test_log_temp.index = df_test_log_temp.index.to_period(freq = 'D')\n",
    "#         df1_train=df1_train.rename(columns={0:'daily_supply'})\n",
    "#         df1_trainW=df1_trainW.rename(columns={0:'daily_supply'})\n",
    "#         start=time.time()\n",
    "#         df_train=df1_train.copy()\n",
    "#         Models=statModels()\n",
    "        \n",
    "#         Models.clus_smape['Lstm']=0\n",
    "#         Models.clus_wape['Lstm']=0\n",
    "#         Models.clus_pe['Lstm']=0\n",
    "#         #Models.models['Lstm'] = fit4\n",
    "#         #self.stores_mape['Es'] = mape\n",
    "#         Models.stores_times['Lstm'] = 0\n",
    "        \n",
    "#         Models.training_infer(df1_train,df1_test,df_test,q.values,wq.values,df1_test.shape[0],df_lstm[:,i])#no_ts\n",
    "#                              #(df_train, df_test, df_test_org,q, wq, forecast_period=30,lstm_pred=None)\n",
    "#         Models.weighted_avg(df1_trainW, df1_test,df_valW.values, df_test,q.values,wq.values, forecast_period=60,lstm_pred=df_lstm[:,i])\n",
    "\n",
    "#         end=time.time()\n",
    "#         print('Wape is: ',Models.clus_wape)\n",
    "#         print('Smape is: ',Models.clus_smape)\n",
    "#         print('Pe is: ',Models.clus_smape)\n",
    "#         print('Time is:' ,Models.clus_pe)\n",
    "#         df1_test=df1_test.to_frame()\n",
    "#         df1_test.index=test_date\n",
    "#         df1_test=df1_test.rename(columns={0:'daily_supply_test'})\n",
    "#         df_train=df_train.rename(columns={'daily_supply':'daily_supply_train'})\n",
    "#         #print('time needed to run the model: ',end-start)\n",
    "#         Times.append(Models.stores_times)\n",
    "#         AggSmape.append(Models.clus_smape)\n",
    "#         AggWape.append(Models.clus_wape)\n",
    "#         AggPe.append(Models.clus_pe)\n",
    "#         pred=Models.dfStore['Avg_ensemble']\n",
    "#         pred.index=test_date\n",
    "#         pred=pred.to_frame()\n",
    "#         pred=pred.rename(columns={'Avg_ensemble':'daily_supply_pred'})\n",
    "\n",
    "#         df_train.plot(title='cluster id: '+str(i)+ ' with Agg WAPE: '+str(Models.clus_wape['Avg_ensemble']),ax=axes[sh[counter][0],sh[counter][1]])\n",
    "#         df1_test.plot(ax=axes[sh[counter][0],sh[counter][1]])\n",
    "#         pred.plot(ax=axes[sh[counter][0],sh[counter][1]])\n",
    "#         counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Time={i:[] for i in Times[0]}\n",
    "\n",
    "AggSmapes={i:[] for i in AggSmape[0]}\n",
    "AggWapes={i:[] for i in AggWape[0]}\n",
    "AggPes={i:[] for i in AggPe[0]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(counter):\n",
    "\n",
    "    for keys,values in Times[i].items():\n",
    "        Time[keys].append(values)\n",
    "    for keys,values in AggSmape[i].items():\n",
    "        AggSmapes[keys].append(values)\n",
    "    for keys,values in AggWape[i].items():\n",
    "        AggWapes[keys].append(values)\n",
    "    for keys,values in AggPe[i].items():\n",
    "        AggPes[keys].append(values)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Time is: ',Time)\n",
    "print('====================')\n",
    "print('AggSmapes is: ',AggSmapes)\n",
    "print('====================')\n",
    "print('AggWapes is: ',AggWapes)\n",
    "print('====================')\n",
    "print('AggPes is: ',AggPes)\n",
    "print('====================')\n",
    "\n",
    "print('====================')\n",
    "print('Original LSTM time is', times_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(AggPes[\"Lstm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "avg_,med_,e80_,e90_,name=[],[],[],[],[]\n",
    "#a=[MAPE_clus,MAPE_agg,WAPE_clus,WAPE_agg]\n",
    "for k,v in Time.items():\n",
    "    avg_.append(np.mean(v))\n",
    "    med_.append(np.percentile(v,50))\n",
    "    e80_.append(np.percentile(v,80))\n",
    "    e90_.append(np.percentile(v,90))\n",
    "    name.append(k)\n",
    "\n",
    "\n",
    "\n",
    "x_axis = np.arange(len(Time))\n",
    "\n",
    "# Multi bar Chart\n",
    "\n",
    "bar_w=.1\n",
    "plt.bar(x_axis -0.15, avg_, width=bar_w, label = 'Avg',color='red')\n",
    "plt.bar(x_axis -0.05, med_, width=bar_w, label = 'Median',color='green')\n",
    "plt.bar(x_axis +0.05, e80_, width=bar_w, label = '80-%ile',color='cyan')\n",
    "plt.bar(x_axis +0.15, e90_, width=bar_w, label = '90-%tile',color='black')\n",
    "\n",
    "plt.xticks(x_axis, Time.keys(),rotation = 70)\n",
    "plt.legend(bbox_to_anchor=(0.41,0.65))\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.xlabel('Model')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "avg_,med_,e80_,e90_,name=[],[],[],[],[]\n",
    "#a=[MAPE_clus,MAPE_agg,WAPE_clus,WAPE_agg]\n",
    "for k,v in AggPes.items():\n",
    "    avg_.append(np.mean(v))\n",
    "    med_.append(np.percentile(v,50))\n",
    "    e80_.append(np.percentile(v,80))\n",
    "    e90_.append(np.percentile(v,90))\n",
    "    name.append(k)\n",
    "\n",
    "\n",
    "\n",
    "x_axis = np.arange(len(AggPes))\n",
    "\n",
    "# Multi bar Chart\n",
    "plt.bar(x_axis -0.15, avg_, width=bar_w, label = 'Avg',color='red')\n",
    "plt.bar(x_axis -0.05, med_, width=bar_w, label = 'Median',color='green')\n",
    "plt.bar(x_axis +0.05, e80_, width=bar_w, label = '80-%ile',color='cyan')\n",
    "plt.bar(x_axis +0.15, e90_, width=bar_w, label = '90-%tile',color='black')\n",
    "\n",
    "plt.xticks(x_axis, AggPes.keys(),rotation = 70)\n",
    "plt.legend(bbox_to_anchor=(0.41,0.65))\n",
    "plt.ylabel('Agg Weighted PE')\n",
    "plt.xlabel('Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "avg_,med_,e80_,e90_,name=[],[],[],[],[]\n",
    "#a=[MAPE_clus,MAPE_agg,WAPE_clus,WAPE_agg]\n",
    "for k,v in AggSmapes.items():\n",
    "    avg_.append(np.mean(v))\n",
    "    med_.append(np.percentile(v,50))\n",
    "    e80_.append(np.percentile(v,80))\n",
    "    e90_.append(np.percentile(v,90))\n",
    "    name.append(k)\n",
    "\n",
    "\n",
    "\n",
    "x_axis = np.arange(len(AggPes))\n",
    "\n",
    "# Multi bar Chart\n",
    "plt.bar(x_axis -0.15, avg_, width=bar_w, label = 'Avg',color='red')\n",
    "plt.bar(x_axis -0.05, med_, width=bar_w, label = 'Median',color='green')\n",
    "plt.bar(x_axis +0.05, e80_, width=bar_w, label = '80-%ile',color='cyan')\n",
    "plt.bar(x_axis +0.15, e90_, width=bar_w, label = '90-%tile',color='black')\n",
    "\n",
    "plt.xticks(x_axis, AggPes.keys(),rotation = 70)\n",
    "plt.legend(bbox_to_anchor=(0.41,0.65))\n",
    "plt.ylabel('Agg Weighted SMAPE')\n",
    "plt.xlabel('Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "avg_,med_,e80_,e90_,name=[],[],[],[],[]\n",
    "#a=[MAPE_clus,MAPE_agg,WAPE_clus,WAPE_agg]\n",
    "for k,v in AggWapes.items():\n",
    "    avg_.append(np.mean(v))\n",
    "    med_.append(np.percentile(v,50))\n",
    "    e80_.append(np.percentile(v,80))\n",
    "    e90_.append(np.percentile(v,90))\n",
    "    name.append(k)\n",
    "\n",
    "\n",
    "\n",
    "x_axis = np.arange(len(AggPes))\n",
    "\n",
    "# Multi bar Chart\n",
    "plt.bar(x_axis -0.15, avg_, width=bar_w, label = 'Avg',color='red')\n",
    "plt.bar(x_axis -0.05, med_, width=bar_w, label = 'Median',color='green')\n",
    "plt.bar(x_axis +0.05, e80_, width=bar_w, label = '80-%ile',color='cyan')\n",
    "plt.bar(x_axis +0.15, e90_, width=bar_w, label = '90-%tile',color='black')\n",
    "\n",
    "plt.xticks(x_axis, AggPes.keys(),rotation = 70)\n",
    "plt.legend(bbox_to_anchor=(0.41,0.65))\n",
    "plt.ylabel('Agg WAPE')\n",
    "plt.xlabel('Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TimesL=[]\n",
    "AggSmapeL=[]\n",
    "AggWapeL=[]\n",
    "AggPeL=[]\n",
    "times_org=[]\n",
    "wape_agg_lstm=np.zeros(50, dtype=np.float64)\n",
    "df_lstm=np.zeros((30,50), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_month=['2020-09-01','2020-10-01','2020-11-01','2020-12-01','2021-01-01','2021-02-01','2021-03-01',\n",
    "            '2021-04-01','2021-05-01','2021-06-01','2021-07-01','2021-08-01']\n",
    "name=['September','October','November','December','January','February','March','April','May','June','July','August']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month=1\n",
    "print('Running for the month of :',name[month])\n",
    "################LSTM##########################\n",
    "################LSTM##########################\n",
    "for i in range(50):\n",
    "    print('cluster id: ',i+1)\n",
    "    current_date = '20220831'\n",
    "    test_group_id = test_group_id_list[i]#'/3944/1060825/447913'\n",
    "    #for top cluster:\n",
    "    path = f\"gs://adtech-artifacts/ds/forecast/month_eval/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_tft_data_{current_date}.csv\"\n",
    "    #path = f\"gs://adtech-artifacts/ds/forecast/month_eval/tail_clusters/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_data_20220831.csv\"\n",
    "    # read in data\n",
    "    df_panda = spark.read.option(\"header\", True).csv(path).toPandas() # The usage should be the same as previous\n",
    "    df_panda=df_panda[['ds', 'group_id', 'query_idx', 'SearchKeyValue',  'LocationKeyValue', 'daily_supply']]\n",
    "    #df_panda=spark.read.csv(\"gs://adtech-artifacts/ds/forecast/query_taxonomy_sample_Addedfeatures_cluster_\"+str(i)+\".csv\",header=True).toPandas()\n",
    "    Cols=['daily_supply']#, 'shifted_annual_daily_supply','Trend_Var','dayofyear_sin', 'dayofyear_cos', 'month_sin', 'month_cos']\n",
    "    for cols in Cols:  \n",
    "        df_panda[cols]=df_panda[cols].astype(float,errors='raise')\n",
    "    start=time.time()\n",
    "    start=time.time()\n",
    "    minError=1e10\n",
    "    param = {\n",
    "        \"learning_rate\": [.1,.01,.001],\n",
    "        \"hidden_size\": [10,20],\n",
    "        'batch_size': [32],\n",
    "        'dropout': [0.05,.1],\n",
    "        'gamma': [0.9],\n",
    "        'optimizer_name': ['AdamW'],\n",
    "        'num_layer': [1,2,3],\n",
    "        'relu': [False]\n",
    "    }\n",
    "    prep = dataprep(df_panda, input_window=90, output_window=30, stride=1,start=start_month[month])\n",
    "    print('Beginning the training...')\n",
    "    train_set_in, train_set_out, val_set_in, val_set_out, test_in_set, test_out_set,df_test_org,  q,wq = prep._get_data()\n",
    "    #print('print len of q, value and wq value',len(q),q[0],wq[0])\n",
    "    train_set_in = torch.from_numpy(train_set_in.astype(np.float64)).float()\n",
    "    train_set_out = torch.from_numpy(train_set_out.astype(np.float64)).float()\n",
    "    val_set_in = torch.from_numpy(val_set_in.astype(np.float64)).float()\n",
    "    val_set_out = torch.from_numpy(val_set_out.astype(np.float64)).float()\n",
    "    test_in_set = torch.from_numpy(test_in_set.astype(np.float64)).float()\n",
    "    test_out_set = torch.from_numpy(test_out_set.astype(np.float64)).float()\n",
    "    print('val shape is: ',val_set_in.shape, val_set_out.shape)\n",
    "    end1=time.time()\n",
    "    if len(train_set_in.shape) < 3:\n",
    "        train_set_in = train_set_in.reshape(train_set_in.shape[0], train_set_in.shape[1], 1)\n",
    "        val_set_in = val_set_in.reshape(val_set_in.shape[0], val_set_in.shape[1], 1)\n",
    "        test_in_set = test_in_set.reshape(test_in_set.shape[0], test_in_set.shape[1], 1)\n",
    "        test_out_set=test_out_set.reshape(test_out_set.shape[0], test_out_set.shape[1])\n",
    "    counter=1\n",
    "    for lr in param['learning_rate']:\n",
    "        for h_s in param['hidden_size']:\n",
    "            for b_s in param['batch_size']:\n",
    "                for dp in param['dropout']:\n",
    "                    for opt in param['optimizer_name']:\n",
    "                        for n_l in param['num_layer']:\n",
    "                            for act in param['relu']:\n",
    "                                for gm in param['gamma']:\n",
    "                                    config = {\n",
    "                                        \"num_epochs\": 200,\n",
    "                                        \"learning_rate\": lr,\n",
    "                                        \"hidden_size\": h_s,\n",
    "                                        \"window_size\": 30,\n",
    "                                        \"output_size\": 30,\n",
    "                                        'batch_size': b_s,\n",
    "                                        'dropout': dp,\n",
    "                                        'dropout1': 0,\n",
    "                                        'gamma': gm,\n",
    "                                        'optimizer_name': opt,\n",
    "                                        'num_layer': n_l,\n",
    "                                        'relu': act\n",
    "                                    }\n",
    "                                    #torch.manual_seed(seed)\n",
    "                                    print('trial run: ',counter)\n",
    "                                    counter+=1\n",
    "                                    e1,e2,e3 = running_lstm(config, save_model=True)\n",
    "                                    if (e1+e2+e3)/3<minError:\n",
    "                                        minError=(e1+e2+e3)/3\n",
    "                                        best_config=config\n",
    "                                        print('the  smape is {} wape is {} and pe is {} and total is {}'.format(e1,e2,e3,(e1+e2+e3)/3))\n",
    "                                        print('the config is {}',config)\n",
    "    if minError>=.7:\n",
    "        param = {\n",
    "        \"learning_rate\": [.005,.1,.0001],\n",
    "        \"hidden_size\": [15,30],\n",
    "        'batch_size': [64],\n",
    "        'dropout': [0.3,.5],\n",
    "        'gamma': [0.9],\n",
    "        'optimizer_name': ['Adam','AdamW'],\n",
    "        'num_layer': [3,5],\n",
    "        'relu': [True]}\n",
    "        for lr in param['learning_rate']:\n",
    "            for h_s in param['hidden_size']:\n",
    "                for b_s in param['batch_size']:\n",
    "                    for dp in param['dropout']:\n",
    "                        for opt in param['optimizer_name']:\n",
    "                            for n_l in param['num_layer']:\n",
    "                                for act in param['relu']:\n",
    "                                    for gm in param['gamma']:\n",
    "                                        config = {\n",
    "                                            \"num_epochs\": 200,\n",
    "                                            \"learning_rate\": lr,\n",
    "                                            \"hidden_size\": h_s,\n",
    "                                            \"window_size\": 30,\n",
    "                                            \"output_size\": 30,\n",
    "                                            'batch_size': b_s,\n",
    "                                            'dropout': dp,\n",
    "                                            'dropout1': 0,\n",
    "                                            'gamma': gm,\n",
    "                                            'optimizer_name': opt,\n",
    "                                            'num_layer': n_l,\n",
    "                                            'relu': act\n",
    "                                        }\n",
    "                                        print('trial run: ',counter)\n",
    "                                        counter+=1\n",
    "                                        e1,e2,e3 = running_lstm(config, save_model=True)\n",
    "                                        \n",
    "                                        #print('the config is {}',config)\n",
    "                                        if (e1+e2+e3)/3<minError:\n",
    "                                            minError=(e1+e2+e3)/3\n",
    "                                            best_config=config\n",
    "                                            print('the  smape is {} wape is {} and pe is {} and total is {}'.format(e1,e2,e3,(e1+e2+e3)/3))\n",
    "                                            print('the config is {}',config)\n",
    "\n",
    "    end = time.time()\n",
    "    print('thus best config is', best_config)\n",
    "    Wsmape, Wape, Wpe, smape_dis, wape_dis, pe_dis, smape_agg, wape_agg, pe_agg, model, df_lstm[:,i],wape_agg_lstm[i] = train_lstm(best_config, save_model=True)\n",
    "    print('val wape is: ',wape_agg_lstm[i])\n",
    "    print('total time ', end1 - end)\n",
    "    print('test Wsmape: ', Wsmape, ' test Wape: ', Wape, ' test Wpe: ', Wpe)\n",
    "    print('Smape dis',smape_dis,'Wape dis',wape_dis,'PE dis',pe_dis)\n",
    "    print('Agg SMAPE: ',smape_agg,'Agg WAPE: ',wape_agg,'Agg PE: ',pe_agg)\n",
    "    print('total time ', end1 - end)\n",
    "\n",
    "    #print('time needed to run the model: ',end-start)\n",
    "    AggSmapeL.append(smape_agg)\n",
    "    AggWapeL.append(wape_agg)\n",
    "    AggPeL.append(pe_agg)\n",
    "    SMAPEsL.append(Wsmape)\n",
    "    WAPEsL.append(Wape)\n",
    "    PEsL.append(Wpe)\n",
    "    median_smape.append(smape_dis[0])\n",
    "    Eighty_per_smape.append(smape_dis[1])\n",
    "    Ninty_per_smape.append(smape_dis[2])\n",
    "    median_wape.append(wape_dis[0])\n",
    "    Eighty_per_wape.append(wape_dis[1])\n",
    "    Ninty_per_wape.append(wape_dis[2])\n",
    "    median_pe.append(pe_dis[0])\n",
    "    Eighty_per_pe.append(pe_dis[1])\n",
    "    Ninty_per_pe.append(pe_dis[2])\n",
    "    TimesL.append(end1-end)\n",
    "    times_org.append(end1-start)\n",
    "    print('===============================================')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Times=[]\n",
    "AggSmape=[]\n",
    "AggWape=[]\n",
    "AggPe=[]\n",
    "\n",
    "counter=0\n",
    "for i in range(50):\n",
    "    if i not in [44,47,48]:\n",
    "        \n",
    "        st=time.time()\n",
    "        print('cluster id: ',i+1)\n",
    "        current_date = '20220831'\n",
    "        test_group_id = test_group_id_list[i]#'/3944/1060825/447913'\n",
    "        print('group id',test_group_id)\n",
    "        #for top cluster:\n",
    "        #path = f\"gs://adtech-artifacts/ds/forecast/month_eval/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_tft_data_{current_date}.csv\"\n",
    "        path = f\"gs://adtech-artifacts/ds/forecast/month_eval/tail_clusters/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_data_20220831.csv\"\n",
    "        # read in data\n",
    "        df = spark.read.option(\"header\", True).csv(path).toPandas() # The usage should be the same as previous\n",
    "        df=df[['ds', 'group_id', 'query_idx', 'SearchKeyValue',  'LocationKeyValue', 'daily_supply']]\n",
    "        #print('query id: ',df['query_idx'].unique())\n",
    "        #df_panda=spark.read.csv(\"gs://adtech-artifacts/ds/forecast/query_taxonomy_sample_Addedfeatures_cluster_\"+str(i)+\".csv\",header=True).toPandas()\n",
    "        Cols=['daily_supply']#, 'shifted_annual_daily_supply','Trend_Var','dayofyear_sin', 'dayofyear_cos', 'month_sin', 'month_cos']\n",
    "        for cols in Cols:df[cols]=df[cols].astype(float,errors='raise')\n",
    "        #print('time needed to preprocess the data is: ',time.time()-st)\n",
    "        df_train,df_test,train_date,test_date,q,wq=_generate_train_test_data(df,start=start_month[month])\n",
    "        df_trainW,df_valW,_,train_dateW,_,_,_=_generate_train_test_data(df,val_req=True,start=start_month[month])\n",
    "        df1_train,df1_test=df_train.sum(axis=1),df_test.sum(axis=1)\n",
    "        df1_trainW,df_valW=df_trainW.sum(axis=1),df_valW.sum(axis=1)\n",
    "        df1_train,df1_trainW,df_valW=df1_train.to_frame(),df1_trainW.to_frame(),df_valW.to_frame()\n",
    "        df1_train.index=train_date\n",
    "        df1_trainW.index=train_dateW\n",
    "        df1_train.index = df1_train.index.to_period(freq = 'D')\n",
    "        df1_trainW.index = df1_trainW.index.to_period(freq = 'D')\n",
    "        #df_test_log_temp.index = df_test_log_temp.index.to_period(freq = 'D')\n",
    "        df1_train=df1_train.rename(columns={0:'daily_supply'})\n",
    "        df1_trainW=df1_trainW.rename(columns={0:'daily_supply'})\n",
    "    #     if i==0:\n",
    "    #         print(q,wq)\n",
    "        start=time.time()\n",
    "        Models=statModels()\n",
    "\n",
    "        Models.clus_smape['Lstm']=AggSmapeL[counter]\n",
    "        Models.clus_wape['Lstm']=AggWapeL[counter]\n",
    "        Models.clus_pe['Lstm']=AggPeL[counter]\n",
    "        #Models.models['Lstm'] = fit4\n",
    "        #self.stores_mape['Es'] = mape\n",
    "        Models.stores_times['Lstm'] = TimesL[counter]\n",
    "        ##################\n",
    "        #Models.wape_Weighted['Lstm'] = wape_agg_lstm[i]\n",
    "\n",
    "        Models.training_infer(df1_train,df1_test,df_test,q.values,wq.values,df1_test.shape[0],df_lstm[:,i])#no_ts\n",
    "                             #(df_train, df_test, df_test_org,q, wq, forecast_period=30,lstm_pred=None)\n",
    "        Models.weighted_avg(df1_trainW, df1_test,df_valW.values, df_test,q.values,wq.values, forecast_period=60,lstm_pred=df_lstm[:,i])\n",
    "\n",
    "        end=time.time()\n",
    "        print('Wape is: ',Models.clus_wape)\n",
    "        print('Smape is: ',Models.clus_smape)\n",
    "        print('Pe is: ',Models.clus_smape)\n",
    "        print('Time is:' ,Models.clus_pe)\n",
    "        #print('time needed to run the model: ',end-start)\n",
    "        Times.append(Models.stores_times)\n",
    "        AggSmape.append(Models.clus_smape)\n",
    "        AggWape.append(Models.clus_wape)\n",
    "        AggPe.append(Models.clus_pe)\n",
    "        #training_j.append(i)\n",
    "        counter+=1\n",
    "    print('===============================================')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Time={i:[] for i in Times[0]}\n",
    "\n",
    "AggSmapes={i:[] for i in AggSmape[0]}\n",
    "AggWapes={i:[] for i in AggWape[0]}\n",
    "AggPes={i:[] for i in AggPe[0]}\n",
    "\n",
    "\n",
    "for i in range(counter):\n",
    "\n",
    "    for keys,values in Times[i].items():\n",
    "        Time[keys].append(values)\n",
    "    for keys,values in AggSmape[i].items():\n",
    "        AggSmapes[keys].append(values)\n",
    "    for keys,values in AggWape[i].items():\n",
    "        AggWapes[keys].append(values)\n",
    "    for keys,values in AggPe[i].items():\n",
    "        AggPes[keys].append(values)\n",
    "    \n",
    "print('Time is: ',Time)\n",
    "print('====================')\n",
    "print('AggSmapes is: ',AggSmapes)\n",
    "print('====================')\n",
    "print('AggWapes is: ',AggWapes)\n",
    "print('====================')\n",
    "print('AggPes is: ',AggPes)\n",
    "print('====================')\n",
    "\n",
    "print('====================')\n",
    "print('Original LSTM time is', times_org)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TimesL=[]\n",
    "AggSmapeL=[]\n",
    "AggWapeL=[]\n",
    "AggPeL=[]\n",
    "times_org=[]\n",
    "wape_agg_lstm=np.zeros(50, dtype=np.float64)\n",
    "df_lstm=np.zeros((30,50), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_month=['2020-09-01','2020-10-01','2020-11-01','2020-12-01','2021-01-01','2021-02-01','2021-03-01',\n",
    "            '2021-04-01','2021-05-01','2021-06-01','2021-07-01','2021-08-01']\n",
    "name=['September','October','November','December','January','February','March','April','May','June','July','August']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month=2\n",
    "print('Running for the month of :',name[month])\n",
    "################LSTM##########################\n",
    "################LSTM##########################\n",
    "for i in range(50):\n",
    "    print('cluster id: ',i+1)\n",
    "    current_date = '20220831'\n",
    "    test_group_id = test_group_id_list[i]#'/3944/1060825/447913'\n",
    "    #for top cluster:\n",
    "    path = f\"gs://adtech-artifacts/ds/forecast/month_eval/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_tft_data_{current_date}.csv\"\n",
    "    #path = f\"gs://adtech-artifacts/ds/forecast/month_eval/tail_clusters/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_data_20220831.csv\"\n",
    "    # read in data\n",
    "    df_panda = spark.read.option(\"header\", True).csv(path).toPandas() # The usage should be the same as previous\n",
    "    df_panda=df_panda[['ds', 'group_id', 'query_idx', 'SearchKeyValue',  'LocationKeyValue', 'daily_supply']]\n",
    "    #df_panda=spark.read.csv(\"gs://adtech-artifacts/ds/forecast/query_taxonomy_sample_Addedfeatures_cluster_\"+str(i)+\".csv\",header=True).toPandas()\n",
    "    Cols=['daily_supply']#, 'shifted_annual_daily_supply','Trend_Var','dayofyear_sin', 'dayofyear_cos', 'month_sin', 'month_cos']\n",
    "    for cols in Cols:  \n",
    "        df_panda[cols]=df_panda[cols].astype(float,errors='raise')\n",
    "    start=time.time()\n",
    "    start=time.time()\n",
    "    minError=1e10\n",
    "    param = {\n",
    "        \"learning_rate\": [.1,.01,.001],\n",
    "        \"hidden_size\": [10,20],\n",
    "        'batch_size': [32],\n",
    "        'dropout': [0.05,.1],\n",
    "        'gamma': [0.9],\n",
    "        'optimizer_name': ['AdamW'],\n",
    "        'num_layer': [1,2,3],\n",
    "        'relu': [False]\n",
    "    }\n",
    "    prep = dataprep(df_panda, input_window=90, output_window=30, stride=1,start=start_month[month])\n",
    "    print('Beginning the training...')\n",
    "    train_set_in, train_set_out, val_set_in, val_set_out, test_in_set, test_out_set,df_test_org,  q,wq = prep._get_data()\n",
    "    #print('print len of q, value and wq value',len(q),q[0],wq[0])\n",
    "    train_set_in = torch.from_numpy(train_set_in.astype(np.float64)).float()\n",
    "    train_set_out = torch.from_numpy(train_set_out.astype(np.float64)).float()\n",
    "    val_set_in = torch.from_numpy(val_set_in.astype(np.float64)).float()\n",
    "    val_set_out = torch.from_numpy(val_set_out.astype(np.float64)).float()\n",
    "    test_in_set = torch.from_numpy(test_in_set.astype(np.float64)).float()\n",
    "    test_out_set = torch.from_numpy(test_out_set.astype(np.float64)).float()\n",
    "    print('val shape is: ',val_set_in.shape, val_set_out.shape)\n",
    "    end1=time.time()\n",
    "    if len(train_set_in.shape) < 3:\n",
    "        train_set_in = train_set_in.reshape(train_set_in.shape[0], train_set_in.shape[1], 1)\n",
    "        val_set_in = val_set_in.reshape(val_set_in.shape[0], val_set_in.shape[1], 1)\n",
    "        test_in_set = test_in_set.reshape(test_in_set.shape[0], test_in_set.shape[1], 1)\n",
    "        test_out_set=test_out_set.reshape(test_out_set.shape[0], test_out_set.shape[1])\n",
    "    counter=1\n",
    "    for lr in param['learning_rate']:\n",
    "        for h_s in param['hidden_size']:\n",
    "            for b_s in param['batch_size']:\n",
    "                for dp in param['dropout']:\n",
    "                    for opt in param['optimizer_name']:\n",
    "                        for n_l in param['num_layer']:\n",
    "                            for act in param['relu']:\n",
    "                                for gm in param['gamma']:\n",
    "                                    config = {\n",
    "                                        \"num_epochs\": 200,\n",
    "                                        \"learning_rate\": lr,\n",
    "                                        \"hidden_size\": h_s,\n",
    "                                        \"window_size\": 30,\n",
    "                                        \"output_size\": 30,\n",
    "                                        'batch_size': b_s,\n",
    "                                        'dropout': dp,\n",
    "                                        'dropout1': 0,\n",
    "                                        'gamma': gm,\n",
    "                                        'optimizer_name': opt,\n",
    "                                        'num_layer': n_l,\n",
    "                                        'relu': act\n",
    "                                    }\n",
    "                                    #torch.manual_seed(seed)\n",
    "                                    print('trial run: ',counter)\n",
    "                                    counter+=1\n",
    "                                    e1,e2,e3 = running_lstm(config, save_model=True)\n",
    "                                    if (e1+e2+e3)/3<minError:\n",
    "                                        minError=(e1+e2+e3)/3\n",
    "                                        best_config=config\n",
    "                                        print('the  smape is {} wape is {} and pe is {} and total is {}'.format(e1,e2,e3,(e1+e2+e3)/3))\n",
    "                                        print('the config is {}',config)\n",
    "    if minError>=.7:\n",
    "        param = {\n",
    "        \"learning_rate\": [.005,.1,.0001],\n",
    "        \"hidden_size\": [15,30],\n",
    "        'batch_size': [64],\n",
    "        'dropout': [0.3,.5],\n",
    "        'gamma': [0.9],\n",
    "        'optimizer_name': ['Adam','AdamW'],\n",
    "        'num_layer': [3,5],\n",
    "        'relu': [True]}\n",
    "        for lr in param['learning_rate']:\n",
    "            for h_s in param['hidden_size']:\n",
    "                for b_s in param['batch_size']:\n",
    "                    for dp in param['dropout']:\n",
    "                        for opt in param['optimizer_name']:\n",
    "                            for n_l in param['num_layer']:\n",
    "                                for act in param['relu']:\n",
    "                                    for gm in param['gamma']:\n",
    "                                        config = {\n",
    "                                            \"num_epochs\": 200,\n",
    "                                            \"learning_rate\": lr,\n",
    "                                            \"hidden_size\": h_s,\n",
    "                                            \"window_size\": 30,\n",
    "                                            \"output_size\": 30,\n",
    "                                            'batch_size': b_s,\n",
    "                                            'dropout': dp,\n",
    "                                            'dropout1': 0,\n",
    "                                            'gamma': gm,\n",
    "                                            'optimizer_name': opt,\n",
    "                                            'num_layer': n_l,\n",
    "                                            'relu': act\n",
    "                                        }\n",
    "                                        print('trial run: ',counter)\n",
    "                                        counter+=1\n",
    "                                        e1,e2,e3 = running_lstm(config, save_model=True)\n",
    "                                        \n",
    "                                        #print('the config is {}',config)\n",
    "                                        if (e1+e2+e3)/3<minError:\n",
    "                                            minError=(e1+e2+e3)/3\n",
    "                                            best_config=config\n",
    "                                            print('the  smape is {} wape is {} and pe is {} and total is {}'.format(e1,e2,e3,(e1+e2+e3)/3))\n",
    "                                            print('the config is {}',config)\n",
    "\n",
    "    end = time.time()\n",
    "    print('thus best config is', best_config)\n",
    "    Wsmape, Wape, Wpe, smape_dis, wape_dis, pe_dis, smape_agg, wape_agg, pe_agg, model, df_lstm[:,i],wape_agg_lstm[i] = train_lstm(best_config, save_model=True)\n",
    "    print('val wape is: ',wape_agg_lstm[i])\n",
    "    print('total time ', end1 - end)\n",
    "    print('test Wsmape: ', Wsmape, ' test Wape: ', Wape, ' test Wpe: ', Wpe)\n",
    "    print('Smape dis',smape_dis,'Wape dis',wape_dis,'PE dis',pe_dis)\n",
    "    print('Agg SMAPE: ',smape_agg,'Agg WAPE: ',wape_agg,'Agg PE: ',pe_agg)\n",
    "    print('total time ', end1 - end)\n",
    "\n",
    "    #print('time needed to run the model: ',end-start)\n",
    "    AggSmapeL.append(smape_agg)\n",
    "    AggWapeL.append(wape_agg)\n",
    "    AggPeL.append(pe_agg)\n",
    "    SMAPEsL.append(Wsmape)\n",
    "    WAPEsL.append(Wape)\n",
    "    PEsL.append(Wpe)\n",
    "    median_smape.append(smape_dis[0])\n",
    "    Eighty_per_smape.append(smape_dis[1])\n",
    "    Ninty_per_smape.append(smape_dis[2])\n",
    "    median_wape.append(wape_dis[0])\n",
    "    Eighty_per_wape.append(wape_dis[1])\n",
    "    Ninty_per_wape.append(wape_dis[2])\n",
    "    median_pe.append(pe_dis[0])\n",
    "    Eighty_per_pe.append(pe_dis[1])\n",
    "    Ninty_per_pe.append(pe_dis[2])\n",
    "    TimesL.append(end1-end)\n",
    "    times_org.append(end1-start)\n",
    "    print('===============================================')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Times=[]\n",
    "AggSmape=[]\n",
    "AggWape=[]\n",
    "AggPe=[]\n",
    "\n",
    "counter=0\n",
    "for i in range(50):\n",
    "    if i not in [44,47,48]:\n",
    "        \n",
    "        st=time.time()\n",
    "        print('cluster id: ',i+1)\n",
    "        current_date = '20220831'\n",
    "        test_group_id = test_group_id_list[i]#'/3944/1060825/447913'\n",
    "        print('group id',test_group_id)\n",
    "        #for top cluster:\n",
    "        #path = f\"gs://adtech-artifacts/ds/forecast/month_eval/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_tft_data_{current_date}.csv\"\n",
    "        path = f\"gs://adtech-artifacts/ds/forecast/month_eval/tail_clusters/query_taxonomy_clusters_sample{'_'.join(test_group_id.split('/'))}_data_20220831.csv\"\n",
    "        # read in data\n",
    "        df = spark.read.option(\"header\", True).csv(path).toPandas() # The usage should be the same as previous\n",
    "        df=df[['ds', 'group_id', 'query_idx', 'SearchKeyValue',  'LocationKeyValue', 'daily_supply']]\n",
    "        #print('query id: ',df['query_idx'].unique())\n",
    "        #df_panda=spark.read.csv(\"gs://adtech-artifacts/ds/forecast/query_taxonomy_sample_Addedfeatures_cluster_\"+str(i)+\".csv\",header=True).toPandas()\n",
    "        Cols=['daily_supply']#, 'shifted_annual_daily_supply','Trend_Var','dayofyear_sin', 'dayofyear_cos', 'month_sin', 'month_cos']\n",
    "        for cols in Cols:df[cols]=df[cols].astype(float,errors='raise')\n",
    "        #print('time needed to preprocess the data is: ',time.time()-st)\n",
    "        df_train,df_test,train_date,test_date,q,wq=_generate_train_test_data(df,start=start_month[month])\n",
    "        df_trainW,df_valW,_,train_dateW,_,_,_=_generate_train_test_data(df,val_req=True,start=start_month[month])\n",
    "        df1_train,df1_test=df_train.sum(axis=1),df_test.sum(axis=1)\n",
    "        df1_trainW,df_valW=df_trainW.sum(axis=1),df_valW.sum(axis=1)\n",
    "        df1_train,df1_trainW,df_valW=df1_train.to_frame(),df1_trainW.to_frame(),df_valW.to_frame()\n",
    "        df1_train.index=train_date\n",
    "        df1_trainW.index=train_dateW\n",
    "        df1_train.index = df1_train.index.to_period(freq = 'D')\n",
    "        df1_trainW.index = df1_trainW.index.to_period(freq = 'D')\n",
    "        #df_test_log_temp.index = df_test_log_temp.index.to_period(freq = 'D')\n",
    "        df1_train=df1_train.rename(columns={0:'daily_supply'})\n",
    "        df1_trainW=df1_trainW.rename(columns={0:'daily_supply'})\n",
    "    #     if i==0:\n",
    "    #         print(q,wq)\n",
    "        start=time.time()\n",
    "        Models=statModels()\n",
    "\n",
    "        Models.clus_smape['Lstm']=AggSmapeL[counter]\n",
    "        Models.clus_wape['Lstm']=AggWapeL[counter]\n",
    "        Models.clus_pe['Lstm']=AggPeL[counter]\n",
    "        #Models.models['Lstm'] = fit4\n",
    "        #self.stores_mape['Es'] = mape\n",
    "        Models.stores_times['Lstm'] = TimesL[counter]\n",
    "        ##################\n",
    "        #Models.wape_Weighted['Lstm'] = wape_agg_lstm[i]\n",
    "\n",
    "        Models.training_infer(df1_train,df1_test,df_test,q.values,wq.values,df1_test.shape[0],df_lstm[:,i])#no_ts\n",
    "                             #(df_train, df_test, df_test_org,q, wq, forecast_period=30,lstm_pred=None)\n",
    "        Models.weighted_avg(df1_trainW, df1_test,df_valW.values, df_test,q.values,wq.values, forecast_period=60,lstm_pred=df_lstm[:,i])\n",
    "\n",
    "        end=time.time()\n",
    "        print('Wape is: ',Models.clus_wape)\n",
    "        print('Smape is: ',Models.clus_smape)\n",
    "        print('Pe is: ',Models.clus_smape)\n",
    "        print('Time is:' ,Models.clus_pe)\n",
    "        #print('time needed to run the model: ',end-start)\n",
    "        Times.append(Models.stores_times)\n",
    "        AggSmape.append(Models.clus_smape)\n",
    "        AggWape.append(Models.clus_wape)\n",
    "        AggPe.append(Models.clus_pe)\n",
    "        #training_j.append(i)\n",
    "        counter+=1\n",
    "    print('===============================================')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Time={i:[] for i in Times[0]}\n",
    "\n",
    "AggSmapes={i:[] for i in AggSmape[0]}\n",
    "AggWapes={i:[] for i in AggWape[0]}\n",
    "AggPes={i:[] for i in AggPe[0]}\n",
    "\n",
    "\n",
    "for i in range(counter):\n",
    "\n",
    "    for keys,values in Times[i].items():\n",
    "        Time[keys].append(values)\n",
    "    for keys,values in AggSmape[i].items():\n",
    "        AggSmapes[keys].append(values)\n",
    "    for keys,values in AggWape[i].items():\n",
    "        AggWapes[keys].append(values)\n",
    "    for keys,values in AggPe[i].items():\n",
    "        AggPes[keys].append(values)\n",
    "    \n",
    "print('Time is: ',Time)\n",
    "print('====================')\n",
    "print('AggSmapes is: ',AggSmapes)\n",
    "print('====================')\n",
    "print('AggWapes is: ',AggWapes)\n",
    "print('====================')\n",
    "print('AggPes is: ',AggPes)\n",
    "print('====================')\n",
    "\n",
    "print('====================')\n",
    "print('Original LSTM time is', times_org)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}